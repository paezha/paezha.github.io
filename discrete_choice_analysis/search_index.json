[
["index.html", "An Introduction to Discrete Choice Analysis A Course in R Preface Choices, choices, choices Price Mechanisms Plan Audience Requisites", " An Introduction to Discrete Choice Analysis A Course in R Antonio Paez 2019-03-24 Preface ‘Would you tell me, please, which way I ought to go from here?’ ‘That depends a good deal on where you want to get to,’ said the Cat. ‘I don’t much care where -’ said Alice. ‘Then it doesn’t matter which way you go,’ said the Cat. ‘- so long as I get SOMEWHERE,’ Alice added as an explanation. ‘Oh, you’re sure to do that,’ said the Cat, ‘if you only walk long enough.’ — Lewis Carroll, Alice in Wonderland “We are our choices.” — Jean-Paul Sartre Choices, choices, choices We are what we choose or we choose what we are. The choices we make have important implications for how we interact with the world. We live in a time when resources continue to become increasingly scarce. In this context, the way people make choices is informative about their preferences and where collectively we are going. This includes routine activities, such as deciding how to travel for everyday purposes, for instance walking or cycling, driving, or using transit. Or longer term decision, such as whether to contract more expensive but environmentally less damaging low impact energy sources; buy hybrid, electric, or gasoline vehicles; the frequency with which they travel by plane; to live in a distant suburb where rent is low, or in a central city where space is at a premium; and many others. Understanding decision-making is also important so that the world, or more accurately the social institutions that collectively represent us in our interactions with the world, can better accommodate and possibly even nudge our preferences towards socially desirable outcomes. What tradeoffs are members of the public willing to contemplate when choosing alternative mobility? Is it the range of vehicles? Their price? The time it takes to charge an electric vehicle? The satisfaction of being green? Should governments subdidize purchases of efficient heating? If so, what is the effect of a certain amount in subsidy? Do consumers prefer more range in electric vehicles, and how much are they willing to pay for it? Does the tradoff justify the increase in production cost? These questions are important as governments and businesses try to understand the way the public will respond to taxation, programs, engineering, or production decisions. In simple terms, discrete choice analysis is a way to understand behavior when there are implicit markets, and in this way it represents a form of hedonic price analysis. What are prices? In very general terms a price is a quantity offered in payment for one unit of a good or service. Early economies were based on bartering, a system of exchange that is limited use in even moderately complex economic systems. For instance, imagine a moderately sophisticated economy where people already have ceased being generalists to become specialists of some sort. Someone who spends time making shoes probably has limited time to tend chickens or understand the law of the land. Imagine that you make and sell shoes. People need shoes and may be willing to offer some quantity of something in exchange for them. The person who breeds chicken maybe willing to offer one chicken for a pair of shoes. Is that a fair price? How can you determine whether that exchange is sensible? Imagine now that you need a divorce, and a lawyer’s services are required for this. How many shoes should you offer the lawyer for a divorce? And what if the lawyer already has shoes?? Maybe the lawyer would prefer to be paid in chickens… The situation is further complicated by the reliance of bartering on some kind of trust system: as a seller, in a bartering system, there are no simple ways of ensuring the quality of the exchange! For instance, what if the lawyer is a crook, or the farmer gives you diseased chickens for your top-notch, high-quality shoes? In small systems, where agents can recognize each other, trust is enforced by reputation - if the lawyer is crooked, or the farmer is known to feed lead to the poultry, other actors can avoid transactions with them. If your shoes fall apart in the first rain, people will begin to avoid doing business with you! As these simple examples illustrate, bartering is a complicated way of setting prices, and this becomes increasingly complex (except in very exceptional situations) when an economy produces hundreds, thousands, or even more different products and services. (An interesting exception is the time when Pepsico bartered with the Soviet Union; see this news item from 1990. In this case, Pepsi-Cola was bartered for ships and vodka. Why did barter make sense in this situation? Hint: the ruble was not freely traded on world currency markets.) Far from being the root of all evil, it is almost certain that no complex economy can exist without the invention of money. The complexities of bartering explain why monetary systems were invented: the need for a common standard for exchange. Instead of needing to figure out how many shoes is worth a chicken and how many chickens a divorce, everything is measured using the same metric: shells, squares of deer skin, rupees, pesos, or dollars. Price Mechanisms The limitations of bartering explain the necessity of monetary systems. A common currency frees the maker of shoes from the need to calculate in chickens the cost of his divorce. But it does not explain how prices are set in the common currency. Price mechanisms depend to a large extent of the institutional framework. Several such frameworks exist. For example, in a centrally planned economy, prices for goods or services are set by a designated agent. This could be the Elder of the Village. The Elder of the village decides how many rupees people should pay for your shoes (alternatively, how much you can charge for a pair of shoes), and how much you should pay a lawyer for each hour of work. Everyone in this kind of setup prays that the Elder of the Village knows what he is doing, and the potential for mistakes clearly is far from negligible. In the Soviet Union prices were set using so called material balances, balancing the inputs to the planned outputs. This approach was not successful for several reasons, including ideological limitations to the mathematical tools used to calculate balances, and the inherent limitations of such planning, which does not allow for deviations from the plan (what if you end up needing two divorces instead of only one or none?). In a free market economy, on the other hand, prices are left to float with no intervention from central planning agencies. There are voluminous literature explaining how this system can allocate resources efficiently. Prices, in this context, are signals of how desirable a good or service is, and how much of it is available. Economists explain this relationship using a relationship between demand and supply. The basic assumption (which happens to hold in many cases) is that the level of demand for a good or service (the quantity that consumers are willing to purchase) declines as the price increases. On the side of producers/providers of goods and services, the level of supply (the quantity that they are willing to produce) increases with price. Demand and supply, then, are influenced by price, but they do not happen in isolation - rather, they interact to set prices. A consumer cannot single-handedly demand that a good/service be sold at a certain (i.e., low) price when many consumers are willing to pay a somewhat higher price for the same (otherwise I would have bought my Nintendo Switch at 50 dollars). Likewise, a producer/provider cannot expect to set a high price for a good/service when other producers are willing to sell at a lower price. (There are aberrant situations, of course. Monopolies and cartels can manipulate prices on behalf of producers, whereas single-payer health care manipulates prices in favor of patients.) The intersection of the supply and demand curves determines simultaneously prices and the quantity of a good/service produced/consumed. Since prices “float”, they can adjust to changes in supply and/or demand. In Figure 1, for example, when demand for a good or service (say divorces) increases, there is an incentive for lawyers to work more hours. Since there is a limited number of hours that lawyers can work (there is scarcity), this is reflected in a higher price, since those who can afford it will be willing to pay more for a scarce but desirable service. Fig 1. Supply and Demand Relationship A third system is a mixed economy, where prices are left to float but within limits or with other corrective mechanisms, such as subsidies or taxes. The most familiar situation for most of us is price mechanisms in free or mixed economies. An underlying assumption is that a market exists for the good or service, that is, a medium where goods or services can be exchanged. Markets exist for many things: for milk, for bread, for insurance, and for complex financial instruments that no one really understands, such as derivatives. Markets do not explicitly exist for composite products or services, and therefore the willingnes to pay of consumers for elements of a specific good. Imagine, for instance, a good such as an automobile. Automobiles are composite goods in the sense that, even if their purpose is to provide transportation, they can do this in many different ways to satisfy a diversity of needs or tastes: with variations in leg room, acceleration, and fuel consumption, to name a few. Pricing mechanisms for the whole leave the question of willingness to pay for specific components in the dark. Are consumers willing to pay more or less for extra leg room, more spacious seats, or horsepower? The markets for each of these items are implicit in the market for automobiles. In fact, hedonic price analysis analysis was invented by an economist named Andrew Court to address such an issue. Court was an economist for the Automobile Manufacturers’ Association in Detroit from 1930 to 1940. He realized that price indexing procedures were not satisfactory for describing the relative importance of various components of automobiles in determining their price. This in turn was important to understand consumer preferences and to differentiate products. Court used the term “hedonic” to express the usefulness and desirability (related to pleasure) that consumers attach to different aspects of a composite product. Although he invented his method in the late 1930s, it lay dormant for approximately twenty years until it was popularized by Zvi Griliches in the 1960s, with work on fertilizers and automobiles. Later on, Sherwin Rosen explained implicit markets within an economic foundation of supply and demand in equilibrium - that is, not just as willingness to pay on the side of consumers, but also as the result of decisions by producers. In brief, Rosen explained how the differentiated hedonic price function represents the envelope of a family of “value” functions (willingness to pay) and a family of offer functions (willingess to sell). Since then, many uses have been found for hedonic price analysis, with applications ranging from the pricing of computers, personal digital assistants, and wine to online purchases and farmland. The field of discrete choice analysis is concerned with the analysis of implicit markets when the outcome of the choice process is discrete. This requires a few things: DESCRIBE THE CONDITIONS FOR A PROCESS TO BE A CHOICE. Plan The plan with these notes is to introduce discrete choice analysis in an intuitive way. To achieve this, I use examples and coding, lots of coding. There are some classical references, for instance Ben-Akiva and Lerman (1985) and Train (2009), and then more specialized books such as Louviere et al. (2000). Other books cover discrete choice analysis as one component of modelling systems (such as transportation; see Ortúzar and Willumsen 2011), or cover related topics but from a statistical perspective (Maddala 1983). The present notes should be appealing to students or others who are approaching this topic for the first time, I strongly encourage readers to become acquainted with these books if they have not already. Each author organizes topics in a way that is logical to them. Some texts begin with a coverage of fundamental mathematics, probability, and statistics. Others with an introduction to a substantive topic (e.g., the context of travel demand analysis). In the book Applied Choice Analysis: A Primer by Hensher et al. (2005), the title of Chapter 10 is “Getting Started Modeling”. Train (2009), in contrast, begins by discussing the properties of discrete choice models and discussing the logit model right away. For presentation I have in the past relied heavily on Train’s book to organize my graduate seminar. I find this style of presentation sufficiently intuitive, when combined with some relevant topics introduced at key points. For example, I find that it makes sense to have a discussion of specification and estimation of models after introducing the logit model. In this way, the details of specifying utility functions can be presented in the context of an operational model. Readers will notice that this notes tend to follow Train closely, using a thematic approach, moving from the fundamentals, introducing the logit model, and then by families of models, i.e., GEV, probit, and so on. Beginning early in the text, readers are asked to get their hands dirty with code. This is a very deliberate decision. Most books on discrete choice analysis are software-independent, meaning that they cover the topics without making reference to a particular statistical package for analysis. Others rely for presentation on a specific software. For instance, Hensher et al. (2005) refer extensively to the software NLOGIT, a software package sold by Econometric Software, Inc.. Yet other packages were originally developed independently of a statistical computing project. One example is Michel Bierlaire’s BIOGEME. Not being associated with a statistical computing project means that synergies with other packages cannot be realized. Newer versions of BIOGEME now exist written almost exclusively in Python and that allow the package to benefit from the Python Data Analysis Library Pandas. For this text, I have chosen R. R is a generalist statistical language with a very broad user base. I personally find R more accessible than Python, for example, as an introduction to statistical and data analysis computing, particularly with the support of a good Interactive Development Environment such as RStudio. Packages (the fundamental units of shareable code in R) benefit from the synergyes of many developers and users sharing their code in a transparent and open way. Ten years ago it would have been very difficult to write a book on discrete choice modelling based on R: the earliest version of Croissant’s mlogit package (Y. Croissant 2018) dates from 2009; the earliest version of Sarrias and Daziano’s gmnl package (Sarrias and Daziano 2017) dates from 2015. Secondly, R and related packages are free. It is my conviction that research can be accelerated by the generous contributions of developers who graciously share their code with the world. By doing this, they help to maintain the cost of research low, and thus enable more people around the world to engage in it. That said, there is a potential disadvantage: unlike more established (especially commercial) packages that have been kicking around for years if not decades, newer R packages may still have some limitations. To mention one, the current versions of the packages mlogit and gmnl are implemented exclusively for universal choice set, in other words, under the assumption that all alternatives are available to all decision-makers. There are situations, For example, suppose My plan for this text, therefore, is to cover a topic in each section that builds on previous material. I have used the materials presented in these notes (in different incarnations) for teaching discrete choice analysis in different settings. Primarily, these notes have been used in the course GEOG 738 Discrete Choice Analysis at McMaster University. This course is a full (Canadian) graduate term, which typically means 11 or 12 weeks of classes. The course is organized as a 2-hour seminar that is offered once per week. Accordingly, each section is designed to cover very approximately the material that I am used to cover in a 2 hour seminar. As I continue to work on these notes, I hope to be able to add optional (or bonus) chapters, that could be used 1) to extend a course on discrete choice analysis beyond the 12 week horizon of the Canadian graduate school term, and/or 2) to offer more advanced material to interested readers. Audience The notes were designed for a graduate course in geography, but are not necessarily limited to geographers, and could indeed be a valuable resource to graduate students and others interested in discrete choice analysis and applications in economics, planning, transportation engineering, public health, etc. The prerequisites are an introductory college/university level course on multivariate statistics, ideally covering the fundamentals of probability theory and hypothesis testing. Working knowledge of multivariate linear regression analysis is a bonus but not strictly required. Requisites This book is not a course to learn R. The language is introduced progressively, and assumes that readers are computer-literate and have possible done some basic coding in the past. For readers who wish to learn R there are other sources such as Wickham and Grolemund (2016)(https://r4ds.had.co.nz/) or Albert and Rizzo (Albert and Rizzo 2012). To fully benefit from this text, up-to-date copies of R and RStudio are highly recommended. There are different packages that implement discrete choice methods in R. I will particularly rely on the packages mlogit and gmnl. References "],
["preliminaries-installing-r-and-rstudio.html", "Chapter 1 Preliminaries: Installing R and RStudio 1.1 Introduction 1.2 Learning Objectives 1.3 R: The Open Statistical Computing Project 1.4 Packages in R", " Chapter 1 Preliminaries: Installing R and RStudio 1.1 Introduction The course makes extensive use of R and RStudio. Here are the instructions to install these two tools. 1.2 Learning Objectives In this reading, you will learn: How to install R. About the RStudio Interactive Development Environment. About packages in R. 1.3 R: The Open Statistical Computing Project 1.3.1 What is R? R is an open-source language for statistical computing. It was created by Ross Ihaka and Robert Gentleman at the University of Auckland, in New Zealand, as a way to offer their students an accessible, no-cost tool for their courses. R is now maintained by the R Development Core Team, and developed by hundreds of contributors around the globe. R is an attractive alternative to other software applications for data analysis (e.g., Microsoft Excel, STATA) due to its open-source character (i.e., it is free), its flexibility, and large and dedicated user community, which means if there’s something you want to do (for instance, linear regression), it is very likely that someone has already developed functionality for it in R. A good way to think about R is as a core package, to which a library, consisting of additional packages, can be attached to increase its functionality. R can be downloaded for free at: https://cran.rstudio.com/ R comes with a built-in console (a user graphical interface), but better alternatives to the basic interface exist, including RStudio, an Integrated Development Environment, or IDE for short. RStudio can also be downloaded for free, by visiting the website: https://www.rstudio.com/products/rstudio/download/ R requires you to work using the command line, which is going to be unfamiliar to many of you accustomed to user-friendly graphical interfaces. Do not fear. People worked for a long time using the command line, or even more cumbersome, punched cards in early computers. Graphical user interfaces are convenient, but they have a major drawback, namely their inflexibility. A program that functions based on graphical user interfaces allows you to do only what is hard-coded in the user interface. Command line, as we will see, is somewhat more involved, but provides much more flexibility in operation. Go ahead. Install R and RStudio in your computer. Before introducing some basic functionality in R, lets quickly take a tour R Studio. 1.3.2 The RStudio IDE The RStudio IDE provides a complete interface to interact with the language R. It consists of a window with several panes. Some panes include in addition several tabs. There are the usual drop-down menus for common operations, such as creating new files, saving, common commands for editing, etc. See Figure 1.1 below. Figure 1.1: The RStudio IDE The editor pane allows you to open and work with text and other files, where you can write instructions that can be passed on to the program. Writing something in the editor does not execute any instructions, it merely records them for possible future use. In fact, much of what is written in the editor will not be instructions, but rather comments, discussion, and other text that is useful to understand code. The console pane is where instructions are passed on to the program. When an instruction is typed (or copied and pasted) there, R will understand that it needs to do something. The instructions must be written in a way that R understands, otherwise errors will occur. If you have typed instructions in the editor, you can use “ctrl-Enter” (in Windows) or “cmd-Enter” (in Mac) to send to the console and execute. The environment is where all data that is currently in memory is reported. The History tab acts like a log: it keeps track of the instructions that have been executed in the console. The last pane includes a number of useful tabs. The File tab allows you to navigate your computer, change the working directory, see what files are where, and so on. The Plot tab is where plots are rendered, when instructions require R to do so. The Packages tab allows you to manage packages, which as mentioned above, are pieces of code that can augment the functionality of R. The Help tab is where you can consult the documentation for functions/packages/see examples, and so on. The Viewer tab is for displaying local web content, for instance, to preview a Notebook (more on Notebooks soon). This brief introduction should have allowed you to install both R and RStudio. The next thing that you will need is packages. 1.4 Packages in R Packages are the basic units of reproducible code in the R multiverse. Packages allow a developer to create a self-contained unit of code that often is meant to achieve some task. For instance, there are packages in R that specialize in statistical techniques, such as cluster analysis, visualization, or data manipulation. Some packages can be miscellaneous tools, or contain mostly datasets. Packages are a very convenient way of maintaining code, data, and documentation, and also of sharing all these resources. Packages can be obtained from different sources (including making them!). One of the reasons why R has become so successful is the relative facility with which packages can be distributed. A package that I use frequently is called tidyverse. The tidyverse is a collection of functions for data manipulation, analysis, and visualization. This package can be downloaded and installed in your personal library of R packages by using the function install.packages, as follows: install.packages(&quot;tidyverse&quot;) The function install.packages retrieves packages from the Comprehensive R Archive Network, or CRAN for short. CRAN is a collection of sites (accessible via the internet) that carry identical materials for distribution for R. "],
["chapter-1.html", "Chapter 2 Data and stuff 2.1 What are models? 2.2 How to use this note 2.3 Learning objectives 2.4 Suggested readings 2.5 Ways of measuring stuff 2.6 Importing data 2.7 Data Classes in R 2.8 More on indexing and data manipulation 2.9 Visualization 2.10 Exercise", " Chapter 2 Data and stuff “Essentially, all models are wrong, but some are useful.” — George E.P. Box “You can have data without information, but you cannot have information without data.” — Daniel Keys Moran 2.1 What are models? Model building requires three things: Raw materials. Tools. Technical expertise (hopefully!). This is true whether the model is physical (for instance a sculpture), conceptual (a mental map), or statistical/mathematical (the gravity model or a regression model). In the case of a sculpture, the raw materials can be marble, wood, or clay; the tools chisels, mallet, and spatula; and the technique the mastery of the sculptor when working with the tools and the materials. Anyone can try sculpture, and most people can create sculptures. These kind of models are evaluated by their aesthetic value, not necessarily their usefulness. But if the scultpure is poorly balanced and falls and breaks, then its value is limited by its structural integrity - the skill of the sculptor matters even if only in this sense. In the case of a mental map, the raw materials are ideas, the tools are a drawing surface and tools for writing, or maybe an app, and the technical expertise is the ability of the modeler to organize ideas in a useful way. There are useful conceptual models, and conceptual models that are anything but. Figure 2.1 shows two example of conceptual models. Figure 2.1: Two Examples of Conceptual Models In the case of mathematical/statistical models, the raw materials are data; the tools are descriptive statistics and statistical plots, and various forms of regression analysis; and the technical expertise is the ability of the modeler to select tools that are appropriate to the data, and to convince the data to “speak”: in other words, to extract information from the data. As Moran said in the aphorism quoted at the top of this section: you can have data without information, but no information without data. Technical mastery is the degree to which a modeller can obtain information from data that is useful, accurate, and precise, to the extent that the raw materials permit. Before moving on to the technical skills required for modeling, it is important to understand the raw materials and the tools. The objective of this note is to introduce some important concerning data and data manipulation, and some useful tools. 2.2 How to use this note The source for the document you are reading is an R Notebook. Notebooks are a form “literate programming”, a style of document that uses code to illustrate a discussion, as opposed to the traditional programming style that uses natural language to discuss/document the code. It flips around the usual technical writing approach to make it more intuitive and accessible. Throughout the notes, you will find examples of code in segments of text called chunks. This is an example of a chunk: print(&quot;Hello, Discrete Choice Analysis!&quot;) ## [1] &quot;Hello, Discrete Choice Analysis!&quot; If you are working with the Notebook version of the document, you can run the code by clicking the ‘play’ icon on the top right corner of the chunk. If you are reading the web-book version of the document, you will often see that the code has already been executed. You can still try it by copying and pasting into your R or RStudio console. Whichever way you are working, you might want to give it a try now! You will see that the chunk of code above instructed R (and trough R the computer) to print (or display on the screen) some text. 2.3 Learning objectives In this practice, you will learn about: Different ways to measure stuff. Basic operations in R. Data classes, data types, and data transformations. The use of packages in R. Basic visualization. 2.4 Suggested readings Grolemund, G., Wickham, H. (2016) R for Data Science, Chapters 3-5, O’Reilly Media. Wickham, H. (2016) ggplot2: Elegant Graphics for Data Analysis, Chapters 2-3, Springer: New York. 2.5 Ways of measuring stuff Previously we said that data are the raw material for modeling, but we did not say precisely what we meant by ‘data’. You probably already have a working understanding of what ‘data’ means, but nonetheless lets begin with a definition. According to Mirriam-Webster, data are: factual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation As an aside, it is interesting to note that Tukey’s classic Exploratory Data Analysis (Tuckey 1977) does not define ‘data’ in the glossary! Measurement theory is a branch of mathematics concerned with observing the facts about something. It is important to note that measurements are not the same as the thing being measured; however, we would like the measurements to be a reasonably close approximation of the thing being measured - otherwise the measurements might be pretty useless, an inadequate way to learn anything valuable from the thing we are measuring. One fundamental contribution of the scientific method has been to produce standardized ways of measuring things. How would you measure the following things? The temperature at which water freezes. The tempreature at which nitrogen freezes. The length of a trip. Blood donations. Different brands of peanut butter. The value of two bedroom apartment. Someone’s opinion regarding taxes. Generally, there are multiple ways of measuring something, but not all of them are necessarily appropriate, partly because the scales of measurement may result in some loss of information. The interpretation of a measurement, as well, depends on what the scale is. Two broad scales of measurement are as follows: 2.5.1 Categorical Categorical measurements assign a label or category to the thing being measured. For example, a way to measure different brands of peanut butter could be to measure their sugar content, their fat content, their consistency, and so on, and in this way describe what makes each brand unique. A different way to do this would be to label one brand “Spooky” and another “Peter’s”. This has the effect of reducing a lot of information to a much simpler category. Is this loss of information inappropriate? Well, it really depends on what is the intended use of data! Categorical measurements are interesting because they may tell us something about the power of brands! Within the class of categorical variables there are two distinct scales of measurement: Nominal scale. When the categories do not follow a natural order. For example, there is no reason to say that “Spooky” brand precedes “Peter’s” or vice versa. Similarly, when measuring modes of travel “walking” is not intrinsically higher or lower or better or worse than “cycling” or “riding bus”. Ordinal scale. When the categories follow a natural or logical sequence. A common way of measuring opinions is by means of the Likert scale, which classifies responses for instance as “strongly disagree”, “disagree”, “neutral”, “agree”, “strongly agree”. In this case, it is sensible to order the responses, since “strongly agree” is probably closer to “agree” than to “strongly disagree”. Responses of this type are often represented by numbers, say, from 1 to 5. It is a mistake to treat the measurements as numbers instead of lables. When treated as numbers there is a temptation to thing of the difference between 4 and 5 and the difference between 3 and 4 as being equivalent, when in fact the strength of disagreement could be stronger than the strength of agreement. In other words, the interval between “strongly disagree” and “disagree” may not be the same as “agree” and “strongly agree”. With ordinal scales we do not know that, all that we know is that they measure a different opinion. Sometimes, different measurement scales might represent different behavioral mechanisms, as Bhat and Pulugurta discuss in their comparison of categorical and ordinal measurements for vehicle ownership (Bhat and Pulugurta 1998). 2.5.2 Quantitative Quantitative measurements assign a number to an attribute, and the number quantifies the presence of the attribute. Within this class of variables, there are also two ways of measuring things. Interval scale. A quantity can be assigned to an attribute, the values follow an order, and their differences can be computed and remain constant. Temperature is typically measured in interval scale. The difference between \\(10\\,^{\\circ}\\mathrm{C}\\) and \\(11\\,^{\\circ}\\mathrm{C}\\) is the same as the difference between \\(25\\,^{\\circ}\\mathrm{C}\\) and \\(26\\,^{\\circ}\\mathrm{C}\\). The intervals are meaningful. However, \\(0\\,^{\\circ}\\mathrm{C}\\) does not imply the absence of temperature! Which is why measurements in Celsius and Farenheit do not coincide at zero. The lack of a natural zero for these scales means that the ratios between two values are not meaningful: \\(4\\,^{\\circ}\\mathrm{C}\\) is not twice as hot as \\(2\\,^{\\circ}\\mathrm{C}\\), and \\(-12\\,^{\\circ}\\mathrm{C}\\) is not four times as cold as \\(-3\\,^{\\circ}\\mathrm{C}\\). Ratio scale. When there is an absolute value of zero to the thing being measured (to indicate absence!), attributes can be measured in a ratio scale. This combines the features of the previous scales of measurement: a number is esentially a label that follows a logical order and with differences that are meaningful. In addition to that, the ratios of variables are meaningful. For example, twenty dollars are twice as valuable as ten, and zero is the absence of value. Weight is a way of measuring mass, and zero is the absence of mass. Two hundred kilograms is twice as much as one hundred kilograms. It is important to understand the different scales of measurement to be able to choose the appropriate tools for each. More on this below. But first, lets bring some actual data to play with. 2.6 Importing data There are several different ways of importing data in R. For this example, we will use part of a dataset that was analyzed by Whalen et al. (2013). At the very beginning, it is good practice to clear the workspace, to ensure that there are no extraneous items there. The workspace is where objects reside in memory during a session with R. The function for removing variables from the workspace is rm(). Another useful function is `ls, which retrieves a list of things in the workspace. So essentially we are asking R to remove all things in the workspace: rm(list = ls()) Once that the workspace is empty, we can proceed to load a few packages that are useful. Packages are the basic units of reproducible code in the R multiverse. Packages allow a developer to create a self-contained unit of code that often is meant to achieve some task. For instance, there are packages in R that specialize in statistical techniques, such as cluster analysis, visualization, or data manipulation. Some packages can be miscellaneous tools, or contain mostly datasets. Packages are a very convenient way of maintaining code, data, and documentation, and also of sharing all these resources. Packages can be obtained from different sources (including making them!). One of the reasons why R has become so successful is the relative facility with which packages can be distributed. A package that I use frequently is called tidyverse. The tidyverse is a collection of functions for data manipulation, analysis, and visualization. This package can be downloaded and installed in your personal library of R packages by using the function install.packages, as follows: install.packages(&quot;tidyverse&quot;) The function install.packages retrieves packages from the Comprehensive R Archive Network, or CRAN for short. CRAN is a collection of sites (accessible via the internet) that carry identical materials for distribution for R. Installing a package is similar to acquiring a book for your library. The book is there, but if you want to use it, you need to bring it to your workspace, so to speak. The function for retrieving a package from the library is naturally enough library(). For the moment, we need the following packages. If you have not done so, take a moment to install them, as illustrated in the previous chunk. library(tidyverse) library(readr) #library(caret) library(mlogit) Ocassionally there are messages displayed when loading a package. These messages are informative (they ask you to cite them in a certain style) or may give you warnings, for instance that identically named functions exist in several packages. The function that we need to read the sample dataset is read_csv(), which is part of the tidyverse package. Note that you can name the value (or output) of a function by using &lt;-. In this case, we wish to read an external file, and assign the results to an object called mc_mode_choice: mc_mode_choice &lt;- read_csv(&quot;Commute Mac.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_double() ## ) ## See spec(...) for full column specifications. It is possible to quickly examine the contents of the object by means of the function head(), which prints the top few rows of the object, if appropriate. For example: head(mc_mode_choice) ## # A tibble: 6 x 39 ## RespondentID choice avcycle avwalk avhsr avcar timecycle timewalk ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 566872636 3 0 1 1 0 6.21 21.3 ## 2 566873140 3 0 1 1 1 3.73 12.8 ## 3 566874266 3 0 0 1 1 100000 100000 ## 4 566874842 2 1 1 1 0 5.83 20 ## 5 566881170 2 1 1 1 0 5.83 20 ## 6 566907438 2 0 1 1 0 100000 10 ## # ... with 31 more variables: accesshsr &lt;dbl&gt;, waitingtimehsr &lt;dbl&gt;, ## # transfer &lt;dbl&gt;, timehsr &lt;dbl&gt;, timecar &lt;dbl&gt;, parking &lt;dbl&gt;, ## # vehind &lt;dbl&gt;, owncycle &lt;dbl&gt;, gender &lt;dbl&gt;, work &lt;dbl&gt;, visa &lt;dbl&gt;, ## # age &lt;dbl&gt;, solo &lt;dbl&gt;, shared &lt;dbl&gt;, family &lt;dbl&gt;, child &lt;dbl&gt;, ## # primary_caregiver &lt;dbl&gt;, LAT &lt;dbl&gt;, LONG &lt;dbl&gt;, DAUID &lt;dbl&gt;, ## # mhi &lt;dbl&gt;, dwell_den &lt;dbl&gt;, lum &lt;dbl&gt;, st_den &lt;dbl&gt;, inter_den &lt;dbl&gt;, ## # SF_P_ratio &lt;dbl&gt;, side_den &lt;dbl&gt;, Shelters_SD &lt;dbl&gt;, Shelters_D &lt;dbl&gt;, ## # Shelters_A &lt;dbl&gt;, Shelters_SA &lt;dbl&gt; Here we can see that what we just read is a table with several variables: an id, a variable called choice, some variables for time, etc. Hopefully, when reading data there is also a metadata file, a data dictionary or something that defines what the data are. For example, what does it mean for choice to be “3” or “1”? Was time measured in hours, seconds, minutes, or something else? These variables will be described below. Before that, however, we can use a different function to get further insights into the contents of the table by means of the summary() function: summary(mc_mode_choice) ## RespondentID choice avcycle avwalk ## Min. :566872636 Min. :1.000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:567814188 1st Qu.:2.000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :568682048 Median :2.000 Median :0.0000 Median :1.0000 ## Mean :570566454 Mean :2.618 Mean :0.2747 Mean :0.6613 ## 3rd Qu.:574925212 3rd Qu.:3.000 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :587675235 Max. :4.000 Max. :1.0000 Max. :1.0000 ## avhsr avcar timecycle timewalk ## Min. :0.0000 Min. :0.0000 Min. : 0.29 Min. : 1.00 ## 1st Qu.:1.0000 1st Qu.:0.0000 1st Qu.: 3.79 1st Qu.: 13.66 ## Median :1.0000 Median :1.0000 Median : 5.83 Median : 20.00 ## Mean :0.9608 Mean :0.5472 Mean : 34014.86 Mean : 37364.73 ## 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:100000.00 3rd Qu.:100000.00 ## Max. :1.0000 Max. :1.0000 Max. :100000.00 Max. :100000.00 ## accesshsr waitingtimehsr transfer timehsr ## Min. : 0.00 Min. : 0.00 Min. : 0 Min. : 1.00 ## 1st Qu.: 2.48 1st Qu.:10.23 1st Qu.: 0 1st Qu.: 4.75 ## Median : 6.21 Median :10.23 Median : 0 Median : 10.00 ## Mean :11.06 Mean :10.25 Mean : 3925 Mean : 3940.57 ## 3rd Qu.:12.42 3rd Qu.:10.23 3rd Qu.: 1 3rd Qu.: 25.00 ## Max. :62.11 Max. :50.00 Max. :100000 Max. :100000.00 ## timecar parking vehind owncycle ## Min. : 1 Min. :0.00000 Min. :0.0000 Min. :0.0000 ## 1st Qu.: 8 1st Qu.:0.00000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median : 30 Median :0.00000 Median :0.0000 Median :0.0000 ## Mean : 45283 Mean :0.08358 Mean :0.2565 Mean :0.4513 ## 3rd Qu.:100000 3rd Qu.:0.00000 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :100000 Max. :1.00000 Max. :1.0000 Max. :1.0000 ## gender work visa age ## Min. :0.0000 Min. :0.000 Min. :0.0000 Min. :17.00 ## 1st Qu.:0.0000 1st Qu.:0.000 1st Qu.:1.0000 1st Qu.:20.00 ## Median :0.0000 Median :0.000 Median :1.0000 Median :21.00 ## Mean :0.4012 Mean :0.492 Mean :0.9622 Mean :22.08 ## 3rd Qu.:1.0000 3rd Qu.:1.000 3rd Qu.:1.0000 3rd Qu.:23.00 ## Max. :1.0000 Max. :1.000 Max. :1.0000 Max. :60.00 ## solo shared family child ## Min. :0.0000 Min. :0.000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:0.000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :0.0000 Median :1.000 Median :0.0000 Median :0.0000 ## Mean :0.1272 Mean :0.625 Mean :0.2478 Mean :0.2115 ## 3rd Qu.:0.0000 3rd Qu.:1.000 3rd Qu.:0.0000 3rd Qu.:0.0000 ## Max. :1.0000 Max. :1.000 Max. :1.0000 Max. :1.0000 ## primary_caregiver LAT LONG DAUID ## Min. : 0 Min. :43.08 Min. :-80.09 Min. :35250031 ## 1st Qu.:100000 1st Qu.:43.25 1st Qu.:-79.92 1st Qu.:35250540 ## Median :100000 Median :43.26 Median :-79.91 Median :35250670 ## Mean : 75218 Mean :43.25 Mean :-79.90 Mean :35250612 ## 3rd Qu.:100000 3rd Qu.:43.26 3rd Qu.:-79.90 3rd Qu.:35250677 ## Max. :100000 Max. :43.28 Max. :-79.64 Max. :35250970 ## mhi dwell_den lum st_den ## Min. : 0.000 Min. : 0.0 Min. :0.0000 Min. : 0.00 ## 1st Qu.: 4.577 1st Qu.: 488.7 1st Qu.:0.2808 1st Qu.:10.36 ## Median : 5.491 Median : 950.0 Median :0.4501 Median :14.29 ## Mean : 6.168 Mean : 1373.0 Mean :0.4183 Mean :13.27 ## 3rd Qu.: 7.556 3rd Qu.: 1688.6 3rd Qu.:0.5038 3rd Qu.:16.18 ## Max. :14.595 Max. :45209.9 Max. :0.9081 Max. :25.22 ## inter_den SF_P_ratio side_den Shelters_SD ## Min. : 0.00 Min. :0.0000 Min. : 0.00 Min. :0.00000 ## 1st Qu.: 25.82 1st Qu.:0.2309 1st Qu.:18.19 1st Qu.:0.00000 ## Median : 41.04 Median :0.2709 Median :22.63 Median :0.00000 ## Mean : 52.09 Mean :0.2625 Mean :24.18 Mean :0.04433 ## 3rd Qu.: 73.08 3rd Qu.:0.3134 3rd Qu.:35.70 3rd Qu.:0.00000 ## Max. :645.86 Max. :0.8808 Max. :59.41 Max. :1.00000 ## Shelters_D Shelters_A Shelters_SA ## Min. :0.0000 Min. :0.0000 Min. :0.00000 ## 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.00000 ## Median :0.0000 Median :0.0000 Median :0.00000 ## Mean :0.2289 Mean :0.3576 Mean :0.02762 ## 3rd Qu.:0.0000 3rd Qu.:1.0000 3rd Qu.:0.00000 ## Max. :1.0000 Max. :1.0000 Max. :1.00000 This function will print a set of summary statistics for the variables in the table. The statistics are appropriate for the scale of measurement of the variable - that is, the way the variable is coded. Presently, all summary statistics are calculated for quantitative variables. We don’t know if this makes sense, until we know what the variables are supposed to measure. For example, the variable choice measures the use of one mode of transportation. There are four values in this scale: 1 through 4, with each indicating one of “Cycle”, “Walk”, “Car”, or “HSR” (the local transit agency in Hamilton, ON, Canada, where these data were collected). Check the results of the summary. What does it mean to say that the mean of choice is 2.618? Does this number make sense? 2.7 Data Classes in R To understand why 2.618 of mode of transportation is not an appropriate summary measure for the variable mode, we need to know that R can work with different data classes, which include the following: Numerical Character Logical Factor The ability to store information in different forms is important, because is allows R to distinguish what kind of operations are appropriate for a certain variable. Consider the following example (using indexing): mc_mode_choice$choice[1] - mc_mode_choice$choice[4] ## [1] 1 Lets unpack what the chunk above did. First, we call our table mc_mode_choice. The string sign $ is used to reference columns in the table. Therefore, we asked R to go and look up the column choice in the table mc_mode_choice. Finally, the value between square brackets [] asks R to retrieve a value out of the column, in this example the first value in that column and then the fourth value. This system of referring to elements in tables is called indexing. Most computer languages use it, but the syntax is different. Again: $ refers to a column, and [] is used to call values in that column. As we can see, the difference between the two values retrieved is \\(1\\). But what is the meaning of “cycle” minus “walk”, for instance? In reality, the variable choice was measured as a nominal variable: it just corresponds to a label indicating what mode was chosen by a respondent. But R does not know this. Before R can treat it as a nominal variable, the numbers need to be converted to a factor. Factors are the way R stores categorical variables (both nominal and ordinal). To convert the variable choice to a factor, we use the factor() function: mc_mode_choice$choice &lt;- factor(mc_mode_choice$choice, labels = c(&quot;Cycle&quot;, &quot;Walk&quot;, &quot;HSR&quot;, &quot;Car&quot;)) In the chunk above, we ask R to replace the contents of mc_mode_choice$choice with the value (output) of the function factor. Factor takes the contents of mc_mode_choice$choice and converts to a factor with labels as indicated by the argument labels = (the function c() is used to concatenate several values). Lets summarize the result, by using the summary() function but only for this variable: summary(mc_mode_choice$choice) ## Cycle Walk HSR Car ## 48 711 336 281 Now the summary is appropriate categorical variable, and is a table of frequencies: as seen there, there were 48 respondents who chose “Cycle”, 711 who chose “Walk”, and so on. What if we tried to calculate the difference? mc_mode_choice$choice[1] - mc_mode_choice$choice[4] ## Warning in Ops.factor(mc_mode_choice$choice[1], mc_mode_choice$choice[4]): ## &#39;-&#39; not meaningful for factors ## [1] NA The message indicates that the operation we tried to perform is not meaningful for factors. As long as R knows the appropirate measurement scale for your variables, it will try to steer you away from doing silly things with them. Other variables included in this table are for time. These variables measure the duration in minutes (actual or imputed) for trips by different modes. For example, timecycle is the duration of a trip by bicycle for the journey reported by the respondent. Lets summarize this variable again: summary(mc_mode_choice$timecycle) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.29 3.79 5.83 34014.86 100000.00 100000.00 Notice that the shortest trip by bicycle would be less than a minute long, whereas the maximum is \\(100,000\\) minutes long. Wait, what? That is over \\(1,600\\) hours long. Is that even possible? In fact, no, it is not. The reason for these values is that when the original data were coded, whenever a respondent said that cycling was not a mode that was available to them, the time was coded as a very large and distinctive value. There were no trips taking \\(100,000\\) minutes, this is just a code for “information not available”. One problem with this manner of coding is that R does not know that the information is actually missing, but rather thinks it is a legitimate quantity. As a consequence, the mean is tens of thousands of minutes, despite the fact that half of all trips by bicycle were measured at less than 6 minutes long (see the median). Next we will see a way to address this. One last thing before doing so: you can check the class an object with the function class class(mc_mode_choice$choice) ## [1] &quot;factor&quot; class(mc_mode_choice$timecycle) ## [1] &quot;numeric&quot; 2.8 More on indexing and data manipulation Indexing is a way of making reference to elements in a data object. There are numerous indexing methods in R that are appropriate for specific objects. Tables such as mc_mode_choice (called data frames) can be indexed in a few different ways. For example the next three chunks are equivalent in that they call the second column (choice) and in that column the second element: mc_mode_choice[2, 2] ## # A tibble: 1 x 1 ## choice ## &lt;fct&gt; ## 1 HSR mc_mode_choice$choice[2] ## [1] HSR ## Levels: Cycle Walk HSR Car mc_mode_choice[[&quot;choice&quot;]][2] ## [1] HSR ## Levels: Cycle Walk HSR Car It is also possible to index by ranges of values. For example, the next chunks retrieves rows 2 to 5 from columns 7 and 8: mc_mode_choice[2:5, 7:8] ## # A tibble: 4 x 2 ## timecycle timewalk ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3.73 12.8 ## 2 100000 100000 ## 3 5.83 20 ## 4 5.83 20 Indexing is useful to subset data selectively. For example, we know that travel times coded as \\(100,000\\) are actually cases where the corresponding mode was not available. Lets say that we wanted to summarize travel time by bicycle but without those cases. We can use logical statements when indexing. We could tell R to retrieve only those values that meet a certain condition. In the next chunk, we save the results of this to a new variable: time.Cycle.clean &lt;- mc_mode_choice$timecycle[mc_mode_choice$timecycle != 100000] where != is R for not. In other words, “find all values not 100000, and retrieve them”. The result of this is a numeric object: class(time.Cycle.clean) ## [1] &quot;numeric&quot; If we summarize this object now: summary(time.Cycle.clean) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.2914 2.9141 4.3711 4.8957 5.8282 45.0000 The summary statistics are much more sensible: the longest trip by bicycle was measured at 45 minutes, and the mean trip at less than 5 minutes. Indexing is a powerful technique, but can be cumbersome (mc_mode_choice$timecycle[mc_mode_choice$timecycle != 100000]!). The package dplyer (part of the tidyverse) provides a grammar for data manipulation that is more intuitive. We will explore three of its elements here, namely the pipe operator (%&gt;%), select, and filter. Suppose that we wanted to select two of the time variables, for cycling and walking, and wanted to retrieve only values other than the offending \\(100,000\\), and save these values in a new object called time.Active.clean. In the grammar of dplyr, this is done as follows: time.Active.clean &lt;- mc_mode_choice %&gt;% select(c(&quot;timecycle&quot;, &quot;timewalk&quot;)) %&gt;% filter(timecycle != 100000 &amp; timewalk != 100000) In natural language this would be something like “take mc_mode_choice and select columns timecycle and timewalk; pass the result to filter and retrieve all rows that meet the conditions timecycle != 100000 AND timewalk != 100000”. The verb select is used to select columns from a data frame, and the verb filter to filter rows. The alternative, using indexing would look something like this: time.Active.clean.the.hard.way &lt;- mc_mode_choice[mc_mode_choice$timecycle != 100000 &amp; mc_mode_choice$timewalk != 100000, 7:8] The expression becomes more convoluted and not as easy to read. It is also easier to make mistakes when writing it. Compare the summaries of the two data frames, to make sure that they are identical: summary(time.Active.clean) ## timecycle timewalk ## Min. : 0.2914 Min. : 1.00 ## 1st Qu.: 2.9141 1st Qu.:10.00 ## Median : 4.3711 Median :15.00 ## Mean : 4.5852 Mean :16.10 ## 3rd Qu.: 5.8282 3rd Qu.:20.00 ## Max. :17.4845 Max. :62.11 summary(time.Active.clean.the.hard.way) ## timecycle timewalk ## Min. : 0.2914 Min. : 1.00 ## 1st Qu.: 2.9141 1st Qu.:10.00 ## Median : 4.3711 Median :15.00 ## Mean : 4.5852 Mean :16.10 ## 3rd Qu.: 5.8282 3rd Qu.:20.00 ## Max. :17.4845 Max. :62.11 The grammar of data manipulation in dplyr is a powerful way of working with data in an intuitive way. We will find other aspects of this, but for the time being you are welcome to consult more about dplyr here 2.9 Visualization The last item in this section is related to visualization. Humans are very much visual creatures, and much can be learned from seeing the data. For example, the data frame, in essence a table, is informative in many ways, but not particularly conducive to observe trends or regularities in the data. The summary statistics are also informative, but partial, and do not convey information to the same effect as a statistical plot. Take the following list of summary statistics: summary(time.Active.clean) ## timecycle timewalk ## Min. : 0.2914 Min. : 1.00 ## 1st Qu.: 2.9141 1st Qu.:10.00 ## Median : 4.3711 Median :15.00 ## Mean : 4.5852 Mean :16.10 ## 3rd Qu.: 5.8282 3rd Qu.:20.00 ## Max. :17.4845 Max. :62.11 Now, compare to the following plot: ggplot(data = time.Active.clean) + geom_area(aes(x = timecycle), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;blue&quot;, color = &quot;blue&quot;, alpha = 0.6) + geom_area(aes(x = timewalk), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;yellow&quot;, color = &quot;yellow&quot;, alpha = 0.6) The plot above was created using a package called ggplot2, also part of tidyverse. This package implements a grammar of graphics, and offers a very flexible way of creating plots in R. ggplot2 works by layering a series of objects, beginning with a blank plot, to which we can add things. The command to create a plot is ggplot(). This command accepts different arguments. For instance, we can pass data to it in the form of a data frame. We can also indicate different aesthetic values, that is, the things that we wish to plot. None of this is plotted, though, until we indicate which kind of geom or geometric object we wish to plot. For instance, you can see above that to create the figure we used geom_area. This geom is essentially a smoothed histogram. Lets break down these instructions. First we ask ggplot2 to create a plot that will use the data frame time.Active.clean. We will name this object p: p &lt;- ggplot(data = time.Active.clean) Notice how ggplot2 creates a blank plot, but it has yet to actually render any of the population information in there: p We have yet to tell ggplot2 what the x axis is, what the y axis is, what should be plotted in the x axis, and so on. We layer elements on a plot by using the + sign. It is only when we tell the package to add some geometric element that it renders something on the plot. In the previous case, we told ggplot2 to use the geom_area to create a smoothed histogram. Next, we need to indicate which aes (short for aesthetics) we wish to plot. The aesthetics map aspect of the dataset to the plot. For instance, by saying that the aesthetics include something for x, we tell ggplot2 that that something should be mapped to the x axis (in this case one of the time variables in the data frame). The second argument is stat = 'bin', which indicates that there is a statistical operation that happens, namely the data are binned for smoothing (small bins lead to less smoothing, large bins lead to more smoothing; try it!). After playing around with a few bin values, I selected 5: p + geom_area(aes(x = timecycle), stat = &quot;bin&quot;, binwidth = 5) To improve the appearence of the plot, we also asked that the geom be rendered using a named color (blue for the color of the line, and also blue for the fill), and that it be transparent (the argument alpha controls opacity; a value of zero is transparent, a value of 1 is solid): p + geom_area(aes(x = timecycle), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;blue&quot;, color = &quot;blue&quot;, alpha = 0.6) The final plot was obtained by layering a second geom (in yellow) for a different variable (time by walking), so that we could compare them: ggplot(data = time.Active.clean) + geom_area(aes(x = timecycle), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;blue&quot;, color = &quot;blue&quot;, alpha = 0.6) + geom_area(aes(x = timewalk), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;yellow&quot;, color = &quot;yellow&quot;, alpha = 0.6) Notice that the x axis is labeled as “timecycle” despite the fact that the plot also includes time by walking. This can be fixed by changing the label as follows: ggplot(data = time.Active.clean) + geom_area(aes(x = timecycle), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;blue&quot;, color = &quot;blue&quot;, alpha = 0.6) + geom_area(aes(x = timewalk), stat = &quot;bin&quot;, binwidth = 5, fill = &quot;yellow&quot;, color = &quot;yellow&quot;, alpha = 0.6) + xlab(&quot;Time (in minutes)&quot;) What do we learn from this plot? Would it have been possible to learn the same from the summary statistics? Which was more effective, the plot or the summary statistics? The plot above is an example of a univariate plot, since it is created to display the distribution of a single variable, not the way two or more variables relate. Imagine now that you would like to see how mode choice and sidewalk density at the place of residence relate. An appropriate statistical plot for two variables, one of which is nominal (choice) and another that is continuous (side_den), is the boxplot. Before creating the plot, lets summarize these two variables (notice the use of the pipe operator): mc_mode_choice %&gt;% select(c(&quot;choice&quot;, &quot;side_den&quot;)) %&gt;% summary() ## choice side_den ## Cycle: 48 Min. : 0.00 ## Walk :711 1st Qu.:18.19 ## HSR :336 Median :22.63 ## Car :281 Mean :24.18 ## 3rd Qu.:35.70 ## Max. :59.41 Sidewalk density is measured in \\(km/km^2\\). Lets create the boxplot next. We begin by defining a ggplot2 object with the data frame and aesthetics that we wish to use. In this case, we want to plot the categorical variable in the x axis and the quantitative variable in the y axis: ggplot(data = mc_mode_choice, aes(x = choice, y = side_den)) + geom_boxplot() What do we learn from this plot? Could we have derived a similar insight from the summary statistics? There are many different geoms that can be used in ggplot2. You can always consult the help/tutorial files by typing ??ggplot2 in the console. See: ??ggplot2 You can also check the ggplot2 Cheat Sheet for more information on how to use this package. A last note. Many other visualization alternatives (for instance, Excel) provide point-and-click functions for creating plots. In contrast, ggplot2 in R requires that the plot be created by meticulously instructing the package what to do. While this is more laborious, it also means that you have complete control over the creation of plots, which in turn allows you to create more flexible and creative visuals. 2.10 Exercise 2.10.1 Questions Define “model”. Why are models on a 1-to-1 scale undesirable? Invoke dataset Mode from package mlogit. To do this you need to first load the package. Once you have done so, usage of datasets is as follows: data(&quot;Mode&quot;) This is a dataset with choices about mode of transportation. Describe this dataset. How many variables are there and of which type (i.e., categorical/quantitative)? How many different modes of transportation are in this data set? What is the most popular mode? What is the least popular mode? In general, what is the most expensive mode? The least expensive? Create a plot showing the univariate distributions of time by car and time by bus. Discuss. How do choices relate to cost by the different modes? References "],
["chapter-2.html", "Chapter 3 Fundamental concepts 3.1 Why modelling choices? 3.2 How to use this note 3.3 Learning objectives 3.4 Suggested readings 3.5 Preliminaries 3.6 Utility maximization 3.7 What about those random terms? 3.8 Probability distribution functions (PDFs) 3.9 A simple random utility discrete choice model 3.10 Other choice mechanisms 3.11 Exercise", " Chapter 3 Fundamental concepts “Ignorance gives one a large range of probabilities.” — George Eliot 3.1 Why modelling choices? In Chapter 2 we discussed in a general fashion the use of models. There, we argued that modelling is an activity that helps us to isolate in a systematic way certain aspects of a process or thing, by way of abstraction and generalization. There are many kinds of models: analog (like sculptures, maquettes, scale models), conceptual (like mental maps), and mathematical/statistical models. The raw materials of mathematical/statistical models are observations about the process or thing of interest, usually measurements that provide data. The tools are the statistical and mathematical techniques used to convert data into information. And the technical expertise is the knowledge and ability of the modeler to use the appropriate tools to the data, in order to extract as much information as possible, given the characteristics of the data and the process or thing. Modelling choices is simply a specialized field in the much broader field of mathematical and statistical modelling. The task of modelling choices is in many way similar to the modelling of limited-dependent and qualitative variables in statistics (Maddala 1983), however it is distinguished from models in that field by a strong behavioral foundations. Indeed, where statistical models deal with probabilities of an item of interest being in a certain state, choice modelling deals with the probability of an agent choosing an alternative. This is a subtle but important difference that we will highlight in due course. For the time being, it is sufficient to say that to model choices we need a conceptual model first on which to build the rest of the apparatus required for applied choice modelling. Before delving into the technical details, we can pause for a philosophical moment to think about human behavior and decision-making. There are different perspectives on human behavior. Some schools of thought affirm that events are predetermined. A famous thought experiment Laplace’s Demon, is as follows: We may regard the present state of the universe as the effect of its past and the cause of its future. An intellect [Laplace’s Deman] which at a certain moment would know all forces that set nature in motion, and all positions of all items of which nature is composed, if this intellect were also vast enough to submit these data to analysis, it would embrace in a single formula the movements of the greatest bodies of the universe and those of the tiniest atom; for such an intellect nothing would be uncertain and the future just like the past would be present before its eyes. — Pierre Simon Laplace, A Philosophical Essay on Probabilities Some schools of sociological thought (see for example the discussion in Degenne and Fors‚ 1999) see social interactions as a predominant, and even a determinant, factor that affects behavior. In their more extreme form, structuralism views social networks as structures that limit the ability of the individual to exercise independent agency, and therefore determine behavior. Laplace’s Demon and other forms of causal determinism assume that all preceding events set the conditions for present and future events via immutable rules. Nowadays determinism is not seriously considered for several reasons, of which it is useful to highlight two: The practical impossibility of knowing at a certain moment all forces that set nature in motion, as well as the positions of all of nature’s items. With respect to physical processes, the uncertainty principle of quantum physics put paid to the notion that we can know all that there is to know about the fundamental items of nature. In terms of human behavior, this is complicated by the inability of an external observer to know the state of mind of a person who acts. On the other hand, it is possible that existing social structures influence behavior (and there is now a wealth of literature that makes this argument; see A. Páez and Scott 2007). However, social determinism seems as implausible as physical determinism, for similar reasons: the difficulties of knowing the state of a system with complete omniscience. The assumption of immutable rules. This assumption has been challenged by studies that suggest some important physical constants can change with the age of the universe (Webb et al. 2001). In terms of human behavior, the assumption is even more problematic, if for no other reason that humans can in general act in a contrarian way simply to demonstrate that there are no immutable social rules. This is only one of many reasons why behavioral detection (for instance, in airports; Kirschenbaum 2013) is problematic: if one knows the rules used for profiling, acting otherwise renders profiling ineffective. Does this mean that the state of the universe is not determined by the past? Not at all. For all we know, from the perspective of a hypothetical all-knowing being, it is. However, in practical terms, and for the reasons described briefly above, we humble non-all-knowing beigns, cannot rely on determinism for making statements about the state of the universe. In particular, we will make a distinction that is useful as part of developing a conceptual model of choice-making: 1) that there is an observer who typically lacks all relevant information about a choice process (let alone about the state of the universe); and 2) that the rules of decision making are not completely known and/or humans can, for idiosincratic reasons, alter them at whim. 3.2 How to use this note Remember that the source for the document you are reading is an R Notebook. Throughout the notes, you will find examples of code in segments of text called chunks. This is an example of a chunk: print(&quot;Hola, Juan de Dios!&quot;) ## [1] &quot;Hola, Juan de Dios!&quot; If you are working with the Notebook version of the document, you can run the code by clicking the ‘play’ icon on the top right corner of the chunk. If you are reading the web-book version of the document, you will often see that the code has already been executed. You can still try it by copying and pasting into your R or RStudio console. 3.3 Learning objectives In this practice, you will learn about: Choice mechanisms: Utility maximization. Probabilities and integration. How to derive a simple choice model. Other choice mechanisms. 3.4 Suggested readings Ben-Akiva, M. Lerman, (1985) Discrete Choice Analysis: Theory and Applications to Travel Demand, Chapter 3, MIT Press. Hensher, D.A., Rose, J.M., Greene, W.H (2005) Applied Choice Analysis: A Primer, Chapter 3, Cambridge University Press. Louviere, J.J., Hensher, D.A., Swait, J.D. (2000) Stated Choice Methods: Analysis and Application, Chapter 1, Cambridge University Press. Ortuzar JD, Willumsen LG (2011) Modelling Transport, Fourth Edition, Chapter 7, John Wiley and Sons. 3.5 Preliminaries Load the packages used in this section: library(tidyverse) library(evd) 3.6 Utility maximization We will begin by defining a conceptual model of choice based on neo-classical economics, fundamentally consumer choice. The conceptual framework in neo-classical economics is based on the concept of utility. What is utility? In simple terms, utility is a summary indicator of the pleasure, usefulness, enjoyment, or attractiveness associated with making a choice (for instance, buying a new phone). Lets begin by positing a very simple choice situation, in which a decision maker chooses between one of two different alternatives. These alternatives constitute the choice set, and provide the context for the decision-making process. Imagine then that this simple choice example is as follows: Alternative 1: Do nothing (keep using current phone) Alternative 2: Buy new phone (for simplicity, think of a generic model). Further, assume that each alternative can be described by means of a vector of attributes \\(X\\) as follows: \\[ X = [x_1, x_2, \\dots, x_k] \\] The attributes describe each alternative in a way that is relevant to the decision maker. In the present example, two relevant attributes are the cost of each alternative and the characteristics of the current and new phones, for instance, their download speeds. In this way, the two choices can be described by their attributes as follows: \\[ \\begin{array}{cc} \\text{Do-Nothing:} &amp; X_A = [\\text{cost}_{\\text{Do-Nothing}}, \\text{ speed}_{\\text{New-Phone}}]\\\\ \\text{New-Phone:} &amp; X_B = [\\text{cost}_{\\text{New-Phone}}, \\text{ speed}_{\\text{New-Phone}}]\\\\ \\end{array} \\] If the decision-maker currently owns a phone that is fully paid, the out-of-pocket cost of doing nothing would be zero. Buying a new phone, on the other hand, would have a positive (and possibly substantial cost). The new phone, on the other hand is faster than the older, currently owned model. The decision-maker can likewise be described by a vector of attributes, say \\(Z\\): \\[ Z = [z_1, z_2, \\dots, z_k] \\] Suppose for example, that decision-maker \\(i\\) can be described in terms of their income, as follows: \\[ Z_i = [\\text{income}_i] \\] The attributes of the decision maker help to capture heterogeneities in behavior: for instance, a decision-maker with a lower income may be more sensitive to cost, since buying a new phone is relatively more expensive. A utility function is a way of summarizing the attributes of the choices and the attributes of the decision-makers in a single quantity, which is what the decision-maker is trying to maximize. We assume that each course of action gives this consumer a level of utility: in other words, he will be more or less happy with each alternative, taking into account their characteristics and his own condition or status: \\[ \\begin{array}{c} U_{i, \\text{Do-Nothing}} = U(\\text{cost}_{\\text{Do-Nothing}}, \\text{ speed}_{\\text{Do-Nothing}}, \\text{ income}_i)\\\\ U_{i, \\text{New-Phone}} = U(\\text{cost}_{\\text{New-Phone}}, \\text{ speed}_{\\text{New-Phone}}, \\text{ income}_i)\\\\ \\end{array} \\] Notice that the utility function is specific to a decision maker \\(i\\) and an alternative. Here we define a decision-making rule. The decision-maker considers the utility of the alternatives, and chooses the one that gives the highest utility. In other words decision-maker \\(i\\) will choose to keep the current phone if: \\[ U_{i,\\text{Do-Nothing}} &gt; U_{i,\\text{New-Phone}}, \\] If the reverse is true, then the decision-maker will choose to buy a new phone (in the case of a tie, the decision-maker is indifferent between the two alternatives). We assume that decision-makers are rational and that they do an analysis of the costs and the benefits of each alternative before making the choice. The analyst, however, may fail to observe all aspects of the decision making process. For instance, a decision-maker may need faster speeds because she lacks internet at home. Or a decision-maker just received a large gift from a relative. Or younger people may be more willing to buy new phones than older people. The analyst may observe a decision-maker with a relatively low income buying a new phone. While income alone would have suggested that the decision-maker would be better off keeping the old phone, the analyst has no way of knowing the idiosyncratic factor of the gift. For this reason, it is convenient to decompose the utility into 1) a systematic component, that is, the part that explains the decision-makers’ response to the attributes of the alternative; and 2) a random component, which captures other aspects of the decision making process that the analyst did not observe: \\[ \\begin{array}{c} U_A = V_A + \\epsilon_A\\\\ U_B = V_B + \\epsilon_B\\\\ \\end{array} \\] The random part of the function is called the random utility. If there was no uncertainty at all, if we knew precisely all there is to know about the decision-making process, we would have that \\(\\epsilon_{\\text{Do-Nothing}}\\) and \\(\\epsilon_{\\text{New-Phone}}\\). Accordingly, \\(U_{\\text{Do-Nothing}} = V_{\\text{Do-Nothing}}\\) and \\(U_{\\text{New-Phone}} = V_{\\text{New-Phone}}\\), and we could predict with complete certainty the choice. However, the presence of the random components means that we cannot be certain whether \\(U_A &gt; U_B\\). While this is unfortunate, the presence of the random terms does allow us to make a probabilistic statement, such as: \\[ P_{\\text{Do-Nothing}} = P(U_{\\text{Do-Nothing}} &gt; U_{\\text{New-Phone}}) \\] In other words, the probability of doing nothing equals the probability that the utility of doing nothing is greater than the utility of buying a new phone. After rearranging things, this is equivalent to: \\[ P_{\\text{Do-Nothing}} = P(V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} &lt; \\epsilon_{\\text{New-Phone}} - \\epsilon_{\\text{Do-Nothing}}) \\] The expression above is the foundation of random utility modelling. Before we make more progress, however, we have to answer an important question. 3.7 What about those random terms? library(tidyverse) library(evd) A probabilistic expression is clearly better than being unable to say anything at all regarding choices. To make this expression of practical use, we must assume some distribution for the random terms. Which means that we need to define some probability distribution function. 3.8 Probability distribution functions (PDFs) A candidate for a probability distribution function is any function that satisfies the following two conditions: \\[ \\begin{array}{l} \\text{Condition 1: }f(x)\\ge 0\\text{ for all }x\\\\ \\text{Condition 2: }\\int_{-\\infty}^{\\infty}f(x)dx=1\\\\ \\end{array} \\] These two conditions say that the function must take values of at least zero for the interval of \\(x\\) of interest, and that the area under the curve (that is what the integral means) must equal 1. Lets use an example to illustrate these properties. We will define the following function: \\[ f(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le -L \\\\ \\frac{1}{2L} &amp; \\quad -L&gt; x &gt; L \\\\ 0 &amp; \\quad x \\ge L \\\\ \\end{array} \\right. \\] This function is shown in Figure 3.1 below, with \\(L=2\\): # Define a function uniform &lt;- function(x, L) ifelse( x &lt;= -L, 0, ifelse( x &gt; -L &amp; x &lt;= L, 1/(2 * L), 0 )) # Define parameter L for the distribution L &lt;- 2 # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -(L+1), to = L+1, by = 0.01)) %&gt;% mutate(y = uniform(x, L)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 3.1: Uniform distribution It is easy to see that the value of the function is always equal to or greater than zero. You can also verify that the area under the curve in this case is simply the area of the rectangle \\(b \\times h\\), where the base of the rectangle is \\(b = L - (-L)\\) and the height is \\(h=\\frac{1}{2L}\\): (L - (-L)) / (2 * L) ## [1] 1 If you are working with the R Notebook file, try changing the value of the parameter \\(L\\) to see what happens! What is the implication of larger values of L? And of smaller values of L? Since the function above satisfies the two necessary conditions, we conclude that it is a valid probability distribution function. In fact, it turns out to be a form of the uniform probability distribution function, which more generally is defined as: \\[ f(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le b \\\\ \\frac{1}{a - b} &amp; \\quad b&gt; x &gt; a \\\\ 0 &amp; \\quad x \\ge a \\\\ \\end{array} \\right. \\] Given a probability distribution function, we can calculate the probability of a random variable \\(x\\) being contained in a defined interval. For instance, the probability of \\(x &lt; -L\\) is zero, since the area under the curve in that case is zero. The probability of \\(x \\le X\\) is: \\[ \\int_{-\\infty}^{X}f(x)dx \\] In the case of our uniform distribution function, this is simply the area of the rectangle defined by the limits of the integral: # Define L L &lt;- 2 # Define an upper limit for calculating the probability X &lt;- 0 # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -(L+1), to = L+1, by = 0.01)) %&gt;% mutate(y = uniform(x, L)) df_p &lt;- data.frame(x =seq(from = -(L+1), to = X, by = 0.01)) %&gt;% mutate(y = uniform(x, L)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + # Plot distribution function geom_area(data = df_p, fill = &quot;orange&quot;, alpha = 1) + # Plot area under the curve ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis What is the probability that \\(x \\le 0\\)? Try changing the upper limit to see what happens. How does the value of the area under the curve change? Associated to a probability distribution function we can define a cumulative distribution function \\(F_X(x) = P(x \\le X)\\), which maps how the probability changes as we change the interval. The cumulative distribution function of our uniform distribution is as follows: \\[ F(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le -L \\\\ \\frac{x + L}{2L} &amp; \\quad -L&gt; x &gt; L \\\\ 1 &amp; \\quad x \\ge L \\\\ \\end{array} \\right. \\] The cumulative distribution function for our uniform distribution appears in Figure 3.2: # Define the cumulative distribution function cuniform &lt;- function(x, L) ifelse( x &lt;= -L, 0, ifelse( x &gt; -L &amp; x &lt;= L, (x + L)/(2 * L), 1 )) # Define L L &lt;- 2 # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -(L+1), to = L+1, by = 0.01)) %&gt;% mutate(y = cuniform(x, L)) # Plot ggplot(data = df, aes(x, y)) + geom_step(color = &quot;orange&quot;) + # Plot cumulative distribution function ylim(c(0, 1)) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;F(x)&quot;) # Label the y axis Figure 3.2: Uniform cumulative distribution function As you can see, the probability of \\(x \\le -L\\) is zero, the probability of \\(x \\le 0\\) is 0.5, and the probability of \\(x \\le L\\) is one. Lets consider a second example, with a function as follows: \\[ f(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le 0 \\\\ 2x &amp; \\quad 0 &gt; x &gt; 1 \\\\ 0 &amp; \\quad x \\ge 1 \\\\ \\end{array} \\right. \\] This function is shown in Figure 3.3. # Define a function linear &lt;- function(x) ifelse( x &lt;= 0, 0, ifelse( x &gt; 0 &amp; x &lt;= 1, 2 * x, 0 )) # Create a data frame for plotting df &lt;- data.frame(x =seq(from = 0, to = 1, by = 0.01)) %&gt;% mutate(y = linear(x)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + ylim(c(0, 2)) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 3.3: Cubic distribution Clearly, \\(f(x) \\ge 0\\) for all values of \\(x\\) in the interval \\(0 \\le x \\le 1\\). We can verify that the area under the curve is 1. In this case the area is that of a triangle, i.e., \\(\\frac{b \\times h}{2}\\). Since the base of the triange is \\(b=1\\) and the height is \\(h=2\\), we see that the area under the curve is 1. Since this is a valid probability distribution function, we can use it to calculate the probability of \\(x \\le X\\) as above. The area under the curve when \\(x \\le X\\) is given by: \\[ F(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le 0 \\\\ \\frac{x \\times 2x}{2} = x^2 &amp; \\quad 0 &gt; x &gt; 1 \\\\ 1 &amp; \\quad x \\ge 1 \\\\ \\end{array} \\right. \\] The plot of the cumulative distribution function in this case is shown in Figure 3.4. # Define a function clinear &lt;- function(x) ifelse( x &lt;= 0, 0, ifelse( x &gt; 0 &amp; x &lt;= 1, x^2, 1 )) # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -0.2, to = 1.2, by = 0.001)) %&gt;% mutate(y = clinear(x)) # Plot ggplot(data = df, aes(x, y)) + geom_step(color = &quot;orange&quot;) + # Plot cumulative distribution function ylim(c(0, 1)) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 3.4: Linear cumulative distribution function It should be clear from the examples above that calculating probabilities is nothing more than finding the area under the curve of a function. When the function is relatively simple, as the uniform or the linear distributions that we used for the examples, calculating the areas is also straightforward, since the functions describe simple geometric shapes. When the function is more involved, that becomes less straighforward. For example, consider the following function: \\[ f(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le 0 \\\\ 4x^3 &amp; \\quad 0 &gt; x &gt; 1 \\\\ 0 &amp; \\quad x \\ge 0 \\\\ \\end{array} \\right. \\] This function is plotted in Figure 3.5. # Define a function cubic &lt;- function(x) ifelse( x &lt;= 0, 0, ifelse( x &gt; 0 &amp; x &lt;= 1, 4 * x^3, 0 )) # Create a data frame for plotting df &lt;- data.frame(x =seq(from = 0, to = 1, by = 0.01)) %&gt;% mutate(y = cubic(x)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + ylim(c(0, 4)) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 3.5: Cubic distribution Unlike the rectangle of the uniform distribution and the triangle of the linear distribution, the area under the curve for this distribution needs to be obtained by integration as follows (do not worry if ): \\[ \\int_{0}^{1}4x^3dx =4\\Big[\\frac{x^4}{4} \\Big]_{0}^{1} = \\Big[x^4 \\Big]_{0}^{1} = 1^4-0 = 1 \\] This shows that the function is a valid probability distribution function. However, the integration makes things more interesting, to say the least! Fortunately, for most applied discrete choice analysis we do not need to solve integrals manually (the monster minds have already done this for us!). The key here is to remember: given a valid probability distribution function the probability that a random variable \\(x \\le X\\) is the area under the curve in the interval \\(-\\infty\\) to \\(X\\). 3.9 A simple random utility discrete choice model We are now ready to deploy a probability distribution function to the probability of choosing an alternative. Returning to our binary choice example, lets assume that the difference in the random utility terms follows the uniform distribution with parameters \\(-L\\) and \\(L\\), that is: \\[ \\epsilon_{\\text{New-Phone}} - \\epsilon_{\\text{Do-Nothing}} \\sim U(-L, L) \\] The probability of choosing the “do-nothing alternative” is: \\[ P_{\\text{Do-Nothing}} = P(V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} &lt; \\epsilon_{\\text{New-Phone}} - \\epsilon_{\\text{Do-Nothing}}) \\] Since we know the probabilities of a random variable being less than a certain value in the uniform distribution, we have that: \\[ P_{\\text{Do-Nothing}} = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} \\le -L \\\\ \\frac{V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} + L}{2L} &amp; \\quad -L&gt; V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} &gt; L \\\\ 1 &amp; \\quad V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} \\ge L \\\\ \\end{array} \\right. \\] Lets unpack this expression. When the systematic utility of a new phone is greater than the systematic utility of doing nothing, the difference between these two terms is negative. The more negative this value is, the lower the probability of doing nothing. When the difference is more negative than \\(-L\\), the probability of doing nothing becomes zero. When the systematic utility of a new phone is identical to the systematic utility of doing nothing, the difference between these two terms is zero, in which case the probability of doing nothing is \\(0.5\\). In other words, there is a 50% chance that the decision maker will do nothing. Finally, when the systematic utility of a new phone is less than the systematic utility of doing nothing, the difference between these two terms is positive. The more the more positive this value is, the higher the probability of doing nothing. When the difference is greater than \\(L\\), the probability of doing nothing becomes one. Now, since the choice set is an exhaustive collection of courses of action, it follows that the probability of the two courses of action must add up to one (the decision-maker does nothing OR buys a new phone): \\[ P_{\\text{Do-Nothing}} + P_{\\text{New-Phone}} = 1 \\] This implies that once we know the probability of doing nothing, the probability of buying a new phone is simply the complement: \\[ P_{\\text{New-Phone}} = 1 - P_{\\text{Do-Nothing}} \\] These probabilities are a discrete choice model. In fact, this is called the linear probability model (see Ben-Akiva and Lerman 1985, 66–68). Other models can be obtained by selecting different probability distribution functions, as we will see in later chapters. The procedure followed here will be the same, even if the probability distribution function selected for the model is different: given a valid probability distribution function, and given the systematic utilities of the alternatives, it is possible to evaluate the probabilistic statement associated with the choice of an alternative. One final note, before discussing other choice mechanisms. The simple example used here was for binary choice, i.e., for a situation with only two alternatives. This was done for convenience of exposition, and we will see how the same ideas generalize for situations with more than two alternatives, that is, for multinomial choice situations. 3.10 Other choice mechanisms Utility maximization is only one of several plausible mechanisms. The utility functions assume that trade-offs among different attributes are possible; for example, the way the utility functions were formulated assumes that a decision-maker is willing to pay more for higher download speeds. While such trade-offs are plausible in many situations, other choice mechanisms could exist in other cases. Ortuzar and Willumsen (2011, Fourth Edition:241–43). For example, a user who is shopping for smartphones may have low tolerance for download speeds below a certain threshold, or may have a budget limit that prevents her from considering certain models. Some alternatives from the choice set may be eliminated or ranked based on some dominant attribute. This kind of choice mechanism is called lexicographic choice, or elimination by attributes. Another plausible choice mechanism is a form of satisficing behavior. Again, a user shopping for a smartphone might find a model that does not maximize her utility, but that is otherwise satisfactory. For example, the decision-maker may consider that the additional time spent finding an even better model is not worth her while, so she stops her search at a suboptimal point. Another choice mechanism seen recently in the literature is regret-minimization (Chorus 2010). In addition to these various mechanisms, it is possible that a decision-maker deploys combinations of them: for example, lexicographic choice to reduce the number of alternatives in a choice set, followed by utility maximization or regret minimization. Despite progress on these models, utility maximization remains the most widely used approach for the analysis of discrete choices. 3.11 Exercise Answer the following questions. 3.11.1 Questions Define utility. Describe in your own words the behavior described by utility maximization. What conditions are necessary for a function to be a valid probability distribution function? Consider the function shown in Figure 3.6. This is called the triangle or tent function. Figure 3.6: Triangle (or tent) function Show that the triangle function in the figure is a valid probability distribution function. Next, consider the following utility functions for two alternatives, namely \\(i\\) and \\(j\\): \\[ \\begin{array}{c} U_i = V_i + \\epsilon_i\\\\ U_j = V_j + \\epsilon_j\\\\ \\end{array} \\] Assume that the difference between the error terms below follows the triangle distribution: \\[ \\epsilon_q = \\epsilon_i - \\epsilon_j \\] Parting from the assumption above, derive a binary choice model for the probability of selecting alternative \\(j\\). References "],
["chapter-3.html", "Chapter 4 Logit 4.1 Modelling choices 4.2 How to use this note 4.3 Learning objectives 4.4 Suggested readings 4.5 Preliminaries 4.6 Once again those random terms 4.7 Now, about those parameters \\(\\mu\\) and \\(\\sigma\\)… 4.8 Multinomial logit 4.9 Properties of the logit model 4.10 Revisiting the systematic utilities 4.11 Exercise", " Chapter 4 Logit “I believe that we do not know anything for certain, but everything probably.” — Christiaan Huygens 4.1 Modelling choices In Chapter 3 a conceptual framework was described to model choice-making behavior. This framework is based on the economic notion of utility, basically that which decision-makers wish to maximize when making choices. The concept of utility has many flaws - key among them that it is not directly observable. If utility could be measured directly by an external observer (or analyst), behavior would seem deterministic. However, unlike Laplace’s Demon, an external observer with only human capabilities has limited knowledge of the conditions under which choices are made, if for no other reason that she cannot possibly know the frame of mind of the decision-maker at the moment when choices are made. A way to implement the conceptual framework unders such conditions involved an acknowledgement that although the decision-maker tries to maximize his utility, some part of it will look random to the observer - therefore the term random utility modelling. This allows the analyst to make probabilitistic statements about the behavior of decision-makers. Accordingly, the analyst does not know with certainty the outcome of a choice process, but can quantify her uncertainty in a fairly precise way. Based on these concepts, Chapter 3 concluded by deriving a simple model for discrete choices, namely the lineary probability model (see Ben-Akiva and Lerman 1985, 66–68). This model is useful for illustrative purposes. However, it suffers from an important limitation: the linear probabilities are a stepwise function, which makes their mathematical treatment unfun, and also imply that certain outcome are certain (i.e., it can return probabilities of exactly one or exactly zero). This would preclude certain behaviors, which seems a somewhat arrogant thing to do on the part of the analyst. A better approach would be to allow any behavior, but assign very small probabilities to more extreme choices. In this chapter we will revisit those random utility terms to derive an alternative to the linear probability model. This will be the logit model, one of the most popular models in discrete choice analysis for reasons that will be discussed below. 4.2 How to use this note Remember that the source for the document you are reading is an R Notebook. Throughout the notes, you will find examples of code in segments of text called chunks. This is an example of a chunk: print(&quot;Hello, Dr. Train&quot;) ## [1] &quot;Hello, Dr. Train&quot; If you are working with the Notebook version of the document, you can run the code by clicking the ‘play’ icon on the top right corner of the chunk. If you are reading the web-book version of the document, you will often see that the code has already been executed. You can still try it by copying and pasting into your R or RStudio console. 4.3 Learning objectives In this practice, you will learn about: The Extreme Value distribution. The binary logit model. The multinomial logit model. Properties of the logit model. 4.4 Suggested readings Ben-Akiva, M. Lerman, (1985) Discrete Choice Analysis: Theory and Applications to Travel Demand, Chapters 4 and 5, MIT Press. Hensher, D.A., Rose, J.M., Greene, W.H (2005) Applied Choice Analysis: A Primer, Chapter 10, Cambridge University Press. Ortuzar JD, Willumsen LG (2011) Modelling Transport, Fourth Edition, Chapter 7, John Wiley and Sons. Louviere, J.J., Hensher, D.A., Swait, J.D. (2000) Stated Choice Methods: Analysis and Application, Chapter 3, pp. 199-205, Cambridge University Press. Train (2009) Discrete Choice Methods with Simulation, Second Edition, Chapter 3, Cambridge University Press. 4.5 Preliminaries Load the packages used in this section: library(tidyverse) library(evd) 4.6 Once again those random terms Recall that in order to implement the probabilitistic statement at the heart of a discrete choice model requires the analyst to make assumptions about the random utility terms. Previously, a number of probability distributions were explored, and one in particular (the uniform distribution) was used to derive a simple discrete choice model. But, is the uniform distribution an appropriate choice for the purpose of modeling the random utility? The uniform distribution (and some of the other stepwise distributions seen in Chapter 3) are useful to illustrate the concept of probability, and more specifically the need to calculate the area under the curve of the distribution. The area under the curve of the uniform distribution is simply the area of a rectangle, which makes this extremely simple. On the other hand, it precludes certain outcomes, which limits its practical usefulness. The reality is that, since the utility is in principle unobservable, there is little theoretical support for any specific distribution of the random utility terms. For this reason, the choice of distribution tends to be very pragmatic, particularly attending the convenience for estimation purposes, i.e., retrieving parameters from a sample of observations. The parameters include the parameters used in the systematic utility function \\(U_{ij}\\) (more on this later), as well as any parameters needed for the distribution itself. For instance, the uniform distribution is defined by two parameters, \\(a\\) and \\(b\\): \\[ f(x) = \\left\\{ \\begin{array}{lc} 0 &amp; \\quad x \\le b \\\\ \\frac{1}{a - b} &amp; \\quad b&gt; x &gt; a \\\\ 0 &amp; \\quad x \\ge a \\\\ \\end{array} \\right. \\] These parameters define the dispersion of the distribution. The dispersion controls the shape of the distribution, in this case how wide or narrow it is. The greater the difference between \\(a\\) and \\(b\\) the greater the range of values with non-zero probability, but the lower the probability for a constant interval of values. These two parameters also determine the center of the distribution. In this way, the uniform distribution is centered at \\(\\frac{a-b}{2}\\). Other distributions also have parameters that determine their shape and position. A convenient choice of distribution is the Extreme Value type I (EV Type I) probability distribution function. This function is defined as: \\[ f(x; \\mu,\\sigma) = e^{-(x + e^{-(x-\\mu)/\\sigma})} \\] The EV Type I distribution has two parameters, namely \\(\\mu\\) and \\(\\sigma\\), which determine the location (i.e., the center) and the dispersion of the distribution, respectively. The shape of this distribution is shown in Figure 4.1 with \\(\\mu = 0\\) and \\(\\sigma = 1\\): # Define parameters for the distribution mu &lt;- 0 sigma &lt;- 1 # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -5, to = 5, by = 0.01)) %&gt;% mutate(y = dgumbel(x, loc = mu, scale = sigma)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + #ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 4.1: Extreme Value Type I distribution If you are working with the R Notebook, you can try changing the parameters to see how the function behaves (remember to adjust the limits if you change the center of the distribution!). The EV Type I has a very interesting property: the difference of two EV Type I distributions follows the logistic distribution. In other words, if \\(X \\sim \\text{EVI}(\\alpha_Y,\\sigma)\\) and \\(Z \\sim \\text{EVI}(\\alpha_Z,\\sigma)\\), then: \\[ X - Y \\sim \\text{Logistic}(\\alpha_X - \\alpha_Y,\\sigma) \\] If we define the difference of the random utility terms as \\(\\epsilon_n = \\epsilon_j - \\epsilon_k\\), the logistic distribution, in turn, is defined as follows: \\[ f(x; \\mu,\\sigma) = \\frac{e^{-(x-\\mu)/\\sigma}}{\\sigma(1 + e^{-(x-\\mu)/\\sigma})^2} \\] Whereas the EV Type I distribution was not symmetric, the shape of the logistic distribution is. The logistic distribution is, in fact, similar to the normal distribution but it has fatter tails, which means that the probability of extreme values is higher. This is illustrated in Figure 4.2: # Define parameters for the distribution mu &lt;- 0 sigma &lt;- 1 # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -5, to = 5, by = 0.01)) %&gt;% mutate(logistic = dlogis(x, location = mu, scale = sigma), normal = dnorm(x, mean = mu, sd = sigma)) # Plot ggplot() + geom_area(data = df, aes(x, logistic), fill = &quot;blue&quot;, alpha = 0.5) + geom_area(data = df, aes(x, normal), fill = &quot;black&quot;, alpha = 0.5) + #ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 4.2: Comparison of the logistic (blue) and normal (grey) distributions Since the probability expression is given in terms of the difference of the random utilities, if we assume that the random terms \\(\\epsilon\\) follow the EV Type I distribution, their difference (i.e., \\(\\epsilon_n = \\epsilon_j - \\epsilon_i\\)) follows the logistic distribution. As before, the area under the curve of the function needs to be calculated to obtain a probability. Unfortunately, this needs to be done by integration. Fortunately, this integral has an analytical solution, or so-called closed form: \\[ F(x; \\mu,\\sigma) = \\frac{1}{1 + e^{-(\\epsilon_n-\\mu)/\\sigma}} \\] Accordingly, the probability expression is as follows: \\[ P_j = P(V_j - V_k \\le \\epsilon_n) = \\frac{1}{1 + e^{-(\\epsilon_n-\\mu)/\\sigma}} = \\frac{1}{1 + e^{-(V_j-V_k-\\mu)/\\sigma}} \\] Which, after some manipulation, can be rewritten as: \\[ P_j = P(V_j - V_k \\le \\epsilon_n) = \\frac{e^{V_j/\\sigma}}{e^{V_j/\\sigma} + e^{(V_k+\\mu)/\\sigma}} \\] The above is called the logit probability and the resulting model is called the logit model. As seen, the probability of choosing alternative \\(j\\) is the area under the curve of the logistic distribution function, as seen in Figure 4.3 (assuming \\(mu = 0\\) and \\(\\sigma = 1\\)): # Define parameters for the distribution mu &lt;- 0 sigma &lt;- 1 # Define an upper limit for calculating the probability X &lt;- -1 # Create data frames for plotting df &lt;- data.frame(x =seq(from = -5, to = 5, by = 0.01)) %&gt;% mutate(y = dlogis(x, location = mu, scale = sigma)) df_p &lt;- data.frame(x =seq(from = -5, to = X, by = 0.01)) %&gt;% mutate(y = dlogis(x, location = mu, scale = sigma)) # Plot ggplot(data = df, aes(x, y)) + geom_area(fill = &quot;orange&quot;, alpha = 0.5) + # Plot distribution function geom_area(data = df_p, fill = &quot;orange&quot;, alpha = 1) + # Plot area under the curve #ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis xlab(expression(paste(epsilon[n]))) + # Label the y axis ylab(&quot;f(x)&quot;) # Label the y axis Figure 4.3: Logit probability Try changing the upper limit in the figure above to explore the behavior of the logit probability. What is the probability of choosing \\(j\\) when \\(V_j - V_k = 0\\)? What is the probability of choosing \\(j\\) when \\(V_j &gt;&gt; V_k\\)? And when \\(V_j &lt;&lt; V_k\\)? Is this as expected? The cumulative distribution function is shown in Figure 4.4. Notice that this function tends asymptotically to 0 when \\(x\\) tends to \\(-\\infty\\) and to 1 when \\(x\\) tends to \\(\\infty\\). This function never assigns values of exactly 0 or exactly 1. # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -5, to = 5, by = 0.01)) %&gt;% mutate(y = plogis(x)) # Plot logit_plot &lt;- ggplot(data = df, aes(x, y)) + geom_line(color = &quot;orange&quot;) + # Plot cumulative distribution function ylim(c(0, 1)) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) # Add x axis logit_plot + xlab(expression(paste(V[j], &quot; - &quot;, V[k], sep=&quot;&quot;))) + # Label the x axis ylab(expression(paste(P[j]))) # Label the y axis Figure 4.4: Linear cumulative distribution function The logit probability exhibits a shape usually called a sigmoid (for its resemblance to the letter “s”). This shape is shared by most other discrete choice models - the uniform distribution in Chapter 3, for instance, resembled an angular letter “s”, whereas the linear and quadratic distribution functions started to display the non-linear aspect of the logit probability function. Sigmoid functions are of interest in many fields. The study of technology adoption is a case in point; new technologies are initially adopted slowly, then go through a rapid growth stage, before reaching saturation. Population growth is often represented by similar curves, with population growing slowly, then explosively, before reaching a carrying capacity limit. In the case of discrete choice analysis, the shape of the function is interesting from a policy perspective. In the vast majority of cities in North America, for example, the two main modes of transportation are cars and transit. However, the shares of transit tend to be very low, sometimes lower than 10% or even 5%. This suggests that the underlying probabilities of choosing transit at the individual level are very low too. Suppose that the logit curve in Figure 4.4 is for the probability of choosing transit. If the initial probability of choosing transit is low, large increases in the utility of transit result in relatively modest gains in probability (see solid blue line in Figure 4.5). If the starting probability of transit had been instead 0.5, an identical increase in the utility of transit would result in a much larger gain in the probability (see dashed red line in Figure 4.5). logit_plot + xlab(expression(paste(V[transit], &quot; - &quot;, V[car], sep=&quot;&quot;))) + # Label the x axis ylab(expression(paste(P[transit]))) + # Label the y axis annotate(&quot;segment&quot;, x = -3.75, xend = -2.5, y = 0.024, yend = 0.024, colour = &quot;blue&quot;, linetype = &quot;solid&quot;) + annotate(&quot;segment&quot;, x = -2.5, xend = -2.5, y = 0.024, yend = 0.075, colour = &quot;blue&quot;, linetype = &quot;solid&quot;) + annotate(&quot;segment&quot;, x = 0, xend = 1.25, y = 0.5, yend = 0.5, colour = &quot;red&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;segment&quot;, x = 1.25, xend = 1.25, y = 0.5, yend = 0.77, colour = &quot;red&quot;, linetype = &quot;dashed&quot;) Figure 4.5: Implication of the sigmoid shape The implication is that when the penetration of an alternative (think transit, hybrid vehicles, clean energy, and other new technologies) is still low, the incentives needed to raise the probabilities need to be very strong even for modest gains. When penetration has increased, the incentives may be eased since their impact is now more than proportional, until reaching saturation, where again large gains in utility result in modest increases in the probability of adoption. 4.7 Now, about those parameters \\(\\mu\\) and \\(\\sigma\\)… Figure 4.3 above was created assuming that \\(\\mu=0\\) and \\(\\sigma=1\\). Can we really set these values in such an arbitrary fashion? The answer is no and yes. In the case of the centering parameter \\(\\mu\\), setting it arbitrarily to zero is not appropriate. The reason is that we can think of this parameter as being key to calculating the difference between the systematic utilities. As seen above, the logit probability is: \\[ P_j = P(V_j - V_k \\le \\epsilon_n) = \\frac{1}{1 + e^{-(V_j-V_k-\\mu)/\\sigma}} \\] Assume that we let one of the utility functions absorb \\(\\mu\\), that is we let either: \\[ V^*_k = V_k + \\mu \\] or: \\[ V^*_j = V_j - \\mu \\] It does not really matter which utility function we choose to absorb \\(\\mu\\) (the only thing that changes is the sign). For convenience, we will say that it is \\(V_k\\), in which case the logit probability can be written as: \\[ P_j = P(V_j - V^*_k \\le \\epsilon_n) = \\frac{1}{1 + e^{-(V_j-V^*_k)/\\sigma}} \\] The difference in other words depends on the value of \\(\\mu\\). When \\(\\mu\\) is a large positive number, the effect is to increase the utility of alternative \\(k\\) (or conversely, since it would enter with a negative sign in \\(V^*_j\\), it would decrease the utility of alternative \\(j\\)). When \\(\\mu\\) is a large negative number, the effect is to increase the utility of \\(j\\) - or alternatively to reduce the utility of \\(k\\). For this reason we do not want to arbitrarily set the value of \\(\\mu\\) to zero, because this parameter contains information about the relative differences between \\(V_j\\) and \\(V_k\\). The utility function that does not contain the centering parameter \\(\\mu\\) is called the reference function. For simplicity of presentation, I will drop the notation \\(V^*\\) and will assume henceforth that one of the utility functions has absorbed parameter \\(\\mu\\). Now, with respect to the dispersion parameter \\(\\sigma\\), this parameter is common to the two utility functions in the logit probability and, as it turns out, it can be arbitrarily set to one. Consider two utility functions as follows: \\[ V_j - V_k \\] Multiplying (alternatively dividing) by a constant greater than zero changes the magnitude of their difference, since: \\[ \\theta(V_j - V_k) = \\theta V_j - \\theta V_k \\] In other words, mutliplying two quantities by a positive constant changes the cardinality of the difference. If you are working with the R Notebook, you might want to try changing the value of theta below, keeping in mind that the value must be greater than zero: V_j &lt;- -4 V_k &lt;- 8 theta &lt;- 0.8 theta * V_j - theta * V_k ## [1] -9.6 You will notice that the difference changes as you change the value of theta. But what about the sign? On the other hand, multiplying two quantities by a positive constant does not affect their ordinality. That is, if \\(V_j &gt; V_k\\) then it is always true that \\(\\theta V_j &gt; \\theta V_k\\). Recall the decision making rule: an alternative is chosen if its utility is greater than that of the competing alternatives. The rule is purely ordinal, it does not matter if the difference between them is small or large - in other words, their cardinality is irrelevant. This is convenient because it allows us to simplify the logit probability as follows, by arbitrarily setting \\(\\sigma=1\\): \\[ P_j = P(V_j - V_k \\le \\epsilon_n) = \\frac{1}{1 + e^{-(V_j-V_k)}} = \\frac{e^{V_j}}{e^{V_j} + e^{V_k}} \\] 4.8 Multinomial logit The logit model above was derived assuming a choice set with only two alternatives. This, of course, is very restrictive, and there are many situations where more than two alternatives are of interest. Fortunately, a multinomial version of the logit model can be derived without much difficulty, and it also results in a closed form expression, as follows: \\[ P_j = P(V_j - V_k \\le \\epsilon_n) = \\frac{e^{V_j}}{\\sum_k^Je^{V_k}} \\] Notice that in this case there are \\(J-1\\) parameters \\(\\mu\\) that are absorbed by all but one of the utility functions. As before, it does not matter which utility is selected to act as the reference, since the signs (and magnitudes) of the centering parameters adjust accordingly. More on this later. 4.9 Properties of the logit model The logit model is the workhorse of discrete choice analysis, in good measure because of its closed form which does not require numerical evaluation of the integrals involved in calculating probabilities (i.e., the “area under the curve”, although in multinomial situations this actually is a volume under the surface!) One important property of the logit model is the way it handles substitution patterns. Consider the ratio of odds for any two alternatives according to the multinomial logit model: \\[ \\frac{P_j}{P_m}=\\frac{\\frac{e^{V_j}}{\\sum_ke^{V_k}}}{\\frac{e^{V_m}}{\\sum_ke^{V_k}}} =\\frac{e^{V_j}}{e^{V_m}} =e^{V_j - V_m} \\] As seen, the ratio of the odds of \\(P_j\\) to \\(P_m\\) depends only on the difference in the utilities of alternatives \\(j\\) and \\(m\\) and nothing more. Furthermore, recall that the choice set is by design an exhaustive set of possible alternatives, and therefore the sum of the probabilities over this set is one: \\[ P_1 + P_2+\\cdots+P_J=1 \\] The above means that if the probability of choosing one alternative, say \\(j\\), increases, then the probabilities of choosing some or all of the other alternatives must decline. But since the ratio of odds for any two alternatives is independent of other alternatives in the choice set, the way the probabilities change depends on the change on the probability that triggered the adjustments. This property is called, quite fittingly, independence from irrelevant alternatives or IIA. Suppose, for instance, that a choice set consists of three alternatives products, say margarine (\\(m\\)) by Naturally, and salted butter butter (\\(sb\\)) and low-sodium butter (\\(lb\\)) by Happy Farms. The initial probabilities of choosing these alternatives are as follows: \\[ \\left\\{ \\begin{array}{ll} P^0_{m}=&amp;\\frac{1}{3}\\\\ P^0_{sb}=&amp;\\frac{1}{3}\\\\ P^0_{lb}=&amp;\\frac{1}{3}\\\\ \\end{array} \\right. \\] Next, suppose that a change in the attribute set of salted butter (\\(sb\\)), for instance a reduction in price, leads to an increase in the probability of choosing this product. Now the probability of choosing salted butter is: \\[ P^1_{sb}=\\frac{1}{2} \\] How do the other probabilities change? On the one hand, we know that the sum of the new probabilities must be one: \\[ P^1_{m} + P^1_{sb} + P^1_{lb} = 1 \\] Since the attributes of margarine and low-sodium butter did not change, we know that their utilities remain unchanged, and therefore: \\[ \\frac{P^1_{m}}{P^1_{lb}} = \\frac{\\frac{1}{3}}{\\frac{1}{3}} = 1 \\] In other words, the probability of \\(P^1_m = P^1_{lb}\\). Substituting: \\[ P^1_{m} + P^1_{sb} + P^1_{lb} = 2P^1_{m} + P^1_{sb} = 1 \\] Solving for \\(P^1_lb\\): \\[ P^1_m = \\frac{1 - P^1_{sb}}{2} = \\frac{1 - \\frac{1}{2}}{2} = \\frac{1}{4} \\] Therefore the new probabilities are: \\[ \\left\\{ \\begin{array}{ll} P^1_{m}=&amp;\\frac{1}{4}\\\\ P^1_{sb}=&amp;\\frac{1}{2}\\\\ P^1_{lb}=&amp;\\frac{1}{4}\\\\ \\end{array} \\right. \\] Notice that the increase in probability of choosing sunflower-based margarine draws proportionally from the other alternatives (i.e., butter and olive oil-based margarine) - in fact, 12.5% from each. Does this result make sense? What is now the market share of Happy Farms-brand line of butter? The property of Independence from Irrelevant Alternatives leads to proportional substitution patterns. Consider the following initial probabilities: \\[ \\left\\{ \\begin{array}{ll} P^0_{m}=0.5\\\\ P^0_{sb}=0.3\\\\ P^0_{lb}=0.2\\\\ \\end{array} \\right. \\] The new probability of \\(sb\\) changes to \\(P^1_{sb}=0.5\\). Following the same logic: \\[ \\frac{P^1_{m}}{P^1_{lb}} = \\frac{0.5}{0.2} = \\frac{5}{2} \\] And: \\[ P^1_m = \\frac{5}{7}(1 - P^1_{sb}) = \\Big(\\frac{5}{7}\\Big)\\Big(\\frac{1}{2}\\Big) = \\frac{5}{14} = 0.3571 \\] So the final probabilities are: \\[ \\left\\{ \\begin{array}{ll} P^1_{m}=\\frac{5}{14}=0.3571\\\\ P^1_{sb}=\\frac{1}{2}=0.5000\\\\ P^1_{lb}=\\frac{2}{14}=0.1429\\\\ \\end{array} \\right. \\] Now, the increase in \\(P1_{sb}\\) to \\(1/2\\) from \\(P^0_{sb}=1/5\\) is drawing more from \\(P^0_m\\) than from \\(P^0_{lb}\\). However, the pattern of substitution is still proportional, as it can be verified: \\[ \\begin{array}{ll} \\frac{P^1_{m}}{P^0_{m}}=\\frac{\\frac{5}{14}}{\\frac{1}{2}}=\\frac{10}{14}\\\\ \\frac{P^1_{lb}}{P^0_{lb}}=\\frac{\\frac{2}{14}}{\\frac{2}{10}}=\\frac{10}{14}\\\\ \\end{array} \\] Proportional substitution patterns are a consequence of the lack of correlation among the random utilities. The logit model considers that the alternatives are all independent. However, in this example, this condition is suspect: the two kinds of margarine are more similar between them than either are to butter. Indeed, if consumers choose butter for flavor, lowering the price of one kind of margarine is likely to draw less than proportionally from the probability of choosing butter - and more than proportionally from the probability of the other kind of margarine if, for instance, consumers prefer margarine for health reasons but respond to price changes. In this case, the correlation between the two kinds of margarine is a consequence of a missing attribute - say flavor, or health, that is necessary to discriminate among the alternatives. In this way, the logit model can be seen as the ideal model - its closed form being a very attractive feature - as long as the systematic utilities are properly and completely specified. When this is not the case, the results can lead to unrealistic and even unreasonable substition patterns. This issue suggests two possible courses of action: Working to ensure that the systematic utility functions are properly and completely specified. Modifying the modelling apparatus to accommodate correlations among the random utilities. As will become clear in later chapters, much work in the field of discrete choice analysis has been concerned with the latter. 4.10 Revisiting the systematic utilities Much of the discussion above has concentrated on the random utility; however, specifying the systematic utility is key. Recall that the utility is a function of the attributes of the alternatives and possibly the attributes of the decision-makers to allow the model to capture heterogeneity in decision-making styles by individuals. the utility function is a convenient way of summarizing all those attributes. Think again of the example of buying a new phone (see Chapter 3). In that simple example, the utilities were a function of three attributes, namely cost, speed, and income - to which we can add the random utility: \\[ \\begin{array}{c} U_{i, \\text{Do-Nothing}} = U(\\text{cost}_{\\text{Do-Nothing}}, \\text{ speed}_{\\text{Do-Nothing}}, \\text{ income}_i) = V(\\text{cost}_{\\text{Do-Nothing}}, \\text{ speed}_{\\text{Do-Nothing}}, \\text{ income}_i) + \\epsilon_{i, \\text{Do-Nothing}}\\\\ U_{i, \\text{New-Phone}} = U(\\text{cost}_{\\text{New-Phone}}, \\text{ speed}_{\\text{New-Phone}}, \\text{ income}_i) = V(\\text{cost}_{\\text{New-Phone}}, \\text{ speed}_{\\text{New-Phone}}, \\text{ income}_i) + \\epsilon_{i, \\text{New-Phone}}\\\\ \\end{array} \\] A common way of specifying the systematic utility is as linear-in-parameters, something that will be familiar to users of regression analysis: \\[ \\begin{array}{c} V(\\text{cost}_{\\text{Do-Nothing}}, \\text{ speed}_{\\text{Do-Nothing}}, \\text{ income}_i) = \\beta_1\\text{cost}_{\\text{Do-Nothing}} + \\beta_2\\text{ speed}_{\\text{Do-Nothing}} + \\beta_3\\text{ income}_i\\\\ V(\\text{cost}_{\\text{New-Phone}}, \\text{ speed}_{\\text{New-Phone}}, \\text{ income}_i) = \\mu + \\beta_1\\text{cost}_{\\text{New-Phone}} + \\beta_2\\text{ speed}_{\\text{New-Phone}} + \\beta_3\\text{ income}_i\\\\ \\end{array} \\] Notice how the location parameter of the logistic function is absorbed by one of the utility functions! The additive form of the utilities reflects a compensatory choice-making strategy: higher costs may be offset by higher speeds, for example. An important consideration is the way attributes enter the utility functions. Recall that one way of writing the logit probability was: \\[ P_j = \\frac{1}{1 + e^{-(V_j-V_k)}} \\] This formulation makes it clear that the probability is a function of the differences between utilities (this remains true in the multinomial logit, even if it is not as clear to see). Now consider what happens when the differences in utility are calculated: \\[ V_{i,\\text{Do-Nothing}} - V_{i,\\text{New-Phone}}= \\beta_1\\text{cost}_{i,\\text{Do-Nothing}} + \\beta_2\\text{ speed}_{i,\\text{Do-Nothing}} + \\beta_3\\text{income}_i - \\mu - \\beta_1\\text{cost}_{i,\\text{New-Phone}} - \\beta_1\\text{speed}_{\\text{i, New-Phone}} - \\beta_1\\text{income}_i\\\\ = \\beta_1(\\text{cost}_{i, \\text{Do-Nothing}} - \\text{cost}_{i, \\text{New-Phone}}) + \\beta_2(\\text{ speed}_{i, \\text{Do-Nothing}} - \\text{ speed}_{i, \\text{New-Phone}}) + \\beta_3(\\text{ income}_i - \\text{ income}_i) - \\mu \\] The income attribute vanishes! It is useful to distinguish between attributes that vary across utility functions and those that do not. Level of service attributes, those that describe the alternatives, generally vary by utility function - indeed, it is those attributes that help discriminate between alternatives. In this instance, income is invariant across utility functions. Personal attributes of the decision-makers, in general, are invariant across utility functions. The most common way of dealing with attributes that are constant across utility functions is to select one utility to act as a reference and set that attribute to zero there. This is illustrated below: \\[ \\begin{array}{c} V_{\\text{Do-Nothing}} = \\beta_1\\text{cost}_{\\text{Do-Nothing}} + \\beta_2\\text{ speed}_{\\text{Do-Nothing}} + \\beta_3(0)\\\\ V_{\\text{New-Phone}} = \\mu + \\beta_1\\text{cost}_{\\text{New-Phone}} + \\beta_2\\text{ speed}_{\\text{New-Phone}} + \\beta_3\\text{ income}_i\\\\ \\end{array} \\] The difference in utilities then becomes: \\[ V_{i,\\text{Do-Nothing}} - V_{i,\\text{New-Phone}}= \\beta_1(\\text{cost}_{i, \\text{Do-Nothing}} - \\text{cost}_{i, \\text{New-Phone}}) + \\beta_2(\\text{ speed}_{i, \\text{Do-Nothing}} - \\text{ speed}_{i, \\text{New-Phone}}) - \\beta_3(\\text{ income}_i) - \\mu \\] When the effect of income is positive (i.e., \\(\\beta_3&gt;0\\)) higher incomes reduce the probability of doing nothing, and when the effect of income is negative (i.e., \\(\\beta_3&lt;0\\)) higher incomes reduce the probability of buying a new phone. The effect of income is relative to the reference alternative. When there are more than two alternatives, the attribute can be entered in all but the reference utility, as shown next: \\[ \\begin{array}{llll} V_{\\text{Do-Nothing}} = &amp;0 &amp;+ 0 &amp;+ \\beta_1\\text{cost}_{\\text{Do-Nothing}} &amp;+ \\beta_2\\text{ speed}_{\\text{Do-Nothing}} &amp;+ \\beta_3(0)&amp; + \\beta_4(0) \\\\ V_{\\text{uPhone}} = &amp;\\mu_{\\text{uPhone}} &amp;+ 0 &amp;+ \\beta_1\\text{cost}_{\\text{uPhone}} &amp;+ \\beta_2\\text{ speed}_{\\text{uPhone}} &amp;+ \\beta_3\\text{ income}_i &amp; + \\beta_4(0)\\\\ V_{\\text{zPhone}} = &amp;0 &amp;+ \\mu_{\\text{zPhone}} &amp;+ \\beta_1\\text{cost}_{\\text{zPhone}} &amp;+ \\beta_2\\text{ speed}_{\\text{zPhone}} &amp;+ \\beta_3(0) &amp;+ \\beta_4\\text{ income}_i\\\\ \\end{array} \\] The above also illustrates how location parameters are absorbed by \\(J-1\\) utility functions. Another way to introduce attributes that do not vary across utility functions is reminiscent of Casetti’s expansion method (Casetti 1972). The expansion method is a systematic approach to introduce variable interactions that proceeds by defining an initial model whose coefficients are subsequently expanded using contextual variables. Suppose that the initial model is comprised of the utility functions with only level of service variables: \\[ \\begin{array}{c} V_{\\text{Do-Nothing}} = \\beta_1\\text{cost}_{\\text{Do-Nothing}} + \\beta_2\\text{ speed}_{\\text{Do-Nothing}}\\\\ V_{\\text{New-Phone}} = \\mu + \\beta_1\\text{cost}_{\\text{New-Phone}} + \\beta_2\\text{ speed}_{\\text{New-Phone}}\\\\ \\end{array} \\] The coefficients are expanded by a contextual variable, in this case income: \\[ \\beta_1 = \\beta_{11} + \\beta_{12}\\text{income}_i\\\\ \\beta_2 = \\beta_{21} + \\beta_{22}\\text{income}_i \\] Substituting the expanded coefficients in the initial model: \\[ \\begin{array}{c} V_{\\text{Do-Nothing}} = (\\beta_{11} + \\beta_{12}\\text{income}_i)\\text{cost}_{\\text{Do-Nothing}} + (\\beta_{21} + \\beta_{22}\\text{income}_i)\\text{ speed}_{\\text{Do-Nothing}}\\\\ V_{\\text{New-Phone}} = \\mu + (\\beta_{11} + \\beta_{12}\\text{income}_i)\\text{cost}_{\\text{New-Phone}} + (\\beta_{21} + \\beta_{22}\\text{income}_i)\\text{ speed}_{\\text{New-Phone}}\\\\ \\end{array} \\] The expanded model then becomes: \\[ \\begin{array}{c} V_{\\text{Do-Nothing}} = \\beta_{11}\\text{cost}_{\\text{Do-Nothing}} + \\beta_{12}\\text{income}_i\\cdot\\text{cost}_{\\text{Do-Nothing}} + \\beta_{21}\\text{ speed}_{\\text{Do-Nothing}} + \\beta_{22}\\text{income}_i\\cdot\\text{ speed}_{\\text{Do-Nothing}}\\\\ V_{\\text{New-Phone}} = \\mu + \\beta_{11}\\text{cost}_{\\text{New-Phone}} + \\beta_{12}\\text{income}_i\\cdot\\text{cost}_{\\text{New-Phone}} + \\beta_{21}\\text{ speed}_{\\text{New-Phone}} + \\beta_{22}\\text{income}_i\\cdot\\text{ speed}_{\\text{New-Phone}}\\\\ \\end{array} \\] The difference of the two utilities in turn is: \\[ \\begin{array}{l} V_{\\text{Do-Nothing}} - V_{\\text{New-Phone}} =\\\\ \\beta_{11}(\\text{cost}_{\\text{Do-Nothing}} - \\text{cost}_{\\text{New-Phone}}) + \\beta_{12}\\text{income}_i\\cdot(\\text{cost}_{\\text{Do-Nothing}} - \\text{cost}_{\\text{New-Phone}} ) \\\\ + \\beta_{21}(\\text{ speed}_{\\text{Do-Nothing}} - \\text{ speed}_{\\text{New-Phone}}) + \\beta_{22}\\text{income}_i\\cdot(\\text{ speed}_{\\text{Do-Nothing}} - \\text{ speed}_{\\text{New-Phone}}) - \\mu \\end{array} \\] Specifying the utility functions is more art than technique. We will return to this when we begin the practice of model estimation. 4.11 Exercise Answer the following questions. 4.11.1 Questions What do we mean when we say that the logit probability has a closed form? Why is it that we can set the dispersion parameter in the logit probabilities to one? Suppose that a choice set consists of two alternatives, travel by car (\\(c\\)) and travel by blue bus (\\(bb\\)). The utilities of these two modes are the same, that is: \\[ V_c = V_{bb} \\] What are the probabilities of choosing these two modes? Suppose that the transit operator decides to introduce a new service, namely a red bus. This red bus is identical to the blue bus in every respect except the color. Under these new conditions, what are the logit probabilities of choosing these modes? Discuss the results of introducing a new mode in the choice process above. References "],
["chapter-4.html", "Chapter 5 Practical specification and estimation 5.1 Theory and practice 5.2 How to use this note 5.3 Learning objectives 5.4 Suggested readings 5.5 Preliminaries 5.6 The anatomy of utility functions 5.7 Example: Specifying the utility functions 5.8 Estimation 5.9 Example: A logit model of mode choice 5.10 Comparing models: McFadden’s \\(\\rho^2\\) 5.11 Comparing models: the likelihood ratio test 5.12 Exercise", " Chapter 5 Practical specification and estimation “In theory, there is no difference between theory and practice. But in practice, there is.” — Benjamin Brewster “An ounce of practice is generally worth more than a ton of theory. — E.F. Schumacher 5.1 Theory and practice Chapters 3 and 4 presented a conceptual framework (a theory of behavior) and the necessary apparatus (based on probability theory) to implement the conceptual framework. This theoretical introduction was necessary to begin work from a solid foundation, and it provides an intuitive and elegant framework to study decision-making, and a powerful one too; Daniel McFadden was awarded the Sveriges Riksbank Prize in Economic Sciences (Nobel Prize) for his contributions to random utility modelling. Although not described in detail in previous chapters, it is worthwhile to dwell for a moment on the history of the development of the logit model as a random utility model. In his Nobel Lecture, McFadden (2001) recounts the path that led to the development of random utility models for discrete choices. Like most important discoveries, it is a meandering path. It began early in the 20th century with a theory for economic behavior (i.e., utility) that considered heterogeneous preferences that in practice were difficult to verify empirically because of data limitations. Indeed, studies before the 1960s mostly considered aggregated demand with representative agents (i.e., archetypical consumers) to accommodate this limitation in data availability. It was only when individual-level data became more widely collected and within reach of researchers that it became possible to pay attention to the behavior of individual agents. While economists were busy with models of aggregated demand, research in psychometrics and mathematical psychology by L.L Thurstone and R.D. Luce was busy providing the technical basis for modelling what Thurstone termed Comparative Judgement (in the sense of making a decision or forming an opinion). In particular, Luce introduced the axiom of Independence of Irrelevant Alternatives (discussed in Chapter 4). According to McFadden (2001, p. 353), this axiom “simplified experimental collection of choice data by allowing multinomial choice probabilities to be inferred from binomial choice experiments.” J. Marschak was the first to introduce the work of Thurstone to econometrics in 1960, and also the author of the term Random Utility Maximizing (RUM) that eventually prevailed over the comparative judgement terminology of Thurstone. McFadden’s early contributions to this body of research was developing an econometric version of Luce’s model, with strict (i.e., systematic) utilities specified as functions of the attributes of the alternatives and linking unobserved preference heterogeneity to a fully consistent description of the distribution of demands. Since the 1970s, discrete choice analysis has been a burgeoning area of research with a plethora of applications in economics, marketing, and travel behavior, among many others. This brief story neatly illustrates the complex interplay between theory and practice. Early attempts to study demand were limited due to practical considerations (i.e., the absence of data at the individual level). Once appropriate data became available, new studies continued to push the theoretical envelope. Indeed, theoretical questions have continued to inspire newer way to collect data and novel methods, and these in turn have helped us to refine our understanding of behavior. See as an example the work on decision-making in social situations (Akerlof 1997; K. Axhausen 2005; A. Páez and Scott 2007) which inspired the use of new data sources (e.g., J. A. Carrasco et al. 2008 K. W. Axhausen (2008); Scott et al. 2012; Chen and Mahmassani 2016) as well as novel modelling approaches (e.g., Dugundji and Walker 2005; Dugundji and Gulyas 2013; Kamargianni, Ben-Akiva, and Polydoropoulou 2014) and empirical work (e.g., Berg, Arentze, and Timmermans 2009; Goetzke and Rave 2011; Matous 2017). Now that the preceding chapters have armed us with the theory and basic concepts to implement random utility modelling, it is proper that we turn our attention to the practical aspects of modelling. The best way to ensure that the concepts take hold, in my view, is to get your hands on a dataset and struggle with the practicalities of cleaning and organizing data, specifying the utility functions (a task that is more art than science), and estimating models. These skills are mostly transferable to other modelling techniques, so we will begin by applying them to the most fundamental discrete choice model, the multinomial logit. 5.2 How to use this note Remember that the source for the document you are reading is an R Notebook. Throughout the notes, you will find examples of code in segments of text called chunks. This is an example of a chunk: print(&quot;Hats off to you, Prof. McFadden&quot;) ## [1] &quot;Hats off to you, Prof. McFadden&quot; If you are working with the Notebook version of the document, you can run the code by clicking the ‘play’ icon on the top right corner of the chunk. If you are reading the web-book version of the document, you will often see that the code has already been executed. You can still try it by copying and pasting into your R or RStudio console. 5.3 Learning objectives In this practice, you will learn about: Specification of utility functions. Maximum likelihood estimation. Estimation of multinomial logit models. McFadden’s \\(\\rho^2\\) The likelihood ratio test. 5.4 Suggested readings Ben-Akiva, M. Lerman, (1985) Discrete Choice Analysis: Theory and Applications to Travel Demand, Chapters 4 and 5, MIT Press. Hensher, D.A., Rose, J.M., Greene, W.H (2005) Applied Choice Analysis: A Primer, Chapter 10, Cambridge University Press. Ortuzar JD, Willumsen LG (2011) Modelling Transport, Fourth Edition, Chapter 8, John Wiley and Sons. Train (2009) Discrete Choice Methods with Simulation, Second Edition, Chapter 3, Cambridge University Press. 5.5 Preliminaries Load the packages used in this section: library(tidyverse) library(evd) library(mlogit) library(kableExtra) library(plotly) Load the dataset used in this section: load(&quot;Commute Mac.RData&quot;) 5.6 The anatomy of utility functions At the end Chapter 4 we took, for the first time, a closer look at the systematic utilities of discrete choice models. It is useful to think about the anatomy of a typical systematic utility function. Previously, we said that some variables vary across utility functions; these are typically the attributes that describe the various alternatives (e.g., level of service and cost). The variables that describe the decision-maker do not vary by alternative. This has implications, as seen before, for how the variables are entered into the functions. Since the model works on the basis of differences between utilities, the attributes must actually measure different levels of something or vanish. We will describe the utilities in terms of the way different variables are introduced in the utility functions. As before, we will assume that the location parameters of the distribution are absorbed by \\(J-1\\) utility functions (where \\(J\\) is the number of alternatives). Consider first variables that vary across alternatives. These variables can have a generic coefficient or they can have alternative-specific coefficients, as seen here: $$ \\[\\begin{array}{l} V_{i1} =\\\\ V_{i2} =\\\\ V_{i3} =\\\\ \\end{array}\\] ^ _{} ^{} $$ In many cases it sensible to have generic coefficients. For instance, if the variable is cost, we might assume that one dollar is valued equally irrespective of the it is spent on alternative. In other cases, alternative-specific coefficients might be informative. For instance, a consistent finding is that time spent traveling by public transportation is perceived as being more expensive than time traveling by car. Occasionally, as well, an attribute might be specific to an alternative: for instance, waiting time is often implicitly zero for travel by car and active modes of transportation (i.e., walking and cycling). The differences of the utilities are as follows: \\[ \\begin{array}{lll} V_{i2}-V_{i1}=&amp;(\\mu_2 - 0) &amp; + &amp;\\beta_1(x_{i2} - x_{i1}) &amp; + &amp;(\\delta_2w_{i2} - \\delta_1w_{i1})\\\\ V_{i3}-V_{i1}=&amp;(\\mu_3 - 0) &amp; + &amp;\\beta_1(x_{i2} - x_{i1}) &amp; + &amp;(\\delta_3w_{i3} - \\delta_1w_{i1})\\\\ V_{i3}-V_{i2}=&amp;(\\mu_3- \\mu_2) &amp; + &amp;\\beta_1(x_{i2} - x_{i1}) &amp; + &amp;(\\delta_3w_{i3} - \\delta_2w_{i2})\\\\ \\end{array} \\] Variables that vary across individuals but not by alternative can be introduced with alternative-specific coefficients: $$ \\[\\begin{array}{l} V_{i1} =\\\\ V_{i2} =\\\\ V_{i3} =\\\\ \\end{array}\\] ^ _{} ^{} _ $$ Following the example above, the differences of utilities are: \\[ \\begin{array}{lll} V_{i2}-V_{i1}=&amp;(\\mu_2 - 0) &amp; + &amp;\\beta_1(x_{i2} - x_{i1}) &amp; + &amp;(\\delta_2w_{i2} - \\delta_1w_{i1}) &amp; + &amp;(\\gamma_2 - 0)z_i\\\\ V_{i3}-V_{i1}=&amp;(\\mu_3 - 0) &amp; + &amp;\\beta_1(x_{i2} - x_{i1}) &amp; + &amp;(\\delta_3w_{i3} - \\delta_1w_{i1}) &amp; + &amp;(\\gamma_3 - 0)z_i\\\\ V_{i3}-V_{i2}=&amp;(\\mu_3- \\mu_2) &amp; + &amp;\\beta_1(x_{i2} - x_{i1}) &amp; + &amp;(\\delta_3w_{i3} - \\delta_2w_{i2}) &amp; + &amp;(\\gamma_3 - \\gamma_2)z_i\\\\ \\end{array} \\] As an alternative, individual-level variables can be introduced as part of an expansion of some coefficients, for example: $$ \\[\\begin{array}{l} V_{i1} =\\\\ V_{i2} =\\\\ V_{i3} =\\\\ \\end{array}\\] ^ _{} ^{} $$ The above expands to: $$ \\[\\begin{array}{l} V_{i1} =\\\\ V_{i2} =\\\\ V_{i3} =\\\\ \\end{array}\\] ^ _{} ^{} $$ And so the differences in utilities are: \\[ \\begin{array}{lll} V_{i2}-V_{i1}=&amp;(\\mu_2 - 0) &amp; + &amp;\\beta_{11}(x_{i2} - x_{i1}) &amp; + &amp;\\beta_{11}(z_ix_{i2} - z_ix_{i1}) &amp; + &amp;(\\delta_2w_{i2} - \\delta_1w_{i1})\\\\ V_{i3}-V_{i1}=&amp;(\\mu_3 - 0) &amp; + &amp;\\beta_{11}(x_{i2} - x_{i1}) &amp; + &amp;\\beta_{11}(z_ix_{i3} - z_ix_{i1}) &amp; + &amp;(\\delta_3w_{i3} - \\delta_1w_{i1})\\\\ V_{i3}-V_{i2}=&amp;(\\mu_3- \\mu_2) &amp; + &amp;\\beta_{11}(x_{i2} - x_{i1}) &amp; + &amp;\\beta_{11}(z_ix_{i3} - z_ix_{i2}) &amp; + &amp;(\\delta_3w_{i3} - \\delta_2w_{i2})\\\\ \\end{array} \\] Understanding the anatomy of utility functions is essential to properly specify and estimate models. 5.7 Example: Specifying the utility functions We will now proceed to work with a practical example, using the dataset that you encountered before in Chapter 2. This dataset contains information on various modes of transportation used by people commuting to McMaster University in Canada (Whalen, Páez, and Carrasco 2013). The dataset was loaded above as part of the preliminaries of this chapter. We can begin by exploring the data. First, we notice that this is a dataframe that has been prepared for use with the mlogit package: class(mc_commute) ## [1] &quot;mlogit.data&quot; &quot;data.frame&quot; Please note that this is the same dataset that you used in Chapter 2, but not the same file. For convenience, the dataset was pre-organized for use with mlogit. The contents of the dataframe can be quickly seen by means of the function head(). This function will display the first few top rows of the dataframe: head(mc_commute, 8) ## id choice HSR.access HSR.wait HSR.transfer parking vehind ## 1.Cycle 566872636 FALSE 3 15 0 No No ## 1.Walk 566872636 FALSE 3 15 0 No No ## 1.HSR 566872636 TRUE 3 15 0 No No ## 1.Car 566872636 FALSE 3 15 0 No No ## 2.Cycle 566873140 FALSE 4 15 0 No Yes ## 2.Walk 566873140 FALSE 4 15 0 No Yes ## 2.HSR 566873140 TRUE 4 15 0 No Yes ## 2.Car 566873140 FALSE 4 15 0 No Yes ## gender age shared family child ## 1.Cycle Male 21 Living in Shared Accommodations No No ## 1.Walk Male 21 Living in Shared Accommodations No No ## 1.HSR Male 21 Living in Shared Accommodations No No ## 1.Car Male 21 Living in Shared Accommodations No No ## 2.Cycle Male 23 No No No ## 2.Walk Male 23 No No No ## 2.HSR Male 23 No No No ## 2.Car Male 23 No No No ## street_density sidewalk_density LAT LONG alt available ## 1.Cycle 14.37621 22.63322 43.26302 -79.90074 Car No ## 1.Walk 14.37621 22.63322 43.26302 -79.90074 Cycle No ## 1.HSR 14.37621 22.63322 43.26302 -79.90074 HSR Yes ## 1.Car 14.37621 22.63322 43.26302 -79.90074 Walk Yes ## 2.Cycle 19.49754 39.64003 43.25885 -79.90476 Car Yes ## 2.Walk 19.49754 39.64003 43.25885 -79.90476 Cycle No ## 2.HSR 19.49754 39.64003 43.25885 -79.90476 HSR Yes ## 2.Car 19.49754 39.64003 43.25885 -79.90476 Walk Yes ## time chid ## 1.Cycle 1.000000e+05 1 ## 1.Walk 6.211180e+00 1 ## 1.HSR 5.000000e+00 1 ## 1.Car 2.131439e+01 1 ## 2.Cycle 2.000000e+00 2 ## 2.Walk 3.726708e+00 2 ## 2.HSR 1.000000e+01 2 ## 2.Car 1.278863e+01 2 As you can see, the dataframe has been organized in a particular way. Now, instead of each row being an individual, each row is a choice situation. Since there are four alternatives in this case, each row corresponds to the choice situation for an alternative for an individual. We notice that the row names now have the format #.Alt, where # is the number of the decision maker and Alt is the name of the alternative. In this way the first four rows of the table correspond to the first decision-maker who, faced with four alternatives, chose HSR (public transportation) - as recorded in the column choice. The next four rows correspond to the second decision-maker in the sample (who also chose HSR), and so on, four rows per decision-maker. More generally, there will be \\(J\\) ro. And so on. The first step is to specify the utility functions for the desired model. The package mlogit uses for formuals mFormula objects that build upon the Formula package for multi-component formulas. As seen above, utility functions can potentially have multiple components, so the utilities to build formulas are quite useful. Formulas for the mlogit package are defined using three parts: \\[ \\text{choice} \\sim \\text{alternative specific vars with generic coefficients }|\\text{ individual specific vars }|\\text{ alternative specific vars with specific coefficients} \\] If we list all columns in the dataframe, we can see what variables are available for this analysis: colnames(mc_commute) ## [1] &quot;id&quot; &quot;choice&quot; &quot;HSR.access&quot; ## [4] &quot;HSR.wait&quot; &quot;HSR.transfer&quot; &quot;parking&quot; ## [7] &quot;vehind&quot; &quot;gender&quot; &quot;age&quot; ## [10] &quot;shared&quot; &quot;family&quot; &quot;child&quot; ## [13] &quot;street_density&quot; &quot;sidewalk_density&quot; &quot;LAT&quot; ## [16] &quot;LONG&quot; &quot;alt&quot; &quot;available&quot; ## [19] &quot;time&quot; &quot;chid&quot; Besides identifier variable id and chid, and the variable for choice, we see that several variables are specific to the individual decision-makers. These are parking (availability of a parking pass), vehind (whether the decision-maker has individual access to a private vehicle), gender, age, shared (living in shared accommodations away from the family home), family (living at the family home), and child (minors are present in the household). Furthermore, some variables relate to the physical environment of the place of residence (street_density and sidewalk_density), in addition to the coordinates of the place of residence (geocoded to the nearest major intersection or postal code centroid). One variable is alternative specific, namely time (travel time in minutes). And three variables are specific to public transportation, namely HSR.access (access time to public transportation in minutes), HSR.wait (waiting time in minutes), and HSR.transfer (number of transfers when traveling by public transportation). We can begin by defining a very simple formula that considers only travel time. We will call this f1: f1 &lt;- mFormula(choice ~ time) The function model.matrix allows us to see how the formula is applied to the data (we use head() to display only the top rows of the model matrix): head(model.matrix(f1, mc_commute), 8) ## Walk:(intercept) HSR:(intercept) Car:(intercept) time ## 1.Cycle 0 0 0 1.000000e+05 ## 1.Walk 1 0 0 6.211180e+00 ## 1.HSR 0 1 0 5.000000e+00 ## 1.Car 0 0 1 2.131439e+01 ## 2.Cycle 0 0 0 2.000000e+00 ## 2.Walk 1 0 0 3.726708e+00 ## 2.HSR 0 1 0 1.000000e+01 ## 2.Car 0 0 1 1.278863e+01 We can see that the formula includes by default the alternative specific coefficients, in this case using as a reference cycling. The corresponding utility functions are as follows: \\[ \\begin{array}{l} V_{i\\text{Cycle}} =\\\\ V_{i\\text{Walk}} =\\\\ V_{i\\text{HSR}} =\\\\ V_{i\\text{HSR}} =\\\\ \\end{array} \\overbrace{ \\begin{array}{lll} 0 &amp; +0 &amp; +0\\\\ \\mu_{\\text{Walk}} &amp; +0 &amp; +0\\\\ 0 &amp; +\\mu_{\\text{HSR}} &amp; +0 \\\\ 0 &amp; +0 &amp; +\\mu_{\\text{Car}}\\\\ \\end{array} }^\\text{alternative specific constants} \\underbrace{\\begin{array}{lll} +\\beta_1\\text{time}_{i\\text{Cycle}}\\\\ +\\beta_1\\text{time}_{i\\text{Walk}}\\\\ +\\beta_1\\text{time}_{i\\text{HSR}}\\\\ +\\beta_1\\text{time}_{i\\text{Car}}\\\\ \\end{array} }_{\\text{alternative vars. with generic coefficients}} \\] Define now a formula with an individual-specific variable, say age, and call it f2: f2 &lt;- mFormula(choice ~ time | age) The model matrix is now: head(model.matrix(f2, mc_commute), 8) ## Walk:(intercept) HSR:(intercept) Car:(intercept) time ## 1.Cycle 0 0 0 1.000000e+05 ## 1.Walk 1 0 0 6.211180e+00 ## 1.HSR 0 1 0 5.000000e+00 ## 1.Car 0 0 1 2.131439e+01 ## 2.Cycle 0 0 0 2.000000e+00 ## 2.Walk 1 0 0 3.726708e+00 ## 2.HSR 0 1 0 1.000000e+01 ## 2.Car 0 0 1 1.278863e+01 ## Walk:age HSR:age Car:age ## 1.Cycle 0 0 0 ## 1.Walk 21 0 0 ## 1.HSR 0 21 0 ## 1.Car 0 0 21 ## 2.Cycle 0 0 0 ## 2.Walk 23 0 0 ## 2.HSR 0 23 0 ## 2.Car 0 0 23 And the utility functions are therefore: \\[ \\begin{array}{l} V_{i\\text{Cycle}} =\\\\ V_{i\\text{Walk}} =\\\\ V_{i\\text{HSR}} =\\\\ V_{i\\text{HSR}} =\\\\ \\end{array} \\overbrace{ \\begin{array}{lll} 0 &amp; +0 &amp; +0\\\\ \\mu_{\\text{Walk}} &amp; +0 &amp; +0\\\\ 0 &amp; +\\mu_{\\text{HSR}} &amp; +0 \\\\ 0 &amp; +0 &amp; +\\mu_{\\text{Car}}\\\\ \\end{array} }^\\text{alternative specific constants} \\underbrace{\\begin{array}{lll} +\\beta_1\\text{time}_{i\\text{Cycle}}\\\\ +\\beta_1\\text{time}_{i\\text{Walk}}\\\\ +\\beta_1\\text{time}_{i\\text{HSR}}\\\\ +\\beta_1\\text{time}_{i\\text{Car}}\\\\ \\end{array} }_{\\text{alternative vars. with generic coefficients}} \\overbrace{ \\begin{array}{lll} 0 &amp; +0 &amp; +0\\\\ \\gamma_{1}\\text{age}_{i} &amp; +0 &amp; +0\\\\ 0 &amp; + \\gamma_{2}\\text{age}_{i} &amp; +0 \\\\ 0 &amp; +0 &amp; +\\gamma_{3}\\text{age}_{i}\\\\ \\end{array} }^\\text{individual vars with specific coefficients} \\] Lets try a different formula, where time has alternative-specific instead of generic coefficients, and call it f3: f3 &lt;- mFormula(choice ~ 0 | age | time) Note that, since we do not define other alternative-specific variables with generic coefficients, we have to explicitly state that there are 0 such variables! This formula leads to the following model matrix: head(model.matrix(f3, mc_commute), 8) ## Walk:(intercept) HSR:(intercept) Car:(intercept) Walk:age HSR:age ## 1.Cycle 0 0 0 0 0 ## 1.Walk 1 0 0 21 0 ## 1.HSR 0 1 0 0 21 ## 1.Car 0 0 1 0 0 ## 2.Cycle 0 0 0 0 0 ## 2.Walk 1 0 0 23 0 ## 2.HSR 0 1 0 0 23 ## 2.Car 0 0 1 0 0 ## Car:age Cycle:time Walk:time HSR:time Car:time ## 1.Cycle 0 1e+05 0.000000 0 0.00000 ## 1.Walk 0 0e+00 6.211180 0 0.00000 ## 1.HSR 0 0e+00 0.000000 5 0.00000 ## 1.Car 21 0e+00 0.000000 0 21.31439 ## 2.Cycle 0 2e+00 0.000000 0 0.00000 ## 2.Walk 0 0e+00 3.726708 0 0.00000 ## 2.HSR 0 0e+00 0.000000 10 0.00000 ## 2.Car 23 0e+00 0.000000 0 12.78863 The utility functions for this are: $$ \\[\\begin{array}{l} V_{i\\text{Cycle}} =\\\\ V_{i\\text{Walk}} =\\\\ V_{i\\text{HSR}} =\\\\ V_{i\\text{HSR}} =\\\\ \\end{array}\\] ^ _ ^{} $$ Given the utility functions, the logit probabilities for each alternative are: \\[ \\begin{array}{l} P(\\text{Cycle}) = \\frac{e^{V_{\\text{Cycle}}}}{e^{V_{\\text{Cycle}}}+e^{V_{\\text{Walk}}}+e^{V_{\\text{HSR}}}+e^{V_{\\text{Car}}}}\\\\ P(\\text{Walk}) = \\frac{e^{V_{\\text{Walk}}}}{e^{V_{\\text{Cycle}}}+e^{V_{\\text{Walk}}}+e^{V_{\\text{HSR}}}+e^{V_{\\text{Car}}}}\\\\ P(\\text{HSR}) = \\frac{e^{V_{\\text{Cycle}}}}{e^{V_{\\text{Cycle}}}+e^{V_{\\text{Walk}}}+e^{V_{\\text{HSR}}}+e^{V_{\\text{Car}}}}\\\\ P(\\text{Car}) =1 - P(\\text{Cycle}) - P(\\text{Walk}) - P(\\text{HSR})\\\\ \\end{array} \\] The utility functions depend on the data but also on the coefficients, which we do not know a priori. Rather, these must be retrieved from the sample, as discussed next. 5.8 Estimation Before we can calculate the choice probabilities, we need to somehow obtain coefficients for the utility functions. The process to do so is called estimation, and it involves the use of a statistical sample. To estimate the coefficients of a model we need to define a criterion. Estimates can take an infinite number of values, after all, so our criterion must be optimal in some sense - in this way, once that we estimate the coefficients we can be satisfied that the coefficients are the best that we can obtain given then inputs. A common criterion used to estimate discrete choice models is the likelihood. So what is this likelihood? Previously we encountered probability distribution functions. These functions were defined by parameters (such as the location parameter and the dispersion parameter). Given the parameters, it is possible to calculate the probability of values for a variable \\(x\\). A likelihood function is a similar concept, except that whereas in the probability functions the parameters were given, in a likelihood function the data are given and the parameters need to be obtained from the function. The relevant likelihood function for the multinomial logit model is as follows: \\[ L = \\prod_{i=n}^N\\prod_{j=1}^J P_{ij}^{y_{ij}} \\] where \\(P_{ij}\\) is the probability of decision-maker \\(i\\) selecting alternative \\(j\\) and \\(y_{ij}\\) is an indicator variable that takes the value of \\(1\\) if individual \\(i\\) chose alternative \\(j\\) and \\(0\\) otherwise. The effect of the indicator variable is to turn the probabilities on and off, since \\(P^0 = 1\\) and \\(P^1 = P\\). Notice that the likelihood function is bounded between 0 and 1, but in the case of the logit model is never exactly zero nor one, since the logit probabilities never thake those values. Lets explore the behavior of this function by means of a simple example, with the binomial logit (i.e., only two alternative in the choice set), in which case the likelihood function becomes: \\[ L = \\prod_{i=n}^N P_{iA}^{y_{iA}}P_{iB}^{y_{iB}} = \\Bigg(\\frac{e^{V_{iA}}}{e^{V_{iA}} + e^{V_{iB}}}\\Bigg)^{y_{iA}} \\Bigg(\\frac{e^{V_{iB}}}{e^{V_{iA}} + e^{V_{iB}}}\\Bigg)^{y_{iB}} \\] The utility functions \\(V_{iA}\\) and \\(V_{iB}\\) depend on the data, which we know (since we have a statistical sample), and the coefficients, which we do not know. For the example, we have the following toy sample with six individuals: Individual Choice yiA yiB xiA xiB 1 A 1 0 5 4 2 A 1 0 2 5 3 B 0 1 5 2 4 A 1 0 1 6 5 B 0 1 4 1 6 B 0 1 3 4 Based on this sample, we can specify the utility functions in this fashion: \\[ \\begin{array}{l} V_{iA} = 0 &amp;+&amp; \\beta x_{iA}\\\\ V_{iB} = \\mu &amp;+&amp; \\beta x_{iB}\\\\ \\end{array} \\] These utility functions are very similar to the first set of utility function we defined in the preceding section for the case of mode choice. Next, lets write the likelihood function for this toy sample, as a function of \\(\\mu\\) and \\(\\beta\\) and calculate the likelihood initially setting \\(\\mu\\) and \\(\\beta\\) to zero. We will call this “Experiment 1”: mu &lt;- 0 beta &lt;- 0 P1A_1 &lt;- (exp(beta * ts$xiA[1])/(exp(beta * ts$xiA[1]) + exp(mu + beta * ts$xiB[1]))) P1B_1 &lt;- (exp(mu + beta * ts$xiB[1])/(exp(beta * ts$xiA[1]) + exp(mu + beta * ts$xiB[1]))) P2A_1 &lt;- (exp(beta * ts$xiA[2])/(exp(beta * ts$xiA[2]) + exp(mu + beta * ts$xiB[2]))) P2B_1 &lt;- (exp(mu + beta * ts$xiB[2])/(exp(beta * ts$xiA[2]) + exp(mu + beta * ts$xiB[2]))) P3A_1 &lt;- (exp(beta * ts$xiA[3])/(exp(beta * ts$xiA[3]) + exp(mu + beta * ts$xiB[3]))) P3B_1 &lt;- (exp(mu + beta * ts$xiB[3])/(exp(beta * ts$xiA[3]) + exp(mu + beta * ts$xiB[3]))) P4A_1 &lt;- (exp(beta * ts$xiA[4])/(exp(beta * ts$xiA[4]) + exp(mu + beta * ts$xiB[4]))) P4B_1 &lt;- (exp(mu + beta * ts$xiB[4])/(exp(beta * ts$xiA[4]) + exp(mu + beta * ts$xiB[4]))) P5A_1 &lt;- (exp(beta * ts$xiA[5])/(exp(beta * ts$xiA[5]) + exp(mu + beta * ts$xiB[5]))) P5B_1 &lt;- (exp(mu + beta * ts$xiB[5])/(exp(beta * ts$xiA[5]) + exp(mu + beta * ts$xiB[5]))) P6A_1 &lt;- (exp(beta * ts$xiA[6])/(exp(beta * ts$xiA[6]) + exp(mu + beta * ts$xiB[6]))) P6B_1 &lt;- (exp(mu + beta * ts$xiB[6])/(exp(beta * ts$xiA[6]) + exp(mu + beta * ts$xiB[6]))) L &lt;- P1A_1^ts$yiA[1] * P1B_1^ts$yiB[1] * P2A_1^ts$yiA[2] * P2B_1^ts$yiB[2] * P3A_1^ts$yiA[3] * P3B_1^ts$yiB[3] * P4A_1^ts$yiA[4] * P4B_1^ts$yiB[4] * P5A_1^ts$yiA[5] * P5B_1^ts$yiB[5] * P6A_1^ts$yiA[6] * P6B_1^ts$yiB[6] # Create data frame to tabulate results: df &lt;- data.frame(Individual = c(1, 2, 3, 4, 5, 6), Choice = c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;), PA = c(P1A_1, P2A_1, P3A_1, P4A_1, P5A_1, P6A_1), PB = c(P1B_1, P2B_1, P3B_1, P4B_1, P5B_1, P6B_1)) kable(df, &quot;html&quot;, digits = 4, align = &quot;c&quot;) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;)) %&gt;% footnote(general = paste(&quot;The value of the likelihood function is &quot;, round(L, digits = 4))) Individual Choice PA PB 1 A 0.5 0.5 2 A 0.5 0.5 3 B 0.5 0.5 4 A 0.5 0.5 5 B 0.5 0.5 6 B 0.5 0.5 Note: The value of the likelihood function is 0.0156 As you can see, that the logit probabilities when all coefficients are zero is \\(0.5\\). By setting the coefficients to zero we have defined what is called a null model. Since the variables are set to zero, this model has no useful information to estimate the probability, and therefore it assigns equal probabilities to all alternative. The likelihood is a relatively small value. Now lets change the coefficients (call this “Experiment 2”): mu &lt;- 0.5 # -0.5 beta &lt;- -1.5 # -0.5 P1A_2 &lt;- (exp(beta * ts$xiA[1])/(exp(beta * ts$xiA[1]) + exp(mu + beta * ts$xiB[1]))) P1B_2 &lt;- (exp(mu + beta * ts$xiB[1])/(exp(beta * ts$xiA[1]) + exp(mu + beta * ts$xiB[1]))) P2A_2 &lt;- (exp(beta * ts$xiA[2])/(exp(beta * ts$xiA[2]) + exp(mu + beta * ts$xiB[2]))) P2B_2 &lt;- (exp(mu + beta * ts$xiB[2])/(exp(beta * ts$xiA[2]) + exp(mu + beta * ts$xiB[2]))) P3A_2 &lt;- (exp(beta * ts$xiA[3])/(exp(beta * ts$xiA[3]) + exp(mu + beta * ts$xiB[3]))) P3B_2 &lt;- (exp(mu + beta * ts$xiB[3])/(exp(beta * ts$xiA[3]) + exp(mu + beta * ts$xiB[3]))) P4A_2 &lt;- (exp(beta * ts$xiA[4])/(exp(beta * ts$xiA[4]) + exp(mu + beta * ts$xiB[4]))) P4B_2 &lt;- (exp(mu + beta * ts$xiB[4])/(exp(beta * ts$xiA[4]) + exp(mu + beta * ts$xiB[4]))) P5A_2 &lt;- (exp(beta * ts$xiA[5])/(exp(beta * ts$xiA[5]) + exp(mu + beta * ts$xiB[5]))) P5B_2 &lt;- (exp(mu + beta * ts$xiB[5])/(exp(beta * ts$xiA[5]) + exp(mu + beta * ts$xiB[5]))) P6A_2 &lt;- (exp(beta * ts$xiA[6])/(exp(beta * ts$xiA[6]) + exp(mu + beta * ts$xiB[6]))) P6B_2 &lt;- (exp(mu + beta * ts$xiB[6])/(exp(beta * ts$xiA[6]) + exp(mu + beta * ts$xiB[6]))) L &lt;- P1A_2^ts$yiA[1] * P1B_2^ts$yiB[1] * P2A_2^ts$yiA[2] * P2B_2^ts$yiB[2] * P3A_2^ts$yiA[3] * P3B_2^ts$yiB[3] * P4A_2^ts$yiA[4] * P4B_2^ts$yiB[4] * P5A_2^ts$yiA[5] * P5B_2^ts$yiB[5] * P6A_2^ts$yiA[6] * P6B_2^ts$yiB[6] # Create data frame to tabulate results: df &lt;- data.frame(Individual = c(1, 2, 3, 4, 5, 6), Choice = c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;), PA = c(P1A_2, P2A_2, P3A_2, P4A_2, P5A_2, P6A_2), PB = c(P1B_2, P2B_2, P3B_2, P4B_2, P5B_2, P6B_2)) kable(df, &quot;html&quot;, digits = 4, align = &quot;c&quot;) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;)) %&gt;% footnote(general = paste(&quot;The value of the likelihood function is &quot;, round(L, digits = 4))) Individual Choice PA PB 1 A 0.1192 0.8808 2 A 0.9820 0.0180 3 B 0.0067 0.9933 4 A 0.9991 0.0009 5 B 0.0067 0.9933 6 B 0.7311 0.2689 Note: The value of the likelihood function is 0.031 Notice how changing the coefficients has two effects, as expected: the probabilities change and the value of the likelihood function changes too. Inspect the probabilities and the value of the likelihood function with the new coefficients. What do you notice? If you are working with the R Notebook, at this point you can try changing the coefficients. Can you improve the value of the likelihood function, or maybe even make it worse? The likelihood function can be plotted as shown below. If you hover over the plot, you can see how the value of the likelihood changes as a function of \\(\\mu\\) and \\(\\beta\\): Figure 5.1: Likelihood function for toy dataset From Figure 4.1 we can see that the approximate values of the coefficients that maximize the likelihood function are \\(\\mu=0.10\\) and \\(\\beta=-0.65\\). If we use these coefficients to calculate the logit probabilities, we can compare to the probabilities of Experiments 1 and 2: # Approximate values that maximize the likelihood function. mu &lt;- 0.10 beta &lt;- -0.65 P1A_3 &lt;- (exp(beta * ts$xiA[1])/(exp(beta * ts$xiA[1]) + exp(mu + beta * ts$xiB[1]))) P1B_3 &lt;- (exp(mu + beta * ts$xiB[1])/(exp(beta * ts$xiA[1]) + exp(mu + beta * ts$xiB[1]))) P2A_3 &lt;- (exp(beta * ts$xiA[2])/(exp(beta * ts$xiA[2]) + exp(mu + beta * ts$xiB[2]))) P2B_3 &lt;- (exp(mu + beta * ts$xiB[2])/(exp(beta * ts$xiA[2]) + exp(mu + beta * ts$xiB[2]))) P3A_3 &lt;- (exp(beta * ts$xiA[3])/(exp(beta * ts$xiA[3]) + exp(mu + beta * ts$xiB[3]))) P3B_3 &lt;- (exp(mu + beta * ts$xiB[3])/(exp(beta * ts$xiA[3]) + exp(mu + beta * ts$xiB[3]))) P4A_3 &lt;- (exp(beta * ts$xiA[4])/(exp(beta * ts$xiA[4]) + exp(mu + beta * ts$xiB[4]))) P4B_3 &lt;- (exp(mu + beta * ts$xiB[4])/(exp(beta * ts$xiA[4]) + exp(mu + beta * ts$xiB[4]))) P5A_3 &lt;- (exp(beta * ts$xiA[5])/(exp(beta * ts$xiA[5]) + exp(mu + beta * ts$xiB[5]))) P5B_3 &lt;- (exp(mu + beta * ts$xiB[5])/(exp(beta * ts$xiA[5]) + exp(mu + beta * ts$xiB[5]))) P6A_3 &lt;- (exp(beta * ts$xiA[6])/(exp(beta * ts$xiA[6]) + exp(mu + beta * ts$xiB[6]))) P6B_3 &lt;- (exp(mu + beta * ts$xiB[6])/(exp(beta * ts$xiA[6]) + exp(mu + beta * ts$xiB[6]))) L &lt;- P1A_3^ts$yiA[1] * P1B_3^ts$yiB[1] * P2A_3^ts$yiA[2] * P2B_3^ts$yiB[2] * P3A_3^ts$yiA[3] * P3B_3^ts$yiB[3] * P4A_3^ts$yiA[4] * P4B_3^ts$yiB[4] * P5A_3^ts$yiA[5] * P5B_3^ts$yiB[5] * P6A_3^ts$yiA[6] * P6B_3^ts$yiB[6] # Create data frame to tabulate results: df &lt;- data.frame(Individual = c(1, 2, 3, 4, 5, 6), Choice = c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;), PA_1 = c(P1A_1, P2A_1, P3A_1, P4A_1, P5A_1, P6A_1), PB_1 = c(P1B_1, P2B_1, P3B_1, P4B_1, P5B_1, P6B_1), PA_1 = c(P1A_2, P2A_2, P3A_2, P4A_2, P5A_2, P6A_2), PB_1 = c(P1B_2, P2B_2, P3B_2, P4B_2, P5B_2, P6B_2), PA_1 = c(P1A_3, P2A_3, P3A_3, P4A_3, P5A_3, P6A_3), PB_1 = c(P1B_3, P2B_3, P3B_3, P4B_3, P5B_3, P6B_3)) kable(df, &quot;html&quot;, digits = 4, col.names = c(&quot;Individual&quot;, &quot;Choice&quot;, &quot;PA&quot;, &quot;PB&quot;, &quot;PA&quot;, &quot;PB&quot;, &quot;PA&quot;, &quot;PB&quot;), align = &quot;c&quot;) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;)) %&gt;% add_header_above(c(&quot; &quot; = 1, &quot; &quot; = 1, &quot;Experiment 1&quot; = 2, &quot;Experiment 2&quot; = 2, &quot;Approx Max Likelihood&quot; = 2)) Experiment 1 Experiment 2 Approx Max Likelihood Individual Choice PA PB PA PB PA PB 1 A 0.5 0.5 0.1192 0.8808 0.3208 0.6792 2 A 0.5 0.5 0.9820 0.0180 0.8641 0.1359 3 B 0.5 0.5 0.0067 0.9933 0.1141 0.8859 4 A 0.5 0.5 0.9991 0.0009 0.9589 0.0411 5 B 0.5 0.5 0.0067 0.9933 0.1141 0.8859 6 B 0.5 0.5 0.7311 0.2689 0.6341 0.3659 Maximizing the likelihood is a useful criterion to estimate the coefficients of the models, since this criterion provides the optimal probabilities of the right alternative being chosen - which does not necessarily mean that those probabilities will be high! In this toy example we “solved” the problem of maximizing the likelihood by hand. This is rather difficult, unfeasible even, in most applied situations with large samples and/or more than one variable. Fortunately, there are a number of numerical algorithms that can be used to maximize the likelihood. We will not discuss this in detail, but interested readers can consult Train [Train (2009); Section 3.7] for details. The mlogit package imports the package maxLik (Henningsen and Toomet 2011), which implements canonical algorithms including Newton-Raphson, the Berndt–Hall–Hall–Hausman (or BHHH), and the Broyden–Fletcher–Goldfarb–Shanno (or BFGS) algorithm. In practice, the algorithms above maximize not the likelihood function, but a transformation thereof, called the log-likelihood: \\[ l = \\sum_{i=n}^N\\sum_{j=1}^J y_{ij}log(P_{ij}) \\] Since the likelihood function is bound between zero and one, the log-likelihood is bound between minus infinity and zero. The value of the maximized log-likelihood function provides a useful diagnostic to compare models, since higher values are indicative of a better model. Several statistical tests (such as the likelihood ratio) can be used to test the hypothesis that a model is a significant improvement over other, and are thus useful for model selection purposes. Before these diagnostics, lets see how multinomial logit models are estimated using mlogit. 5.9 Example: A logit model of mode choice Coming back to the transportation mode choice dataset, we already defined some formulas (i.e., utility functions) that we can use to estimate a model. The function to estimate a model is mlogit(). This function requires at least two arguments: an mFormula object and a dataset. We can verify that the formulas we created above are of this class: class(f1) ## [1] &quot;mFormula&quot; &quot;Formula&quot; &quot;formula&quot; class(f2) ## [1] &quot;mFormula&quot; &quot;Formula&quot; &quot;formula&quot; class(f3) ## [1] &quot;mFormula&quot; &quot;Formula&quot; &quot;formula&quot; The value (output) of the function can be named and saved to an object for further analysis or post-estimation processing. Begin by estimating the model1 &lt;- mlogit(f1, mc_commute) summary(model1) ## ## Call: ## mlogit(formula = choice ~ time, data = mc_commute, method = &quot;nr&quot;, ## print.level = 0) ## ## Frequencies of alternatives: ## Cycle Walk HSR Car ## 0.034884 0.516715 0.244186 0.204215 ## ## nr method ## 6 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 4.14E-05 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## Walk:(intercept) 2.8006e+00 1.5419e-01 18.1632 &lt; 2.2e-16 *** ## HSR:(intercept) 1.7270e+00 1.5493e-01 11.1471 &lt; 2.2e-16 *** ## Car:(intercept) 1.8978e+00 1.6151e-01 11.7506 &lt; 2.2e-16 *** ## time -9.2134e-06 1.0585e-06 -8.7037 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -1509.6 ## McFadden R^2: 0.026441 ## Likelihood ratio test : chisq = 82 (p.value = &lt; 2.22e-16) The output of the function includes the estimated frequencies of alternatives in addition to information about the optimization procedure. For instance, the message “successive function values within tolerance limits” indicates that the algorithm converged normally. The output also reports the estimated values of the coefficients, along with standard errors, z-values, and p-values. Recall that the null hypothesis in this case is that the coefficient is zero. Small p-values can be used to reject the null hypothesis. In the present case, the null hypothesis can be comfortably rejected. This simple model includes three alternative-specific constants and one alternative-specific variable with a generic coefficient. The signs of the coefficients are informative. Since the reference mode is “Cycle”, the positive values of the constants indicate that, other things being equal, cycling is the least preferred mode, followed by HSR and then Car. The most preferred mode (again, other things being equal), is Walk. This is verified from the estimated frequencies of the modes. The negative coefficient for time indicates that time is a “cost”, in other words, the utility tends to decline with increasing travel times. This means that modes that tend to be slower will have lower utilities. Finally, the maximized value of the log-likelihood function is reported, along with two diagnostics, McFadden R^2 (in reality \\(\\rho^2\\)) and a likelihood ratio tests. We will come back to these diagnostics below, but first, lets estimate a model using the second formula. model2 &lt;- mlogit(f2, mc_commute) summary(model2) ## ## Call: ## mlogit(formula = choice ~ time | age, data = mc_commute, method = &quot;nr&quot;, ## print.level = 0) ## ## Frequencies of alternatives: ## Cycle Walk HSR Car ## 0.034884 0.516715 0.244186 0.204215 ## ## nr method ## 6 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 4.3E-05 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## Walk:(intercept) 4.7672e+00 6.9102e-01 6.8988 5.245e-12 *** ## HSR:(intercept) 1.9714e+00 6.5141e-01 3.0263 0.002476 ** ## Car:(intercept) 1.0572e+00 6.4186e-01 1.6471 0.099543 . ## time -9.0888e-06 1.0778e-06 -8.4326 &lt; 2.2e-16 *** ## Walk:age -9.0142e-02 2.9816e-02 -3.0233 0.002501 ** ## HSR:age -1.0414e-02 2.7417e-02 -0.3798 0.704064 ## Car:age 3.7357e-02 2.6923e-02 1.3876 0.165271 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -1480.4 ## McFadden R^2: 0.045309 ## Likelihood ratio test : chisq = 140.51 (p.value = &lt; 2.22e-16) Now there is an individual-specific variable in the model (i.e., age). Only one of those coefficients is significant at conventional levels (i.e., \\(p&lt;0.05\\)), and it is negative. Since the reference is “Cycle”, a negative value indicates that the utility of walking declines with age with respect to the utility of cycling. Two other coefficients for age (in the utility of HSR and CAR) are not significantly different from zero, meaning that age does not change the utility of travel by HSR and Car with respect to Cycle. Note that it is possible to select the reference level for the utilities when estimating the model. For example, lets reestimate the model above, but now using the utility of Walk as the reference: model2 &lt;- mlogit(f2, mc_commute, reflevel = &quot;Walk&quot;) summary(model2) ## ## Call: ## mlogit(formula = choice ~ time | age, data = mc_commute, reflevel = &quot;Walk&quot;, ## method = &quot;nr&quot;, print.level = 0) ## ## Frequencies of alternatives: ## Walk Cycle HSR Car ## 0.516715 0.034884 0.244186 0.204215 ## ## nr method ## 6 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 4.3E-05 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## Cycle:(intercept) -4.7672e+00 6.9102e-01 -6.8988 5.245e-12 *** ## HSR:(intercept) -2.7959e+00 4.1947e-01 -6.6652 2.644e-11 *** ## Car:(intercept) -3.7100e+00 4.1768e-01 -8.8825 &lt; 2.2e-16 *** ## time -9.0888e-06 1.0778e-06 -8.4326 &lt; 2.2e-16 *** ## Cycle:age 9.0142e-02 2.9816e-02 3.0233 0.002501 ** ## HSR:age 7.9728e-02 1.9135e-02 4.1666 3.091e-05 *** ## Car:age 1.2750e-01 1.8769e-02 6.7931 1.098e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -1480.4 ## McFadden R^2: 0.045309 ## Likelihood ratio test : chisq = 140.51 (p.value = &lt; 2.22e-16) Now all age-related coefficients are significant! Whereas some of them are not significantly different with respect to each other as seen above (e.g. Cycle and HSR), the are all significantly different from the reference. Since the coefficients are positive, this indicates that the utilities of cycling, using HSR, and traveling by car all increase with age with respect to walking. The value of the maximized log-likelihood and other diagnostics are identical, irrespective of which mode is selected as a utility. In essence, the models are the same, but they provide a different perspective on how some coefficients relate to each other across alternatives. We can visually explore how the probability of choosing different modes varies with age. First summarize the age variable: summary(mc_commute$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 17.00 20.00 21.00 22.08 23.00 60.00 Copy the dataframe used to estimate the model, but only enough columns to explore an age range of 10 years, say from 17 to 26. Since there are four alternatives, this means that we need fourty rows: mc_commute_predict &lt;- mc_commute[1:40,] Replace the age variable by values for ages 17 to 26: mc_commute_predict$age &lt;- rep(c(17:26), each = 4) Replace time by median travel time: mc_commute_predict$time &lt;- median(mc_commute$time) Next, predict the probabilities using the predict() function: probs &lt;- predict(model2, newdata = mc_commute_predict) The value (output) of predict is a 10-by-4 matrix that contains the probability for ten age values (i.e., 16, 17, 18, …, 26), and four modes (Walk, Cycle, HSR, Car). To facilitate plotting, we add the age values and then reshape that 10-by-4 matrix as follows: probs &lt;- data.frame(age = c(17:26), probs) %&gt;% gather(key = &quot;Mode&quot;, value = &quot;Probability&quot;, -age) By “gathering” the probabilities, now the data frame has one column with the mode and one column with the probability. We can then plot: ggplot(data = probs, aes(x = age, y = Probability, color = Mode)) + geom_line() We can see that the probability of walking (for a trip that takes the median duration in the sample) declines with age. The probability of using the three other modes increases with age, but more rapidly for car than for transit or cycling. 5.10 Comparing models: McFadden’s \\(\\rho^2\\) The log-likelihood reported in the summary of the model is useful as a measure of goodness of fit. Recall that the likelihood of this model is bounded between \\(0\\) and \\(1\\), and therefore the log-likelihood is bounded at the upper end by \\(0\\) (it is minus infinity at the lower end). We also know that higher values of the likelihood represent better fits. One simple diagnostic to compare the fit of models is McFadden’s \\(\\rho^2\\). This summary diagnostic is defined as follows: \\[ \\rho^2 = 1 - \\frac{l^*}{l_0} \\] where \\(l^*\\) is the value of the maximized log-likelihood and \\(l_0\\) is the value of the log-likelihood of a null model (perhaps without constants, or a constants only model). If the model is uninformative, its log-likelihood will tend to the likelihood of the null model. In this case \\(l^*/l_0\\) tends to one and therefore \\(\\rho^2\\) tends to zero. If the maximized log-likelihood of the model tends to 0 (the upper limit for the log-likelihood function), \\(\\rho^2\\) tends to one. Although \\(\\rho^2\\) is bounded between zero and one, similar to the coefficient of determination \\(R^2\\) in regression analysis, its interpretation is not the same as for \\(R^2\\). Whereas \\(R^2\\) is interpreted as the proportion of variance explained by the model, \\(\\rho^2\\) lacks such an interpretation. Also, the values of \\(\\rho^2\\) tend to be lower, and values of \\(0.4\\) are considered very good fits. The main utility of McFadden’s \\(\\rho^2\\) is as a quick way of comparing the relative fit of different models, rather than assessing the fit against an absolute value of goodness of fit. 5.11 Comparing models: the likelihood ratio test Another way to compare models is by means of the likelihood ratio test. This test compares the log-likelihood of two models to assess whether they are significantly different. The test follows the \\(\\chi^2\\) distribution with degrees of freedom equal to the difference in the number of coefficients between the two models. The test requires a base model and a full model, and the base model must nest within the full model. Nesting in this sense means that full model must be reducible to the base model by setting some coefficients to zero. For example, consider the utility functions of model2: \\[ \\begin{array}{l} V_{i\\text{Cycle}} = 0 &amp;+&amp; \\beta_1\\text{time}_{i\\text{Cycle}} &amp;+&amp; 0\\\\ V_{i\\text{Walk}} = \\mu_{\\text{Walk}} &amp;+&amp; \\beta_1\\text{time}_{i\\text{Walk}} &amp;+&amp; \\gamma_{1}\\text{age}_{i}\\\\ V_{i\\text{HSR}} = \\mu_{\\text{HSR}} &amp;+&amp; \\beta_1\\text{time}_{i\\text{HSR}} &amp;+&amp; \\gamma_{2}\\text{age}_{i}\\\\ V_{i\\text{HSR}} = \\mu_{\\text{Car}} &amp;+&amp; \\beta_1\\text{time}_{i\\text{Car}} &amp;+&amp; \\gamma_{3}\\text{age}_{i}\\\\ \\end{array} \\] We can reduce this model to model1 by setting \\(\\gamma_{1}=\\gamma_{2}=\\gamma_{3}=0\\): \\[ \\begin{array}{l} V_{i\\text{Cycle}} = 0 &amp;+&amp; \\beta_1\\text{time}_{i\\text{Cycle}}\\\\ V_{i\\text{Walk}} = \\mu_{\\text{Walk}} &amp;+&amp; \\beta_1\\text{time}_{i\\text{Walk}}\\\\ V_{i\\text{HSR}} = \\mu_{\\text{HSR}} &amp;+&amp; \\beta_1\\text{time}_{i\\text{HSR}}\\\\ V_{i\\text{HSR}} = \\mu_{\\text{Car}} &amp;+&amp; \\beta_1\\text{time}_{i\\text{Car}}\\\\ \\end{array} \\] In this way, model1 “nests” in model2. In the summary of the models, the likelihood ratio test is reported. See: summary(model1) ## ## Call: ## mlogit(formula = choice ~ time, data = mc_commute, method = &quot;nr&quot;, ## print.level = 0) ## ## Frequencies of alternatives: ## Cycle Walk HSR Car ## 0.034884 0.516715 0.244186 0.204215 ## ## nr method ## 6 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 4.14E-05 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## Walk:(intercept) 2.8006e+00 1.5419e-01 18.1632 &lt; 2.2e-16 *** ## HSR:(intercept) 1.7270e+00 1.5493e-01 11.1471 &lt; 2.2e-16 *** ## Car:(intercept) 1.8978e+00 1.6151e-01 11.7506 &lt; 2.2e-16 *** ## time -9.2134e-06 1.0585e-06 -8.7037 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -1509.6 ## McFadden R^2: 0.026441 ## Likelihood ratio test : chisq = 82 (p.value = &lt; 2.22e-16) In this case, the test is against the null model, that is, a model with no variables at all. This is the least informative of all models. When two non-null models need to be compared, the lrtest function implements the likelihood ratio test for two inputs, which are two mlogit models, as follows: lrtest(model1, model2) ## Likelihood ratio test ## ## Model 1: choice ~ time ## Model 2: choice ~ time | age ## #Df LogLik Df Chisq Pr(&gt;Chisq) ## 1 4 -1509.6 ## 2 7 -1480.4 3 58.512 1.222e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice that the number of degrees of freedom (Df) is \\(3\\): this is because there are three individual-specific parameters in model2 that are not present in model1. The null hypothesis of the test is that the log-likelihood of the two models is not different, in other words, that the alternate model is not an improvement over the base model. In the present case, the very small \\(p\\)-value leads us to reject the null hypothesis, and the conclusion is that model2, which includes age, is a significant improvement over model1, which does not. 5.12 Exercise In the example in this chapter we estimated the probabilities of choosing different modes by age setting travel time to the in-sample median. Calculate the probability of choosing the modes as a function of time for ages 17, 20, 23, and 26. Estimate a model using formula f3 (call it model3). Discuss the output of this model. Use the likelihood ratio test to compare model3 to model1. Can you use the likelihood ratio test to comare model3 to model2? Discuss. References "],
["chapter-5.html", "Chapter 6 Behavioral insights from choice models 6.1 Inferring and forecasting behavior 6.2 How to use this note 6.3 Learning objectives 6.4 Suggested readings 6.5 Preliminaries 6.6 The meaning of the coefficients 6.7 Marginal effects 6.8 Elasticity 6.9 Calculating elasticities based on an mlogit model 6.10 A note about attributes in dummy format 6.11 Willingness to pay and discount rate 6.12 Simulating market changes 6.13 Exercise", " Chapter 6 Behavioral insights from choice models “Men’s actions are the best guides to their thoughts.” — John Locke, An Essay Concerning Human Understanding - Volume I “Prediction is not proof.” — King and Kraemer, Models, facts, and the policy process: the political ecology of estimated truth “Human behavior is incredibly pliable, plastic.” — Philip Zimbardo 6.1 Inferring and forecasting behavior In Chapter 6 you learned some important practical aspects needed to estimate the logit model. Many of those aspects transfer to other kinds of discrete choice models as well. Now that you have the practical skills to estimate a model, we can take the opportunity to see how the model can be used to infer behavior. In the preceding chapters we have made the assumption that an observer/analyst cannot know the state of mind of the decision-maker, and that a model is a way to infer the decision-making mechanisms based on what people do (or say they would do - more on this in a latter chapter.) Once this has been achieved, what is a model good for? In addition to providing a plausible description of the decision-making mechanism of interest, a model can be used to examine how behavior might change if the conditions of the decision-making situation changed. McFadden, in one of the seminal papers on discrete choice analysis (see McFadden 1974), is concerned with travel demand forecasting. The basis for this is a model of mode choice that helps to tease out the factors that influence the choice between car and bus as a mode for commuting to work. Once that this model was estimated, McFadden was interested in the level of demand of a new mode, a rail system called BART (Bay Area Rail Transit). This system had not been built yet, and so obtaining estimates of level of demand was an important part of policy analysis. In other words, McFadden was interested in how behavior might change with the introduction of a new mode of transportation. What would be the demand for the new mode at a certain fare? If fares changed? If waiting times for BART were longer or shorter? And so on. You will hopefully become familiar with this process: Estimate and select a plausible model for the behavior of interest. Analyze scenarios: what would happen if? This process is valuable in several ways (King and Kraemer 1993). First, it helps to clarify issues that are of policy interest (p. 365). Modelers need to document their assumptions, specification choices, and so on, resulting in a systematic approach of simplification (and remember, all models are wrong, but some are useful). Secondly, the modeling process and subsequently the use of models, help to enforce discipline in analysis and discourse. The behavior of models is to some extent limited by their mechanics, and the results should reflect this. For example, incremental changes in the inputs should result in plausible changes in the predictions. Making predictions outside of the calibration range of the model (beyond the range of values of variables used to estimate the model) will likely result in predictions that are implausibly out of bounds. As King and Kraemer note (1993) “the results of radical changes are unlikely to be predicted accurately by models based on the performance under the status quo” (p. 365).And models when used to analyze scenarios, provide cautionary advice on what not to do by revealing the possible consequences of a bad policy. In this way, models can help inform policy-makers about the range of possible outcomes and whether these outcomes are in a sense in some “acceptable range” (King and Kraemer 1993, 365–66). In other words, models help to probe the limits of the plasticity of human behavior. We will explore the preceding issues in this chapter, looking at different ways to derive behavioral insights based on models of choices. 6.2 How to use this note Remember that the source for the document you are reading is an R Notebook. Throughout the notes, you will find examples of code in segments of text called chunks. This is an example of a chunk: print(&quot;Are these the shadows of the things that Will be, or are they shadows of the things that May be only?&quot;) ## [1] &quot;Are these the shadows of the things that Will be, or are they shadows of the things that May be only?&quot; If you are working with the Notebook version of the document, you can run the code by clicking the ‘play’ icon on the top right corner of the chunk. If you are reading the web-book version of the document, you will often see that the code has already been executed. You can still try it by copying and pasting into your R or RStudio console. 6.3 Learning objectives In this practice, you will learn about: The meaning of the coefficients of a model. The concept of marginal effects. The concept of elasticity. The concept of willingness to pay. How to simulate outcomes. 6.4 Suggested readings Ben-Akiva, M. Lerman, (1985) Discrete Choice Analysis: Theory and Applications to Travel Demand, Chapter 5, pp. 111-113, MIT Press. Hensher, D.A., Rose, J.M., Greene, W.H (2005) Applied Choice Analysis: A Primer, Chapter 11, Cambridge University Press. King, J.L., Kraemer, K.L. (1993) Models, facts, and the policy process: the political ecology of estimated truth. In: Environmental Modelling with GIS, Eds. Goodchild, M., Parks, B.O., Stayaert, L.T., Oxford University Press. Ortuzar JD, Willumsen LG (2011) Modelling Transport, Fourth Edition, Chapter 2, pp. 43-44 and , John Wiley and Sons. Train (2009) Discrete Choice Methods with Simulation, Second Edition, Chapter 2, pp. 29-31, Cambridge University Press. 6.5 Preliminaries Load the packages used in this section: library(tidyverse) library(evd) library(mlogit) library(kableExtra) Load the dataset used in this section (from the mlogit package): data(&quot;Heating&quot;) This dataset includes choices by a sample of consumers in California with respect to different heating systems. In-depth analysis of this dataset was conducted by Train and Croissant (2012) in a document available here. Five heating systems are considered in this choice problem: Gas Central (gc) Gas Room (gr) Electric Central (ec) Electric Room (er) Heat Pump (hp) These heating systems differ in terms of their installation cost (ic) and annual operation cost (oc). See the following table with the median installation cost, annual operation costs, and proportion of choices in sample: Proportion &lt;- Heating %&gt;% group_by(depvar) %&gt;% summarise(no_rows = length(depvar)) df &lt;- data.frame(System = c(&quot;Gas Central&quot;, &quot;Gas Room&quot;, &quot;Electric Central&quot;, &quot;Electric Room&quot;, &quot;Heat Pump&quot;), Installation = c(median(Heating$ic.gc), median(Heating$ic.gr), median(Heating$ic.ec), median(Heating$ic.er), median(Heating$ic.hp)), Operation = c(median(Heating$oc.gc), median(Heating$oc.gr), median(Heating$oc.ec), median(Heating$oc.er), median(Heating$oc.hp)), Proportion = Proportion$no_rows/900 ) df %&gt;% kable() %&gt;% kable_styling() System Installation Operation Proportion Gas Central 778.505 172.105 0.6366667 Gas Room 924.305 154.110 0.1433333 Electric Central 824.840 480.055 0.0711111 Electric Room 989.700 430.665 0.0933333 Heat Pump 1046.550 220.845 0.0555556 In addition, the dataset also includes some information about the consumers, including income, age of household head, number of rooms in the house, and region in California: summary(Heating[,13:16]) %&gt;% kable() %&gt;% kable_styling() income &lt;/th&gt; agehed &lt;/th&gt; rooms &lt;/th&gt; region &lt;/th&gt; Min. :2.000 Min. :20.00 Min. :2.000 valley:177 1st Qu.:3.000 1st Qu.:30.00 1st Qu.:3.000 scostl:361 Median :5.000 Median :45.00 Median :4.000 mountn:102 Mean :4.641 Mean :42.94 Mean :4.424 ncostl:260 3rd Qu.:6.000 3rd Qu.:55.00 3rd Qu.:6.000 NA Max. :7.000 Max. :65.00 Max. :7.000 NA The dataset is in “wide” form, which means that there is one record per decision making unit (i.e. per household). The package mlogit works with data in “long” format, and fortunately (since changing the format of the dataset is a tedious if fairly elementary process), there is a function for changing the format: H &lt;- mlogit.data(Heating, shape = &quot;wide&quot;, choice = &quot;depvar&quot;, varying = c(3:12)) The argument varying lets the function know that the variables in the columns 3 through 13 in the dataframe are alternative-specific, and therefore vary across utility functions. Once that the format of the table has been changed from “wide” to “long”, the number of rows changes from \\(900\\) (the number of decision makers) to \\(4500\\): this is the number of decision makers (\\(900\\)) times the number of alternatives (\\(5\\)). Before estimating an initial model, we need to define the utility functions that we wish to estimate. Since there are five alternatives, we define the following five functions (with heat pump as the reference level): \\[ \\begin{array}{ccc} V_{\\text{ec}} = &amp; 0 &amp; + \\beta_1\\text{ic.ec}\\\\ V_{\\text{er}} = &amp; \\beta_{er} &amp; + \\beta_1\\text{ic.er}\\\\ V_{\\text{gc}} = &amp; \\beta_{gc} &amp; + \\beta_1\\text{ic.gc}\\\\ V_{\\text{gr}} = &amp; \\beta_{gr} &amp; + \\beta_1\\text{ic.gr}\\\\ V_{\\text{hp}} = &amp; \\beta_{hp} &amp; + \\beta_1\\text{ic.hp}\\\\ \\end{array} \\] These functions include only the instalation cost of the systems (ic). The mlogit function can be used to estimate this model (call this Model 1), using the electric central system (“ec”) as the reference level: mod1 &lt;- mlogit(depvar ~ ic, H, reflevel = &quot;ec&quot;) summary(mod1) ## ## Call: ## mlogit(formula = depvar ~ ic, data = H, reflevel = &quot;ec&quot;, method = &quot;nr&quot;, ## print.level = 0) ## ## Frequencies of alternatives: ## ec er gc gr hp ## 0.071111 0.093333 0.636667 0.143333 0.055556 ## ## nr method ## 6 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 1.02E-05 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## er:(intercept) 0.53681271 0.19194838 2.7967 0.005164 ** ## gc:(intercept) 2.11877361 0.13413083 15.7963 &lt; 2.2e-16 *** ## gr:(intercept) 0.86420949 0.16435236 5.2583 1.454e-07 *** ## hp:(intercept) 0.12259544 0.23188795 0.5287 0.597025 ## ic -0.00168108 0.00061843 -2.7183 0.006561 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -1018.5 ## McFadden R^2: 0.0036292 ## Likelihood ratio test : chisq = 7.4197 (p.value = 0.0064514) 6.6 The meaning of the coefficients The coefficients of a discrete choice model are informative because they modulate the effect of various variables on the probabilities of selecting alternatives. In addition, they also can be used to understand how decision-makers trade-off different attributes. The first thing to note is the sign of the coefficients. In the case of the binomial and multinomial logit model, the signs of the coefficients are informative because they tell us something about how the utility is affected by an attribute (either of the alternatives or the decision-maker). A positive sign indicates that the utility increases as the attribute increases. For instance, when considering mobile phones, we would expect speed to increase the utility of the alternatives. On the other hand, we would expect the price of the mobile phones to decrease their utility: the higher the cost, the lower the utility derived from an alternative. In this case, the expected sign of the coefficient for price would be negative. Inspecting the results of Model 0, we notice that the sign of the coefficient for installation costs is negative; this implies that the utility of a system tends to decrease as the installation cost increases. Not surprisingly, since central gas systems have the lowest installation costs, they are also the most popular, whereas heat pumps, with the highest installation costs, are the least popular. Alas, beyond this qualitative assessment of the signs of the coefficients, their magnitudes are not directly interpretable. Recall that underlying the logit models is a sigmoid relationship between the probabilities and the differences in attributes among utility functions. As discussed in Chapter 4, the effect on the probability of choosing an alternative, given a change on some attribute, is not constant along the logit probability curve, and in fact depends on the initial value of the variable, as seen in Figure 4.5) below. # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -5, to = 5, by = 0.01)) %&gt;% mutate(y = plogis(x)) # Plot logit_plot &lt;- ggplot(data = df, aes(x, y)) + geom_line(color = &quot;orange&quot;) + # Plot cumulative distribution function ylim(c(0, 1)) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) # Add x axis logit_plot + xlab(expression(paste(V[j], &quot; - &quot;, V[i], sep=&quot;&quot;))) + # Label the x axis ylab(expression(paste(P[j]))) + # Label the y axis annotate(&quot;segment&quot;, x = -3.75, xend = -2.5, y = 0.024, yend = 0.024, colour = &quot;blue&quot;, linetype = &quot;solid&quot;) + annotate(&quot;segment&quot;, x = -2.5, xend = -2.5, y = 0.024, yend = 0.075, colour = &quot;blue&quot;, linetype = &quot;solid&quot;) + annotate(&quot;segment&quot;, x = 0, xend = 1.25, y = 0.5, yend = 0.5, colour = &quot;red&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;segment&quot;, x = 1.25, xend = 1.25, y = 0.5, yend = 0.77, colour = &quot;red&quot;, linetype = &quot;dashed&quot;) Figure 6.1: The logit probability is not linear on the variables A number of different techniques are available instead to assess the behavioral implications of changes in some of the attributes. We will cover some relevant techniques next. 6.7 Marginal effects A marginal effect is a summary measure of the amount of change in a dependent variable \\(y\\) when an independent variable \\(x_k\\) changes by one unit: \\[ M^y_{x_k}=\\frac{\\partial y}{\\partial x_k} \\] In the case of discrete choice models, the dependent variable that we are interested in is the probability of choosing alternative $j. Louviere et al. (2000, 58–59) show that, given a discrete choice model, the marginal effect is: \\[ M^{P_{in}}_{x_{jnk}} = \\beta_{k}(\\delta_{ij}-P_{jn}) \\] where \\(P_{jn}\\) is the probability of decision-maker \\(n\\) selecting alternative \\(j\\) and \\(\\beta_{jk}\\) is the coefficient that corresponds to variable \\(x_{jnk}\\). This measures the change in probability as a result of a one-unit change in the units of \\(x_{jnk}\\). Two cases result from this expression: \\[ \\begin{array}{c} \\text{1. When }i = j\\text{ then }\\delta_{ii} = 1 \\text{ (this is called a direct marginal effect)}\\\\ \\text{2. When }i \\ne j\\text{ then }\\delta_{ij} = 0 \\text{ (this is called a cross-marginal effect)}\\\\ \\end{array} \\] We discuss these cases next, but first, to continue to work with an example, we need to estimate the probability of choosing different systems at different levels of the installation costs. We use different levels of the installation cost because the effects will not be the same at different starting points in the logit curve!. To simulate this situation, we begin by copying the input dataframe. Here the first 20 rows are copied so that we can modify the data for 5 different systems (and their cost) and 4 regions: ic_min &lt;- H[1:20,] ic_mean &lt;- H[1:20,] ic_max &lt;- H[1:20,] Next, we define the following vectors to retrieve the minimum, mean, and maximum installation costs for each heating system: min_cost &lt;- with(H, data.frame(ic = tapply(ic, index(mod1)$alt, min))) mean_cost &lt;- with(H, data.frame(ic = tapply(ic, index(mod1)$alt, mean))) max_cost &lt;- with(H, data.frame(ic = tapply(ic, index(mod1)$alt, max))) We now replace the cost of installation with these vectors. Since each vector contains five values (for five heating systems) we need to repeat the vector 4 times (for 4 regions): ic_min$ic &lt;- rep(t(min_cost), times = 4) ic_mean$ic &lt;- rep(t(mean_cost), times = 4) ic_max$ic &lt;- rep(t(max_cost), times = 4) And finally, we replace the regions: ic_min$region &lt;- rep(c(&quot;valley&quot;, &quot;scostl&quot;, &quot;mountn&quot;, &quot;ncostl&quot;), each = 5) ic_mean$region &lt;- rep(c(&quot;valley&quot;, &quot;scostl&quot;, &quot;mountn&quot;, &quot;ncostl&quot;), each = 5) ic_max$region &lt;- rep(c(&quot;valley&quot;, &quot;scostl&quot;, &quot;mountn&quot;, &quot;ncostl&quot;), each = 5) If we quickly examine the dataframe with the minimum installation costs: head(ic_min, 10) ## idcase depvar income agehed rooms region alt ic oc chid ## 1.ec 1 FALSE 7 25 6 valley ec 469.61 553.34 1 ## 1.er 1 FALSE 7 25 6 valley er 546.82 505.60 1 ## 1.gc 1 TRUE 7 25 6 valley gc 431.83 199.69 1 ## 1.gr 1 FALSE 7 25 6 valley gr 574.94 151.72 1 ## 1.hp 1 FALSE 7 25 6 valley hp 532.32 237.88 1 ## 2.ec 2 FALSE 5 60 5 scostl ec 469.61 520.24 2 ## 2.er 2 FALSE 5 60 5 scostl er 546.82 486.49 2 ## 2.gc 2 TRUE 5 60 5 scostl gc 431.83 168.66 2 ## 2.gr 2 FALSE 5 60 5 scostl gr 574.94 168.66 2 ## 2.hp 2 FALSE 5 60 5 scostl hp 532.32 199.19 2 We can see that we have simulated a dataset that includes the minimum installation cost of every system for each of four regions. We did not modify any of the other variables the age, income Given the different values of installation cost (at min, mean, and max), we can predict the probabilities as follows: p_mod1_ic_min &lt;- predict(mod1, newdata = ic_min) p_mod1_ic_mean &lt;- predict(mod1, newdata = ic_mean) p_mod1_ic_max &lt;- predict(mod1, newdata = ic_max) The probabilities at their corresponding costs are summarized below (since “region” was not a covariate we can pick any region - they are all the same): data.frame(System = c(&quot;Electric Central&quot;, &quot;Electric Room&quot;, &quot;Gas Central&quot;, &quot;Gas Room&quot;, &quot;Heat Pump&quot;), Cost_min = ic_min$ic[1:5], Prob_min = p_mod1_ic_min[1,], Cost_mean = ic_mean$ic[1:5], Prob_mean = p_mod1_ic_mean[1,], Cost_max = ic_max$ic[1:5], Prob_max = p_mod1_ic_max[1,]) %&gt;% kable(col.names = c(&quot;System&quot;, &quot;Cost&quot;, &quot;Probability&quot;, &quot;Cost&quot;, &quot;Probability&quot;, &quot;Cost&quot;, &quot;Probability&quot;), digits = 3) %&gt;% kable_styling() %&gt;% add_header_above(c(&quot; &quot; = 2, &quot;Minimum Cost&quot; = 2, &quot;Mean Cost&quot; = 2, &quot;Maximum Cost&quot; = 2)) Minimum Cost Mean Cost Maximum Cost System Cost Probability Cost Probability Cost Probability ec Electric Central 469.61 0.070 824.543 0.071 1230.5 0.072 er Electric Room 546.82 0.105 983.928 0.093 1496.3 0.078 gc Gas Central 431.83 0.617 776.827 0.639 1158.9 0.672 gr Gas Room 574.94 0.138 921.770 0.143 1344.0 0.140 hp Heat Pump 532.32 0.071 1046.481 0.055 1679.8 0.038 6.7.1 Direct marginal effects The direct marginal effect is defined as follows: \\[ M^{P_{in}}_{x_{ink}} = \\beta_{k}(1-P_{in}) \\] This measure is useful to answer the question: “How much would the probability of choosing alternative \\(i\\) change if its attribute \\(k\\) changed by one unit?” Or, alternatively, in terms of the present example: “How much would the probability of choosing heating system \\(i\\) change if its installation cost changed by one unit?” Based on the values summarized above, we can calculate the direct marginal effect of the gas central system at the min, mean, max installation costs as: -0.00168108 * (1 - 0.617) ## [1] -0.0006438536 -0.00168108 * (1 - 0.639) ## [1] -0.0006068699 -0.00168108 * (1 - 0.672) ## [1] -0.0005513942 The values above indicate that the probabilities of choosing an electric central system would decrease by approximately \\(0.00064\\)%, \\(0.00061\\)%, \\(0.00055\\)% if the installation cost increased by one dollar from the minimum, mean, and maximum installation cost, respectively. 6.7.2 Cross-marginal effects The cross-marginal effect is defined as follows: \\[ M^{P_{in}}_{x_{jnk}} = -\\beta_{jk}P_{jn} \\] This measure is useful to answer the question: “How much would the probability of choosing alternative \\(i\\) change if attribute \\(k\\) of alternative \\(j\\) changed by one unit?” Or, alternatively, in terms of the present example: “How much would the probability of choosing an electric central heating system change if the installation cost of the gas central heating system changed by one percent?” Based on the values summarized above, we can calculate the cross-marginal effect of the gas central system at the min, mean, max installation costs as: -(-0.00168108 * 0.617) ## [1] 0.001037226 -(-0.00168108 * 0.639) ## [1] 0.00107421 -(-0.00168108 * 0.672) ## [1] 0.001129686 The values above indicate that the probabilities of choosing an electric central system would increase by approximately \\(0.0010\\)%, \\(0.0011\\)%, \\(0.0011\\)% if the cost of installing a gas central heating system increased by one dollar from the minimum, mean, and maximum installation cost, respectively. 6.8 Elasticity An alternative way to explore the way a variable responds to changes in another variable is by means of the elasticity. The elasticity is a concept from economics that is useful to summarize the way a dependent variable \\(y\\) changes in response to changes in an independent variable \\(x_k\\). This is defined as follows: \\[ E^y_{x_k}=\\frac{\\partial y}{\\partial x_k}\\frac{x_k}{y} \\] It can be seen that the elasticity takes the marginal effect and makes it relative to the values of \\(y\\) and \\(x_k\\), in effect producing a unit-less summary measure that is interpreted as the percentage change in \\(y\\) when attribute \\(x_i\\) changes by \\(1\\)%. Following a similar logic as above, there are two cases of the elasticity, direct-point elasticity and cross-point elasticity. These will be discussed next. 6.8.1 Direct-point elasticity Direct-point elasticity is calculated at a point value of \\(x_ink\\) (attribute \\(k\\) of alternative \\(i\\) of decision-maker \\(k\\)) with respect to the probability of selecting alternative \\(i\\). This elasticity is useful to ask the question: “How much would the probability of choosing alternative \\(i\\) change if its attribute \\(k\\) changed by one percent?” In terms of the present example: “How much would the probability of choosing heating system \\(i\\) change if its installation cost changed by one percent?” The direct-point elasticity for a discrete choice model is given by: \\[ E^{P_{in}}_{x_{ink}} = \\beta_{ik}x_{ink}(1-P_{in}) \\] where \\(P_{in}\\) is the probability of decision-maker \\(n\\) selecting alternative \\(i\\), given a variable \\(x_{ink}\\) and its corresponding coefficient \\(\\beta_{ik}\\). Based on the values summarized above, we can calculate the direct-point elasticity of the gas central system at the min, mean, max installation costs as: -0.00168108 * 431.830 * (1 - 0.617) ## [1] -0.2780353 -0.00168108 * 776.827 * (1 - 0.639) ## [1] -0.4714329 -0.00168108 * 1158.90 * (1 - 0.672) ## [1] -0.6390108 These values indicate that the probability of choosing the electric central system with the lowest installation cost declines by approximately \\(0.31\\)% when the installation cost increases by \\(1\\)%. When the installation cost is the mean and the max, the probability of selecting the electric central system declines by approximately \\(0.47\\)% and \\(0.64\\)% respectively when the cost of installation increases by \\(1\\)%. Notice that the elasticity tends to give greater values than the marginal effect. This is because a one-percent change in installation costs represents a much larger amount of change in the variable than a one dollar change. 6.8.2 Cross-point elasticity Another useful measure of elasticity is the cross-point elasticity. This is useful to ask the question: “How much would the probability of choosing alternative \\(i\\) change if attribute \\(k\\) of alternative \\(j\\) changed by one percent?” Or, for example: “How much would the probability of choosing an electric central heating system change if the installation cost of the gas central heating system changed by one percent?” Again, Louviere et al. (2000, 58–59) show that, given a discrete choice model, the cross-point elasticity is given by: \\[ E^{P_{in}}_{x_{ink}} = -\\beta_{jk}x_{jnk}P_{jn} \\] where \\(P_{jn}\\) is the probability of decision-maker \\(n\\) selecting alternative \\(j\\), given a variable \\(x_{jnk}\\) and its corresponding coefficient \\(\\beta_jk\\). The cross-point elasticities of gas central heating at the min, mean, and max values of installation cost are: -(-0.00168108 * 431.830 * 0.617) ## [1] 0.4479055 -(-0.00168108 * 776.827 * 0.639) ## [1] 0.8344754 -(-0.00168108 * 1158.90 * 0.672) ## [1] 1.309193 In other words, the probability of choosing a system other than gas central system increases by approximately \\(0.45\\)%, \\(0.83\\)%, \\(1.31\\)% when the cost of installing a gas central system goes up by \\(1\\)% from the min, mean, and max base installation costs. 6.9 Calculating elasticities based on an mlogit model Fortunately, all the effects discussed above can be easily calculated once that a model has been estimated using the mlogit function for effects. This is illustrated next. 6.9.1 Computing the marginal effects The marginal effects can be computed by means of the effects function. This function takes four arguments, as follows: an mlogit model, the name of the covariate (or attribute) that we wish to examine, the type of effect, and the data for calculating the effects. For the marginal effects, the “type” of the effect is “(r)elative” for the probability and “(a)bsolute”&quot; for the covariate. In the case of the marginal effects at the minimum values of cost we use mod0 (our mlogit model object), indicate the covariate of interest (ic for installation cost), the type of effect (“ra”), and the input data: effects(mod1, covariate = &quot;ic&quot;, type = &quot;ra&quot;, data = ic_min[1:5,]) ## ec er gc gr hp ## ec -0.0015641293 0.0001169511 0.0001169511 0.0001169511 0.0001169511 ## er 0.0001756990 -0.0015053814 0.0001756990 0.0001756990 0.0001756990 ## gc 0.0010369517 0.0010369517 -0.0006441287 0.0010369517 0.0010369517 ## gr 0.0002325016 0.0002325016 0.0002325016 -0.0014485788 0.0002325016 ## hp 0.0001189771 0.0001189771 0.0001189771 0.0001189770 -0.0015621033 The values on the diagonal of the table above are the direct marginal effects, whereas other values are the cross-marginal effects. The marginal effects at the mean values (sometimes called MEM, i.e., marginal effects at the mean) are: effects(mod1, covariate = &quot;ic&quot;, type = &quot;ra&quot;, data = ic_mean[1:5,]) ## ec er gc gr hp ## ec -1.562008e-03 1.190728e-04 1.190728e-04 1.190728e-04 0.0001190728 ## er 1.558057e-04 -1.525275e-03 1.558056e-04 1.558056e-04 0.0001558056 ## gc 1.073548e-03 1.073548e-03 -6.075323e-04 1.073548e-03 0.0010735481 ## gr 2.399663e-04 2.399663e-04 2.399663e-04 -1.441114e-03 0.0002399663 ## hp 9.268762e-05 9.268762e-05 9.268762e-05 9.268762e-05 -0.0015883927 And the marginal effects at the maximum values are: effects(mod1, covariate = &quot;ic&quot;, type = &quot;ra&quot;, data = ic_max[1:5,]) ## ec er gc gr hp ## ec -1.560758e-03 1.203225e-04 1.203225e-04 0.0001203225 0.0001203225 ## er 1.316513e-04 -1.549429e-03 1.316514e-04 0.0001316513 0.0001316514 ## gc 1.129256e-03 1.129256e-03 -5.518241e-04 0.0011292563 0.0011292563 ## gr 2.359411e-04 2.359411e-04 2.359412e-04 -0.0014451392 0.0002359412 ## hp 6.390908e-05 6.390909e-05 6.390909e-05 0.0000639091 -0.0016171713 You can verify that, with some small rounding error, they correspond to the values calculated previously. 6.9.2 Computing the elasticities As well, the direct-point and cross-point elasticities can be calculated using the effects function, however in this case the type of the effects is relative both for the probability and for the covariate. effects(mod1, covariate = &quot;ic&quot;, type = &quot;rr&quot;, data = ic_min[1:5,]) ## ec er gc gr hp ## ec -0.73453075 0.05492140 0.05492140 0.05492140 0.05492140 ## er 0.09607572 -0.82317265 0.09607573 0.09607572 0.09607572 ## gc 0.44778685 0.44778685 -0.27815410 0.44778684 0.44778685 ## gr 0.13367447 0.13367447 0.13367448 -0.83284588 0.13367448 ## hp 0.06333387 0.06333387 0.06333387 0.06333386 -0.83153884 The values on the diagonal of the table above are the direct-point elasticities, whereas other values are the cross-point elasticities. The effects can be calculated at various levels of the covariate of interest, for instance the mean and max: effects(mod1, covariate = &quot;ic&quot;, type = &quot;rr&quot;, data = ic_mean[1:5,]) ## ec er gc gr hp ## ec -1.28794317 0.09818068 0.09818067 0.09818069 0.09818068 ## er 0.15330155 -1.50076056 0.15330153 0.15330154 0.15330154 ## gc 0.83396070 0.83396069 -0.47194728 0.83396070 0.83396070 ## gr 0.22119376 0.22119375 0.22119375 -1.32837610 0.22119376 ## hp 0.09699586 0.09699586 0.09699586 0.09699586 -1.66222324 effects(mod1, covariate = &quot;ic&quot;, type = &quot;rr&quot;, data = ic_max[1:5,]) ## ec er gc gr hp ## ec -1.9205126 0.1480568 0.1480568 0.1480568 0.1480568 ## er 0.1969899 -2.3184107 0.1969899 0.1969899 0.1969899 ## gc 1.3086951 1.3086951 -0.6395089 1.3086951 1.3086951 ## gr 0.3171049 0.3171049 0.3171049 -1.9422671 0.3171049 ## hp 0.1073545 0.1073545 0.1073545 0.1073545 -2.7165243 6.10 A note about attributes in dummy format The attribute used above for the example was installation cost, a continuous variable. In many cases, the attributes of alternatives or decision-makers are continuous variables, but not always. In the dataset about heating systems, for example, there is a dummy variable that indicates the region of residence of the decision-maker: summary(Heating$region) ## valley scostl mountn ncostl ## 177 361 102 260 As can be seen, there are four regions. In the sample, \\(177\\) respondents lived in the region labeled as “valley”, \\(361\\) in the south coastal (“scostl”), \\(102\\) in the mountain (“mountn”), and \\(260\\) in north coastal (“ncostl”). The marginal effects and elasticities discussed above are not appropriate for use when it comes to dummy variables. The reason for this is that marginal changes are not meaningful (e.g., what does it mean to increase region “mountain” by one unit of by 1%?) When variable \\(x_{ink}\\) is a dummy variable, the marginal effect must be calculated as follows: \\[ M^{P_{in}}_{x_{ink}} = P_{in}(x_{ink} = 1) - P_{in}(x_{ink} = 0) \\] In other words, the marginal effect is the difference in the probability of choosing \\(i\\) when the dummy variable is \\(1\\) versus when it is \\(0\\). Let estimate a second model that uses the regions variable to illustrate this (call this Model 2): mod2 &lt;- mlogit(depvar ~ ic | region, H) summary(mod2) ## ## Call: ## mlogit(formula = depvar ~ ic | region, data = H, method = &quot;nr&quot;, ## print.level = 0) ## ## Frequencies of alternatives: ## ec er gc gr hp ## 0.071111 0.093333 0.636667 0.143333 0.055556 ## ## nr method ## 6 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 0.000192 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## er:(intercept) 0.60026296 0.37803680 1.5878 0.112322 ## gc:(intercept) 2.03028891 0.29516076 6.8786 6.045e-12 *** ## gr:(intercept) 0.89557390 0.34321429 2.6094 0.009071 ** ## hp:(intercept) 0.28142118 0.42207287 0.6668 0.504926 ## ic -0.00166573 0.00062007 -2.6864 0.007223 ** ## er:regionscostl 0.04953858 0.44699277 0.1108 0.911754 ## gc:regionscostl 0.07371855 0.36192685 0.2037 0.838601 ## gr:regionscostl 0.10263835 0.41405948 0.2479 0.804225 ## hp:regionscostl -0.14496763 0.50052467 -0.2896 0.772098 ## er:regionmountn -0.02228435 0.59062797 -0.0377 0.969903 ## gc:regionmountn -0.08617193 0.47825549 -0.1802 0.857012 ## gr:regionmountn 0.03599409 0.54612045 0.0659 0.947451 ## hp:regionmountn -0.03937414 0.65470562 -0.0601 0.952044 ## er:regionncostl -0.33843774 0.49386054 -0.6853 0.493161 ## gc:regionncostl 0.23183246 0.38396435 0.6038 0.545986 ## gr:regionncostl -0.32954624 0.45479104 -0.7246 0.468691 ## hp:regionncostl -0.39942540 0.55412324 -0.7208 0.471018 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -1012.5 ## McFadden R^2: 0.0094891 ## Likelihood ratio test : chisq = 19.4 (p.value = 0.11122) Notice that none of the regional coefficients is significant, so we would likely not include these variables in the model. However, for the sake of the example, lets calculate the marginal effect of the dummy variables at the mean of installation cost. We had already created a matrix with the mean of the installation cost at various regions. p_region_ic_mean &lt;- data.frame(Region = c(&quot;valley&quot;, &quot;scostl&quot;, &quot;mountn&quot;, &quot;ncostl&quot;), predict(mod2, newdata = ic_mean, outcome = FALSE)) p_region_ic_mean ## Region ec er gc gr hp ## 1 valley 0.06893601 0.06868371 0.7167905 0.1032603 0.04232956 ## 2 scostl 0.07709246 0.10537150 0.5832442 0.1664375 0.06785436 ## 3 mountn 0.07330246 0.10244901 0.6044788 0.1526602 0.06710956 ## 4 ncostl 0.06922558 0.10166470 0.6145323 0.1597530 0.05482440 The first row contains the probabilities of the different systems for valley (the reference region), and then for each of the three other regions. The marginal effects of changing from the valley to other regions are: data.frame (Effect = c(&quot;valley to scostl&quot;, &quot;valley to mountn&quot;, &quot;valley to ncostl&quot;), rbind (p_region_ic_mean[2, 2:6] - p_region_ic_mean[1, 2:6], p_region_ic_mean[3, 2:6] - p_region_ic_mean[1, 2:6], p_region_ic_mean[4, 2:6] - p_region_ic_mean[1, 2:6])) ## Effect ec er gc gr ## 2 valley to scostl 0.0081564496 0.03668779 -0.1335463 0.06317722 ## 3 valley to mountn 0.0043664503 0.03376531 -0.1123117 0.04939991 ## 4 valley to ncostl 0.0002895745 0.03298099 -0.1022582 0.05649277 ## hp ## 2 0.02552480 ## 3 0.02478000 ## 4 0.01249484 6.11 Willingness to pay and discount rate Another interesting question for policy analysis is at what rate are consumers willing to trade-off one attribute for another? In terms of the present example, this question could be: “How much are consumers willing to pay in increased installation costs for lower annual operating costs?” To answer this question, we first need an appropriate choice model, that is, one that includes both installation and annual operation costs. The following utility functions are one possible model specification (with heat pump as the reference level): \\[ \\begin{array}{ccc} V_{\\text{ec}} = &amp; 0 &amp; + \\beta_1\\text{ic.ec} + \\beta_2\\text{oc.ec}\\\\ V_{\\text{er}} = &amp; \\beta_{er} &amp; + \\beta_1\\text{ic.er} + \\beta_2\\text{oc.er}\\\\ V_{\\text{gc}} = &amp; \\beta_{gc} &amp; + \\beta_1\\text{ic.gc} + \\beta_2\\text{oc.gc}\\\\ V_{\\text{gr}} = &amp; \\beta_{gr} &amp; + \\beta_1\\text{ic.gr} + \\beta_2\\text{oc.gr}\\\\ V_{\\text{hp}} = &amp; \\beta_{hp} &amp; + \\beta_1\\text{ic.hp} + \\beta_2\\text{oc.hp}\\\\ \\end{array} \\] The model can be reestimated using these constants. The option reflevel is used to select the alternative that will work as reference (call this Model 3): mod3 &lt;- mlogit(depvar ~ ic + oc, H, reflevel = &quot;ec&quot;) summary(mod3) ## ## Call: ## mlogit(formula = depvar ~ ic + oc, data = H, reflevel = &quot;ec&quot;, ## method = &quot;nr&quot;, print.level = 0) ## ## Frequencies of alternatives: ## ec er gc gr hp ## 0.071111 0.093333 0.636667 0.143333 0.055556 ## ## nr method ## 6 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 9.58E-06 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## er:(intercept) 0.19459102 0.20424212 0.9527 0.3407184 ## gc:(intercept) 0.05213336 0.46598878 0.1119 0.9109210 ## gr:(intercept) -1.35058266 0.50715442 -2.6631 0.0077434 ** ## hp:(intercept) -1.65884594 0.44841936 -3.6993 0.0002162 *** ## ic -0.00153315 0.00062086 -2.4694 0.0135333 * ## oc -0.00699637 0.00155408 -4.5019 6.734e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -1008.2 ## McFadden R^2: 0.013691 ## Likelihood ratio test : chisq = 27.99 (p.value = 8.3572e-07) So, how is this model used to understand consumer preferences? Recall that the question is at what rate are consumers willing to pay for installation relative to operation. The coefficients of the model provide useful information. Suppose that we would like to know at what rate consumers would be willing to trade one aspect of the good for another, whithout compromising the utility they derive from it. In effect we would like to know how changes in one attribute relate to changes in the other. In the present case, we want to know how installation costs change with respect to operation costs, while mainting the utility (i.e., the change in utility is zero): \\[ \\partial U = 0 = \\beta_1\\partial ic + \\beta_{2}\\partial oc \\] It follows then that: \\[ -\\frac{\\partial ic}{\\partial oc} = \\frac{\\beta_2}{\\beta_1} \\] The ratio of the coefficients represents the willingness to pay. In this example, since: \\[ -\\frac{\\partial ic}{\\partial oc} = \\frac{\\beta_2}{\\beta_1} = \\frac{-0.0069}{-0.0015} = 4.56 \\] The willingness to pay is an additional 4.56 dollars in installation costs per every dollar of operation cost per year. The discount rate is: \\[ r = \\frac{1}{4.56} = 0.219 = 21.9\\% \\] This information can be used to assess the behavior of consumers. 6.12 Simulating market changes To the extent that random utility models can capture consumer preferences, they are useful to understand patterns of substitution. Once a model has been estimated simulating market changes involves creating a new data matrix to which the model can be applied. Lets take a look at two examples. 6.12.1 Incentives Heat pumps are on average more expensive than other heating systems, but they are also more energy-efficient. Suppose then that the government, which has perhaps carbon emission targets that it wishes to meet, is analyzing a policy to encourage the adoption of heat pumps. The policy is to offer a rebate of 15% on the installation cost of heat pumps. As a consequence of this policy, consumers who install a heat pump and apply for the rebate pay only 85% of the cost of installation. To simulate this scenario, we begin by copying the input dataframe: H_rebate &lt;- H In the new dataframe that will simulate the rebate, replacing the cost of installation as follows: H_rebate[H_rebate$alt == &quot;hp&quot;, &quot;ic&quot;] &lt;- 0.85 * H_rebate[H_rebate$alt == &quot;hp&quot;, &quot;ic&quot;] We can calculate the market shares of the “do nothing” and “rebate” policies and compare their shares (which are the mean values of the predictions): data.frame(Policy = c(&quot;Do nothing&quot;, &quot;15% rebate&quot;), rbind(apply(predict(mod3, newdata = H), 2, mean), apply(predict(mod3, newdata = H_rebate), 2, mean))) ## Policy ec er gc gr hp ## 1 Do nothing 0.07111111 0.09333333 0.6366667 0.1433333 0.05555556 ## 2 15% rebate 0.07009125 0.09199196 0.6272957 0.1412092 0.06941196 6.12.2 Introduction of a new system Suppose for example that a more efficient electric system is developed using newer technologies. The cost of installation is more expensive due to the cost of the new technology (cost of installation is $200 dollars higher than the electric central system). On the other hand, the cost of operation is only 75% that of the electric central systems. The preceding analysis suggests that consumers are willing to spend more in installation in exchange for savings in operation costs. What would be the penetration of this new system? To simulate this situation, we begin by creating a model matrix based on Model 3: X &lt;- model.matrix(mod3) Then, we create a new alternative by copying the attributes of electric central: alt &lt;- index(H)$alt Xn &lt;- X[alt == &quot;ec&quot;,] Next, we’ll modify the attributes to reflect the attributes of the new system (+$200 to ic and 0.75 of oc): Xn[, &quot;ic&quot;] &lt;- Xn[, &quot;ic&quot;] + 200 Xn[, &quot;oc&quot;] &lt;- Xn[, &quot;oc&quot;] * 0.75 We also want to identify the unique choice ids, which we will add as row names to the new systems: chid &lt;- index(H)$chid unchid &lt;- unique(index(H)$chid) rownames(Xn) &lt;- paste(unchid, &#39;new&#39;, sep = &quot;.&quot;) chidb &lt;- c(chid, unchid) After this, we can join the new system to the model matrix and sort by choice id: X &lt;- rbind(X, Xn) X &lt;- X[order(chidb), ] This calculates the expression \\(e^{X\\beta}\\) and the sum, which are needed to compute the logit probabilities: exp_Xb &lt;- as.numeric(exp(X %*% coef(mod3))) sum_exp_Xb &lt;- as.numeric(tapply(exp_Xb, sort(chidb), sum)) This is the vector of logit probabilities: P &lt;- exp_Xb / sum_exp_Xb[sort(chidb)] Convert to a matrix of logit probabilities, so that each row is the choice probabilities for a household: P &lt;- data.frame(matrix(P, ncol = 6, byrow = TRUE)) P &lt;- transmute(P, hp = P[, 5], ec = P[, 1], er = P[, 2], gc = P[, 3], gr = P[, 4], new = P[, 6]) We can verify that the sum of the probabilities for each household is 1: summary(rowSums(P)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 1 1 1 1 1 The estimated penetration of the new system is the average probability of households choosing this system: apply(P, 2, mean) ## hp ec er gc gr new ## 0.04977350 0.06311578 0.08347713 0.57145108 0.12855080 0.10363170 The new technology is estimated to have a penetration rate of approximately \\(10.4\\%\\). Compare to the original proportions of the systems: apply(fitted(mod3, outcome = FALSE), 2, mean) ## ec er gc gr hp ## 0.07111111 0.09333333 0.63666667 0.14333333 0.05555556 Can you discern the patterns of substitution here? Are these patterns of substitution reasonable? 6.13 Exercise What is the difference between a marginal effect and an elasticity? Why is it inappropriate to calculate the elasticity of a dummy variable? Use Model 3 in this chapter and calculate the marginal effects and the elasticities for operating cost at the mean of all variables. Use Model 3 in this chapter to calculate the rebate needed to reach a 10% penetration rate of heat pumps. Estimate a new model that extends Model 3 by introducing the age of the household head. Use the electric room system (“er”) as the reference level. Use the likelihood ratio test to compare your new model to Model 3. Discuss the results. Is the ratio of the coefficient of installation (or operation) cost to the coefficient of age of household head meaningful? Explain. References "],
["chapter-6.html", "Chapter 7 Non-Proportional Substitution Patterns I: Generalized Extreme Value Models 7.1 The limits of perfection 7.2 How to use this note 7.3 Learning objectives 7.4 Suggested readings 7.5 Preliminaries 7.6 Generalized Extreme Value models: a recipe for deriving discrete choice models 7.7 Nested Logit model 7.8 Properties of the nested logit model 7.9 Estimation of the nested logit model 7.10 Substitution patterns with the nested logit model 7.11 Elasticities of the nested logit model 7.12 Exercise", " Chapter 7 Non-Proportional Substitution Patterns I: Generalized Extreme Value Models “Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away.” — Antoine de Saint-Exupéry, Airman’s Odyssey “Perfection is a stick with which to beat the possible.” — Rebecca Solnit, Hope in the Dark “The maxim, ‘Nothing prevails but perfection,’ may be spelled PARALYSIS.” — Winston S. Churchill 7.1 The limits of perfection The multinomial logit model is the workhorse of discrete choice analysis. As seen in the preceding chapters, it is a model that is intuitive, and moreover, its closed analytical form makes it simple and convenient to estimate. When originally developed by Luce (Luce 1959; cited in Train 2009), the logit model was based on the axiom of Independence of Irrelevant Alternatives. As discussed in Chapter 4, this property of the logit model implies proportional substitution patterns. To illustrate this, lets revisit the model estimated in Chapter 6. This was a model of the choice of heating systems, with five different systems, namely Gas Central (gc), Gas Room (gr), Electric Central (ec), Electric Room (er), and Heat Pump (hp). We will begin by loading the packages needed to estimate this model: library(tidyverse) library(mlogit) library(kableExtra) And, we also need to load the dataset used in this section (from the mlogit package): data(&quot;Heating&quot;) The dataset is in “wide” form, which means that there is one record per decision making unit (i.e. per household), so we need to change the data to “long” format: H &lt;- mlogit.data(Heating, shape = &quot;wide&quot;, choice = &quot;depvar&quot;, varying = c(3:12)) Now we are ready to estimate a multinomial logit model (we called this Model 3 in Chapter 4): mod3 &lt;- mlogit(depvar ~ ic + oc, H, reflevel = &quot;ec&quot;) summary(mod3) ## ## Call: ## mlogit(formula = depvar ~ ic + oc, data = H, reflevel = &quot;ec&quot;, ## method = &quot;nr&quot;, print.level = 0) ## ## Frequencies of alternatives: ## ec er gc gr hp ## 0.071111 0.093333 0.636667 0.143333 0.055556 ## ## nr method ## 6 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 9.58E-06 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## er:(intercept) 0.19459102 0.20424212 0.9527 0.3407184 ## gc:(intercept) 0.05213336 0.46598878 0.1119 0.9109210 ## gr:(intercept) -1.35058266 0.50715442 -2.6631 0.0077434 ** ## hp:(intercept) -1.65884594 0.44841936 -3.6993 0.0002162 *** ## ic -0.00153315 0.00062086 -2.4694 0.0135333 * ## oc -0.00699637 0.00155408 -4.5019 6.734e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -1008.2 ## McFadden R^2: 0.013691 ## Likelihood ratio test : chisq = 27.99 (p.value = 8.3572e-07) To explore the patterns of substitution according to this model, we will simulate the adoption rates of the systems after removing one system at the time. To simulate this situation, we begin by creating a model matrix based on Model 3: X &lt;- model.matrix(mod3) Then, we remove each one of the alternatives at a time: alt &lt;- index(H)$alt Xmec &lt;- X[alt != &quot;ec&quot;,] Xmer &lt;- X[alt != &quot;er&quot;,] Xmgc &lt;- X[alt != &quot;gc&quot;,] Xmgr &lt;- X[alt != &quot;gr&quot;,] Xmhp &lt;- X[alt != &quot;hp&quot;,] We also want to identify the unique choice ids to reduce the number of choice situations for each decision-maker (from five alternatives to four): # Unique identifiers by decision-maker chid &lt;- index(H)$chid # Remove the fifth identifier for each decision-maker chid &lt;- chid[-seq(1, length(chid), 5)] Based on the above, we can calculate the expression \\(e^{X\\beta}\\) and the sum, which are needed to compute the logit probabilities: # After removing ec exp_Xb_mec &lt;- as.numeric(exp(Xmec %*% coef(mod3))) sum_exp_Xb_mec &lt;- as.numeric(tapply(exp_Xb_mec, sort(chid), sum)) P_mec &lt;- exp_Xb_mec / sum_exp_Xb_mec[sort(chid)] # After removing er exp_Xb_mer &lt;- as.numeric(exp(Xmer %*% coef(mod3))) sum_exp_Xb_mer &lt;- as.numeric(tapply(exp_Xb_mer, sort(chid), sum)) P_mer &lt;- exp_Xb_mer / sum_exp_Xb_mer[sort(chid)] # After removing gc exp_Xb_mgc &lt;- as.numeric(exp(Xmgc %*% coef(mod3))) sum_exp_Xb_mgc &lt;- as.numeric(tapply(exp_Xb_mgc, sort(chid), sum)) P_mgc &lt;- exp_Xb_mgc / sum_exp_Xb_mgc[sort(chid)] # After removing gr exp_Xb_mgr &lt;- as.numeric(exp(Xmgr %*% coef(mod3))) sum_exp_Xb_mgr &lt;- as.numeric(tapply(exp_Xb_mgr, sort(chid), sum)) P_mgr &lt;- exp_Xb_mgr / sum_exp_Xb_mgr[sort(chid)] # After removing hp exp_Xb_mhp &lt;- as.numeric(exp(Xmhp %*% coef(mod3))) sum_exp_Xb_mhp &lt;- as.numeric(tapply(exp_Xb_mhp, sort(chid), sum)) P_mhp &lt;- exp_Xb_mhp / sum_exp_Xb_mhp[sort(chid)] We can convert the vector of logit probabilities to a matrix, so that each row contains the choice probabilities for a household: # After removing ec P_mec &lt;- data.frame(matrix(P_mec, ncol = 4, byrow = TRUE)) P_mec &lt;- transmute(P_mec, ec = NA, er = P_mec[, 1], gc = P_mec[, 2], gr = P_mec[, 3], hp = P_mec[, 4]) # After removing er P_mer &lt;- data.frame(matrix(P_mer, ncol = 4, byrow = TRUE)) P_mer &lt;- transmute(P_mer, ec = P_mer[, 1], er = NA, gc = P_mer[, 2], gr = P_mer[, 3], hp = P_mer[, 4]) # After removing gc P_mgc &lt;- data.frame(matrix(P_mgc, ncol = 4, byrow = TRUE)) P_mgc &lt;- transmute(P_mgc, ec = P_mgc[, 1], er = P_mgc[, 2], gc = NA, gr = P_mgc[, 3], hp = P_mgc[, 4]) # After removing gr P_mgr &lt;- data.frame(matrix(P_mgr, ncol = 4, byrow = TRUE)) P_mgr &lt;- transmute(P_mgr, ec = P_mgr[, 1], er = P_mgr[, 2], gc = P_mgr[, 3], gr = NA, hp = P_mgr[, 4]) # After removing hp P_mhp &lt;- data.frame(matrix(P_mhp, ncol = 4, byrow = TRUE)) P_mhp &lt;- transmute(P_mhp, ec = P_mhp[, 1], er = P_mhp[, 2], gc = P_mhp[, 3], gr = P_mhp[, 4], hp = NA) Given the above, we can summarize the choice probabilities in the form of adoption rates. The table below shows the original adoption rates (when no alternative was removed) and for the different situations of interest, after removing each alternative: df &lt;- data.frame(Alternative = c(&quot;None&quot;, &quot;ec&quot;, &quot;er&quot;, &quot;gc&quot;, &quot;gr&quot;, &quot;hp&quot; ), rbind(apply(fitted(mod3, outcome = FALSE), 2, mean), apply(P_mec, 2, mean), apply(P_mer, 2, mean), apply(P_mgc, 2, mean), apply(P_mgr, 2, mean), apply(P_mhp, 2, mean)) ) df %&gt;% kable(col.names = c(&quot;Alternative Removed&quot;, &quot;ec&quot;, &quot;er&quot;, &quot;gc&quot;, &quot;gr&quot;, &quot;hp&quot;), digits = 2) %&gt;% kable_styling() Alternative Removed ec er gc gr hp None 0.07 0.09 0.64 0.14 0.06 ec NA 0.10 0.68 0.15 0.06 er 0.08 NA 0.70 0.16 0.06 gc 0.19 0.25 NA 0.40 0.15 gr 0.08 0.11 0.74 NA 0.06 hp 0.08 0.10 0.67 0.15 NA Examine for a moment the patterns of substitution when alternatives are removed. In general, the probability of choosing any of the remainder alternatives increases by the same percentage (recall the elasticities), which means that the alternatives that had a higher probability of being chosen to begin with will increase more. Consider, for instance, what happens when Gas Central is removed. Would you say that this pattern of substitution is sensible? At issue is the possibility that there are some hidden correlations between some alternatives. This does not mean that the model is the problem per se, what it means is that the model was not properly specified. The logit model is the ideal modeling alternative, as long as the utilities are reasonably well-specified. Unfortunately for us, there is no way to say, a priori, whether some alternatives are necessarily correlated given a model specification. For instance, in the context of the present example, it is plausible to say that central systems (i.e., Electric Central, Gas Central, Heat Pump) are more similar between them than room systems (i.e., Electric Room, Gas Room). If the specification of the model is not complete and some attribute common to central systems is missing, we would expect the patterns of substitution between those systems to be stronger than between central and room systems. On the other hand, we could also say that the source of energy makes some systems more similar between them, for example, Electric Central, Electric Room, and Heat Pump, whereas Gas Central and Gas Room might be more similar between them. If any relevant attributes missing are common to gas and electric systems, then we would again expect the patterns of substitution to be stronger within a class of systems than between. A way to capture this is by specifying a branching structure whereby alternatives thought to share similar but missing attributes. Three examples of choice structures are shown in Figure 7.1: a multinomial choice structure, such as in the model above; and two plausible nested choice structures. Figure 7.1: Three Examples of Choice Structures The intuition behind the nested choice structures is that decision-makers decide first whether they want an electric- or gas-based system first, before deciding which system specifically; or, as an alternative, that decision-makers decide whether they want a room or central system, before deciding on a specific system. It is important to know that if the systems were highly differentiated through the specification of the utility functions, the nesting structure would be unnecessary. However, since, the question of whether the patterns of substitution should be non-proportional is an empirical one, and the objective of this chapter is to introduce a class of models that are useful to capture non-proportional substitution patterns based on the Extreme Value distribution used to derive the multinomial logit model. 7.2 How to use this note Remember that the source for the document you are reading is an R Notebook. Throughout the notes, you will find examples of code in segments of text called chunks. This is an example of a chunk: print(&quot;Do NOT believe a word Gwyneth Paltrow says. Repeat. DO NOT BELIEVE HER.&quot;) ## [1] &quot;Do NOT believe a word Gwyneth Paltrow says. Repeat. DO NOT BELIEVE HER.&quot; If you are working with the Notebook version of the document, you can run the code by clicking the ‘play’ icon on the top right corner of the chunk. If you are reading the web-book version of the document, you will often see that the code has already been executed. You can still try it by copying and pasting into your R or RStudio console. 7.3 Learning objectives In this practice, you will learn about: The Generalized Extreme Value approach to derive models. The nested logit model. Properties of the nested logit model. Patterns of substitution with the nested logit model. 7.4 Suggested readings Ben-Akiva, M. Lerman, (1985) Discrete Choice Analysis: Theory and Applications to Travel Demand, Chapter 5, pp. 126-128 and Chapter 10, pp. 285-299, MIT Press. Hensher, D.A., Rose, J.M., Greene, W.H (2005) Applied Choice Analysis: A Primer, Chapters 13 and 14, Cambridge University Press. Louviere, J.J., Hensher, D.A., Swait, J.D. (2000) Stated Choice Methods: Analysis and Application, Chapter 6, Cambridge University Press. Ortuzar JD, Willumsen LG (2011) Modelling Transport, Fourth Edition, Chapter 2, pp. 43-44 and , John Wiley and Sons. Train (2009) Discrete Choice Methods with Simulation, Second Edition, Chapter 4, Cambridge University Press. 7.5 Preliminaries Load the packages used in this section: library(tidyverse) library(evd) library(mlogit) library(kableExtra) 7.6 Generalized Extreme Value models: a recipe for deriving discrete choice models McFadden’s Generalized Extreme Value model (or GEV, for short; McFadden 1978) is a general approach to derive discrete choice models. In this sense, it is not a single model, but rather a recipe for deriving discrete choice models based on the Extreme Value distribution. The definition of the model is discussed here, along with a preliminary example (see Ben-Akiva and Lerman 1985, 126–27). Then, we will see how the GEV recipe can be used to derive models with nested structures such as shown in Figure 7.1. For simplicity, we will postulate that: \\[ y_j=e^{V_j} \\text{ , for }j=1,2,\\cdots,J \\] where \\(J\\) is the number of alternatives. Notice that: \\[ y_1\\ge0, y_2\\ge0, \\cdots,y_{J}\\ge0 \\] We will define a function \\(G\\) as follows: \\[ G(y_1,y_2, \\cdots,y_{J}) \\] Function \\(G\\) must satisfy the following conditions: \\(G\\) is non-negative. \\(G\\) is a homogeneous function of degree one. This means that multiplying every \\(y_j\\) by a factor \\(\\alpha\\) is identical to multiplying \\(G\\) by \\(\\alpha\\): \\[ G(\\alpha y_1, \\alpha y_1, \\cdots, \\alpha y_{J_n}) = \\alpha G(y_1, y_2, \\cdots, y_{J_n}) \\] When any one of \\(y_i\\) grows without bound, \\(G\\) does too: \\[ \\lim_{y_j \\to \\infty}G(y_1,y_2,\\cdots,y_{J_n})=\\infty \\text{, for all }y_j \\] The cross-partial derivatives of the function \\(G\\) change signs in a specific way, alternating between non-positive and non-negative for higher order derivatives: \\[ \\begin{array}{c} G_j = \\frac{\\partial G}{\\partial y_j}\\ge0 \\text{, for all }j\\\\ G_{jl} = \\frac{\\partial G}{\\partial y_j\\partial y_l}\\le0 \\text{, for all }l\\ne j\\\\ G_{jlm} = \\frac{\\partial G}{\\partial y_j\\partial y_l\\partial y_m}\\ge0 \\text{, for any }m\\ne l \\ne j\\\\ \\end{array} \\] A function that satisfies these conditions leads to the following choice probability: \\[ P_i = y_i\\frac{G_i}{G} \\] Train (2009) notes that the conditions above lack any evident behavioral intuition. While this means that any function \\(G\\) that satisfies them is a legitimate discrete choice probability. On the other hand, the lack of evident intuitions means that someone who is trying to develop a model also lacks guidance in terms of how to translate a desired correlation structure into a function \\(G\\), a limitation that research by Daly and Bierlaire (2003) aimed to address. For the above reason, we will not be concerned with the particular meaning of the conditions, but rather will see how the GEV framework has been employed to derive various discrete choice models. We begin in this section with the following function: \\[ G = \\sum_{j=1}^{J}y_j \\] This function satisfies the conditions to be a GEV model, since: \\(G\\) is always non-negative as long as \\(y_j\\) is non-negative for every \\(j\\). The function is homogeneous of degree one, since: \\[ G(\\alpha y_1, \\alpha y_1, \\cdots, \\alpha y_{J}) = \\sum_{j=1}^{J}(\\alpha y_j) = \\alpha \\sum_{j=1}^{J}y_j = \\alpha G(y_1,y_1, \\cdots,y_{J}) \\] \\(G\\) grows unbounded if any \\(y_j\\) grows unbounded; and The first derivative of thiss satisty the condition since: \\[ \\begin{array}{c} \\frac{\\partial G}{\\partial y_j} =1\\\\ \\frac{\\partial G}{\\partial y_j\\partial y_l}=0\\\\ \\end{array} \\] The first partial derivative is 1, which satisfies the non-negativity condition. All higher order derivatives are zero, which are non-positive and non-negative as required. Given this function, we can substitute in the choice probability expression: \\[ P_i = y_i\\frac{G_i}{G}=\\frac{y_i}{\\sum_{j=1}^{J}y_j} \\] And since: \\[ y_j = e^{V_j} \\] It follows then that: \\[ P_i =\\frac{e^{V_i}}{\\sum_{j=1}^{J_n}e^{V_j}} \\] which is simply the multinomial logit model. As this illustrates, the multinomial logit is a particular case of a GEV model. Next, we will see how a model with nesting structures is derived. 7.7 Nested Logit model The nested logit model is derived following the GEV modelling approach by assuming that the \\(J\\) alternatives are uniquely allocated to one class the alternatives, called a nest. There are \\(S\\) nests \\(B_s\\) (\\(s=1,3,\\cdots,S\\)). Based on the above, we can define the following function: \\[ G=\\sum_{s=1}^S \\Big(\\sum_{j \\in B_s}y_j^{1/\\lambda_s} \\Big)^{\\lambda_s} \\] We can verify that this function satisfies the conditions to be a GEV model: \\(G\\) is always non-negative since \\(y_j\\) is non-negative for every \\(j\\). The function is homogeneous of degree one, since: \\[ \\begin{eqnarray*} G(\\alpha y_1, \\alpha y_2, \\cdots, \\alpha y_{J}) &amp;=&amp; \\\\ &amp; &amp;\\sum_{s=1}^S \\Big(\\sum_{j \\in B_s}(\\alpha y)_j^{1/\\lambda_s} \\Big)^{\\lambda_s} = \\\\ &amp; &amp;\\sum_{s=1}^S \\Big(\\sum_{j \\in B_s}\\alpha_j^{1/\\lambda_s} y_j^{1/\\lambda_s} \\Big)^{\\lambda_s} = \\\\ &amp; &amp;\\sum_{s=1}^S \\Big(\\sum_{j \\in B_s}\\alpha_j^{1/\\lambda_s} y_j^{1/\\lambda_s} \\Big)^{\\lambda_s} = \\\\ &amp; &amp;\\alpha \\sum_{s=1}^S \\Big(\\sum_{j \\in B_s} y_j^{1/\\lambda_s} \\Big)^{\\lambda_s} = \\\\ \\alpha G(y_1, y_2, \\cdots, y_{J}) &amp; &amp; \\end{eqnarray*} \\] \\(G\\) grows unbounded if any \\(y_j\\) grows unbounded as long as \\(0 \\le \\lambda_s \\le 1\\) for all \\(s\\); and The first derivative of this function for \\(j\\) in nest \\(B_t\\) is: \\[ \\begin{eqnarray*} \\frac{\\partial G}{\\partial y_i} &amp;=&amp; \\lambda_t \\Big(\\sum_{j \\in B_t}y_j^{1/\\lambda_t} \\Big)^{\\lambda_t - 1} \\frac{1}{\\lambda_t} y_i^{1/\\lambda_t - 1}= \\\\ &amp; &amp;y_i^{1/\\lambda_t - 1} \\Big(\\sum_{j \\in B_t}y_j^{1/\\lambda_t} \\Big)^{\\lambda_t - 1} \\end{eqnarray*} \\] This derivative is always non-negative, since \\(y_j\\) is non-negative for all \\(j\\). The second partial derivative is: \\[ \\frac{\\partial G}{\\partial y_i \\partial y_l}= y_i^{1/\\lambda_t - 1}\\frac{\\partial}{\\partial y_l} \\Big(\\sum_{j \\in B_t}y_j^{1/\\lambda_t} \\Big)^{\\lambda_t - 1} \\] When \\(y_l\\) is not part of nest \\(B_t\\) the second derivative is zero, which satisfies the non-positive condition. When \\(y_l\\) is part of nest \\(B_t\\) the second derivative is: \\[ \\begin{eqnarray*} \\frac{\\partial G}{\\partial y_i \\partial y_l} &amp;=&amp; y_i^{1/\\lambda_t - 1}\\frac{\\partial}{\\partial y_l} \\Big(\\sum_{j \\in B_t}y_j^{1/\\lambda_t} \\Big)^{\\lambda_t - 1}\\\\ &amp;=&amp;(\\lambda_t - 1)y_i^{1/\\lambda_t - 1}\\Big(\\sum_{j \\in B_t}y_j^{1/\\lambda_t} \\Big)^{\\lambda_t - 2} \\frac{1}{\\lambda_t} y_l^{1/\\lambda_t-1}\\\\ &amp;=&amp;\\frac{\\lambda_t - 1}{\\lambda_t}(y_iy_l)^{1/\\lambda_t - 1}\\Big(\\sum_{j \\in B_t}y_j^{1/\\lambda_t} \\Big)^{\\lambda_t - 2} \\end{eqnarray*} \\] Since \\(0 \\le \\lambda_s \\le 1\\), the term \\(\\frac{\\lambda_s - 1}{\\lambda_s}\\) is at most zero, which again satisfies the non-positive condition. Higher order derivatives satisfy the required conditions. Since \\(G\\) is a valid function for the GEV recipe, we have that: \\[ \\begin{eqnarray*} P_i &amp;=&amp; y_i\\frac{y_i^{1/\\lambda_t - 1} \\Big(\\sum_{j \\in B_t}y_j^{1/\\lambda_t} \\Big)^{\\lambda_t - 1}}{\\sum_{s=1}^S \\Big(\\sum_{j \\in B_s}y_j^{1/\\lambda_s} \\Big)^{\\lambda_s}}\\\\ &amp;=&amp;\\frac{y_i^{\\lambda_t/\\lambda_t}y_i^{1/\\lambda_t - 1} \\Big(\\sum_{j \\in B_t}y_j^{1/\\lambda_t} \\Big)^{\\lambda_t - 1}}{\\sum_{s=1}^S \\Big(\\sum_{j \\in B_s}y_j^{1/\\lambda_s} \\Big)^{\\lambda_s}}\\\\ &amp;=&amp;\\frac{y_i^{1/\\lambda_t} \\Big(\\sum_{j \\in B_t}y_j^{1/\\lambda_t} \\Big)^{\\lambda_t - 1}}{\\sum_{s=1}^S \\Big(\\sum_{j \\in B_s}y_j^{1/\\lambda_s} \\Big)^{\\lambda_s}}\\\\ \\end{eqnarray*} \\] Replacing \\(y_j = e^{V_j}\\) gives the expression for the choice probability of the nested model: \\[ P_i = \\frac{e^{V_i/\\lambda_t} \\Big(\\sum_{j \\in B_s}e^{V_j/\\lambda_t} \\Big)^{\\lambda_t - 1}}{\\sum_{s=1}^S \\Big(\\sum_{j \\in B_s}e^{V_j/\\lambda_s} \\Big)^{\\lambda_s}} \\] Superficially, the probability of the nested model resembles the multinomial probability, but the differences are important. One of the examples of nesting in Figure 7.1 can help to unpack the probability of the nested model. Suppose that there are \\(S=2\\) nests, namely central (“c”) and room (“r”). The probability of choosing an alternative in the nest “central”, say Electric Central, is: \\[ P_{ec} = \\frac{e^{V_{ec}/\\lambda_{c}}(e^{V_{ec}/\\lambda_{c}}+e^{V_{gc}/\\lambda_{c}}+e^{V_{hp}/\\lambda_{c}})^{\\lambda_c-1}} {(e^{V_{ec}/\\lambda_{c}}+e^{V_{gc}/\\lambda_{c}}+e^{V_{hp}/\\lambda_{c}})^{\\lambda_c}+(e^{V_{er}/\\lambda_{r}}+e^{V_{gr}/\\lambda_{r}}+e^{V_{hp}/\\lambda_{r}})^{\\lambda_r}} \\] The probability of choosing another alternative in the nest “central”, say Gas Central, is: \\[ P_{gc} = \\frac{e^{V_{gc}/\\lambda_{c}}(e^{V_{ec}/\\lambda_{c}}+e^{V_{gc}/\\lambda_{c}}+e^{V_{hp}/\\lambda_{c}})^{\\lambda_c-1}} {(e^{V_{ec}/\\lambda_{c}}+e^{V_{gc}/\\lambda_{c}}+e^{V_{hp}/\\lambda_{c}})^{\\lambda_c}+(e^{V_{er}/\\lambda_{r}}+e^{V_{gr}/\\lambda_{r}}+e^{V_{hp}/\\lambda_{r}})^{\\lambda_r}} \\] Notice that the ratio of odds is: \\[ \\frac{P_{ec}}{P_{gc}} = \\frac{\\frac{e^{V_{ec}/\\lambda_{c}}(e^{V_{ec}/\\lambda_{c}}+e^{V_{gc}/\\lambda_{c}}+e^{V_{hp}/\\lambda_{c}})^{\\lambda_c-1}} {(e^{V_{ec}/\\lambda_{c}}+e^{V_{gc}/\\lambda_{c}}+e^{V_{hp}/\\lambda_{c}})^{\\lambda_c}+(e^{V_{er}/\\lambda_{r}}+e^{V_{gr}/\\lambda_{r}}+e^{V_{hp}/\\lambda_{r}})^{\\lambda_r}}}{\\frac{e^{V_{gc}/\\lambda_{c}}(e^{V_{ec}/\\lambda_{c}}+e^{V_{gc}/\\lambda_{c}}+e^{V_{hp}/\\lambda_{c}})^{\\lambda_c-1}} {(e^{V_{ec}/\\lambda_{c}}+e^{V_{gc}/\\lambda_{c}}+e^{V_{hp}/\\lambda_{c}})^{\\lambda_c}+(e^{V_{er}/\\lambda_{r}}+e^{V_{gr}/\\lambda_{r}}+e^{V_{hp}/\\lambda_{r}})^{\\lambda_r}}} = \\frac{e^{V_{ec}/\\lambda_{c}}}{e^{V_{gr}/\\lambda_{r}}} = \\frac{e^{V_{ec}}}{e^{V_{gc}}} \\] As you can see, the ratio of odds is independent from other alternatives: this is the same as the multinomial logit model. In fact, substitution patterns within a nest are proportional, as they are in the case of the multinomial logit. This is not the case when the ratio of odds is for two alternatives in different nests. For example, the probability of choosing an alternative in the nest for room systems, say “er”, is: \\[ P_{er} = \\frac{e^{V_{er}/\\lambda_{r}}(e^{V_{ec}/\\lambda_{r}}+e^{V_{gr}/\\lambda_{r}}+e^{V_{hp}/\\lambda_{r}})^{\\lambda_r-1}} {(e^{V_{ec}/\\lambda_{c}}+e^{V_{gc}/\\lambda_{c}}+e^{V_{hp}/\\lambda_{c}})^{\\lambda_c-1}+(e^{V_{er}/\\lambda_{r}}+e^{V_{gr}/\\lambda_{r}}+e^{V_{hp}/\\lambda_{c}})^{\\lambda_r}} \\] The ratio of odds for electric central and electric room is: \\[ \\frac{P_{ec}}{P_{er}} = \\frac{\\frac{e^{V_{ec}/\\lambda_{c}}(e^{V_{ec}/\\lambda_{c}}+e^{V_{gc}/\\lambda_{c}}+e^{V_{hp}/\\lambda_{c}})^{\\lambda_c-1}} {(e^{V_{ec}/\\lambda_{c}}+e^{V_{gc}/\\lambda_{c}}+e^{V_{hp}/\\lambda_{c}})^{\\lambda_c}+(e^{V_{er}/\\lambda_{r}}+e^{V_{gc}/\\lambda_{r}}+e^{V_{hp}/\\lambda_{c}})^{\\lambda_r}}}{\\frac{e^{V_{er}/\\lambda_{r}}(e^{V_{er}/\\lambda_{r}}+e^{V_{gr}/\\lambda_{r}}+e^{V_{hp}/\\lambda_{r}})^{\\lambda_r-1}} {(e^{V_{ec}/\\lambda_{c}}+e^{V_{gc}/\\lambda_{c}}+e^{V_{hp}/\\lambda_{c}})^{\\lambda_c}+(e^{V_{er}/\\lambda_{r}}+e^{V_{gr}/\\lambda_{r}}+e^{V_{hp}/\\lambda_{r}})^{\\lambda_r}}} = \\frac{e^{V_{ec}/\\lambda_{c}}(e^{V_{ec}/\\lambda_{c}}+e^{V_{gc}/\\lambda_{c}}+e^{V_{hp}/\\lambda_{c}})^{\\lambda_c-1}}{e^{V_{er}/\\lambda_{r}}(e^{V_{er}/\\lambda_{r}}+e^{V_{gr}/\\lambda_{r}}+e^{V_{hp}/\\lambda_{r}})^{\\lambda_r-1}} \\] The ratio of odds is no longer independent from other alternatives! In fact, the ratio of odds depends on the alternatives in each of the two nests of interest. This property leads to non-proportional substitution patterns. 7.8 Properties of the nested logit model As seen above, the nested logit model behaves as the multinomial model within a nest (with proportional substitution), but between nests the patterns of substitution are not necessarily proportional. An intuitive way to think about the nested logit is in terms of marginal and conditional probabilities. Suppose that we decompose the systematic utility of the alternatives in the following way: \\[ V_j = Z_j + W_s \\] where \\(Z_j\\) are the attributes specific to the alternatives, whereas \\(W_s\\) are attributes specific to nest \\(s\\). For example, imagine that central heating systems receive a flat subsidy of $100. Since the alternatives within the nest all share this attribute, and the model operates based on the difference in utilities, this subsidy is not useful to discriminate between alternatives in the “central” nest. However, the subsidy has potentially the effect of making alternatives in the “central” nest more attractive than alternatives in the “room” nest. Based on the above decomposition, we can rewrite the choice probabilities as follows: \\[ \\begin{eqnarray*} P_i &amp;=&amp; \\frac{e^{(Z_i + W_t)/\\lambda_t} \\Big(\\sum_{j \\in B_t}e^{(Z_j + W_t)/\\lambda_t} \\Big)^{-1}\\Big(\\sum_{j \\in B_t}e^{(Z_j + W_t)/\\lambda_t} \\Big)^{\\lambda_t}}{\\sum_{s=1}^S \\Big(\\sum_{j \\in B_s}e^{{Z_j + W_s}/\\lambda_s} \\Big)^{\\lambda_s}} \\\\ &amp;=&amp;\\frac{e^{(Z_i + W_t)/\\lambda_t} }{\\sum_{j \\in B_s}e^{(Z_j + W_t)/\\lambda_t}}\\frac{\\Big(\\sum_{j \\in B_t}e^{(Z_j + W_t)/\\lambda_t} \\Big)^{\\lambda_t}}{\\sum_{s=1}^S \\Big(\\sum_{j \\in B_s}e^{(Z_j + W_s)/\\lambda_s} \\Big)^{\\lambda_s}} \\end{eqnarray*} \\] Lets now take the each term of the probability above in turn. The first term we are going to call the conditional probability: \\[ P_{i|t} = \\frac{e^{(Z_i + W_t)/\\lambda_t} }{\\sum_{j \\in B_t}e^{(Z_j + W_t)/\\lambda_t}} \\] We can rearrange this term as follows: $$ \\[\\begin{eqnarray*} P_{i|t} &amp;=&amp; \\frac{e^{(Z_i + W_s)/\\lambda_t} }{\\sum_{j \\in B_t}e^{(Z_j + W_t)/\\lambda_t}}\\\\ &amp;=&amp;\\frac{e^{Z_i/\\lambda_t} e^{W_t/\\lambda_t}}{\\sum_{j \\in B_t}e^{Z_j/\\lambda_s} e^{W_t/\\lambda_t}}\\\\ &amp;=&amp;\\frac{e^{Z_i/\\lambda_t} e^{W_t/\\lambda_t}}{ e^{W_t/\\lambda_t}\\Big(\\sum_{j \\in B_t}e^{Z_j/\\lambda_t} \\Big)}\\\\ &amp;=&amp;\\frac{e^{Z_i/\\lambda_t}}{\\sum_{j \\in B_t}e^{Z_j/\\lambda_t}} \\end{eqnarray*}\\] $$ The expression above is the probability of choosing alternative \\(i\\) conditional on choosing nest \\(t\\). Now the second term, which we will call the marginal probability of choosing the nest \\(t\\): \\[ \\begin{eqnarray*} P_t &amp;=&amp;\\frac{\\Big(\\sum_{j \\in B_t}e^{(Z_j + W_t)/\\lambda_t} \\Big)^{\\lambda_t}}{\\sum_{s=1}^S \\Big(\\sum_{j \\in B_s}e^{(Z_j + W_s)/\\lambda_s} \\Big)^{\\lambda_s}}\\\\ &amp;=&amp;\\frac{\\Big(\\sum_{j \\in B_t}e^{Z_j/\\lambda_t} e^{W_t/\\lambda_t} \\Big)^{\\lambda_t}}{\\sum_{s=1}^S \\Big(\\sum_{j \\in B_s}e^{(Z_j + W_s)/\\lambda_s} \\Big)^{\\lambda_s}}\\\\ &amp;=&amp;\\frac{\\Big(e^{W_t/\\lambda_t}\\sum_{j \\in B_t} e^{Z_j/\\lambda_t} \\Big)^{\\lambda_t}}{\\sum_{s=1}^S \\Big(e^{W_s/\\lambda_s}\\sum_{j \\in B_s}e^{Z_j/\\lambda_s} \\Big)^{\\lambda_s}}\\\\ &amp;=&amp;\\frac{e^{W_t}\\Big(\\sum_{j \\in B_t} e^{Z_j/\\lambda_t} \\Big)^{\\lambda_t}}{\\sum_{s=1}^S e^{W_s}\\Big(\\sum_{j \\in B_s}e^{Z_j/\\lambda_s} \\Big)^{\\lambda_s}}\\\\ \\end{eqnarray*} \\] For the last step, we make use of the following property of logarithms: \\[ e^xb^c = e^{x+c\\ln b} \\] Therefore: \\[ \\begin{eqnarray*} P_t &amp;=&amp;\\frac{e^{W_t + \\lambda_t\\ln\\Big(\\sum_{j \\in B_t} e^{Z_j/\\lambda_t} \\Big)}}{\\sum_{s=1}^S e^{W_s+\\lambda_s\\ln\\Big(\\sum_{j \\in B_s}e^{Z_j/\\lambda_s} \\Big)}}\\\\ &amp;=&amp;\\frac{e^{W_t + \\lambda_tI_t}}{\\sum_{s=1}^S e^{W_s+\\lambda_sI_s}} \\end{eqnarray*} \\] with: \\[ I_s = \\ln\\Big(\\sum_{j \\in B_s}e^{Z_j/\\lambda_s} \\Big) \\] \\(I_s\\) is variously called the logsum, or expected maximum utility of a nest. It can be appreciated that the probability of choosing alternative \\(i\\) in a nested model is the product of two multinomial logit models in the form of the marginal probability of choosing nest \\(t\\) (sometimes called the upper model), and the probability of choosing \\(i\\) conditional on choosing nest \\(t\\): \\[ P_i = P_{i|t}\\cdot P_t= \\frac{e^{(Z_i + W_t)/\\lambda_t} }{\\sum_{j \\in B_t}e^{(Z_j + W_t)/\\lambda_t}}\\ \\cdot \\frac{e^{W_t + \\lambda_tI_t}}{\\sum_{s=1}^S e^{W_s+\\lambda_sI_s}} \\] The role of parameters \\(\\lambda_s\\) is more intuitive in this formulation of the model. As noted above, \\(\\lambda_s\\) is bounded between zero and one. We can examine the limiting cases, as follows: When \\(\\lambda_s &lt;0\\) for any \\(s\\), increases in the value of \\(I_s\\) would actually decrease the probability of choosing that nest; a result that is inconsistent with utility maximization. When \\(\\lambda \\to 0\\), any changes in the value of \\(I_s\\) would not change the probability of selecting that nest. When \\(\\lambda_s=1\\) for all \\(s\\), the choice probability becomes: \\[ P_i = \\frac{e^{V_j} \\Big(\\sum_{j \\in B_s}e^{V_j} \\Big)^{0}}{\\sum_{s=1}^S \\Big(\\sum_{j \\in B_s}e^{V_j} \\Big)^{1}}= \\frac{e^{V_j}}{\\sum_{k=1}^J e^{V_k}} \\] The model collapses the the multinomial logit model, since the denominator simply becomes the sum of \\(e^{V_j}\\) for all alternatives across every nest! In fact, \\(1-\\lambda_s\\) is a measure of the correlation in the nest, so when \\(\\lambda=1\\) this indicates a correlation of zero. It is possible to test the hypothesis that a nesting parameter is identical to one, by means of a t-test, as follows: \\[ t=\\frac{\\lambda_s-1}{\\sigma_{\\lambda_s}} \\] where \\(\\sigma_{\\lambda_s}\\) is the standard deviation of the inclusive value. 7.9 Estimation of the nested logit model The nested logit model can be estimated using mlogit by using additional argument in the mlogit function. We will illustrate this using the same example. The chunk of code below defines a formula with installation costs (ic) and operation costs (oc) as alternative specific variables with a generic coefficient. In addition, the nests are defined by means of a list. In this model, we define two nests that correspond to “room” systems (er and gr) and “central” systems (ec, gc, and hp). Call this model nl1 for “nested logit 1”: nl1 &lt;- mlogit(depvar ~ ic + oc, H, nests = list(room = c( &#39;er&#39;, &#39;gr&#39;), central = c(&#39;ec&#39;, &#39;gc&#39;, &#39;hp&#39;)), steptol = 1e-12) summary(nl1) ## ## Call: ## mlogit(formula = depvar ~ ic + oc, data = H, nests = list(room = c(&quot;er&quot;, ## &quot;gr&quot;), central = c(&quot;ec&quot;, &quot;gc&quot;, &quot;hp&quot;)), steptol = 1e-12) ## ## Frequencies of alternatives: ## ec er gc gr hp ## 0.071111 0.093333 0.636667 0.143333 0.055556 ## ## bfgs method ## 22 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = -268 ## last step couldn&#39;t find higher value ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## er:(intercept) -1.1306e+00 8.1194e-02 -13.9248 &lt; 2.2e-16 *** ## gc:(intercept) -5.9927e-03 1.4482e-02 -0.4138 0.679023 ## gr:(intercept) -1.1754e+00 7.9973e-02 -14.6976 &lt; 2.2e-16 *** ## hp:(intercept) -3.7729e-02 1.5564e-02 -2.4240 0.015349 * ## ic -7.1706e-05 2.5598e-05 -2.8012 0.005091 ** ## oc -1.8999e-04 6.1450e-05 -3.0919 0.001989 ** ## iv:room 2.0968e-02 2.3418e-03 8.9538 &lt; 2.2e-16 *** ## iv:central 2.4194e-02 9.0264e-03 2.6804 0.007353 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -1003.4 ## McFadden R^2: 0.01842 ## Likelihood ratio test : chisq = 37.66 (p.value = 1.3171e-07) It can be seen that the coefficients for installation costs and operation costs are significant and have the expected signs. In addition, inclusive values are estimated for each nest. Both coefficients appear as significant; however, in this case, the z-value is calculated in reference to zero. As explained in the preceding section, however, in reality we should think of those coefficients as correlations (\\(1-\\lambda_s\\)). Accordingly, the correlations for the two nests are: 1 - nl1$coefficients[&quot;iv:room&quot;] ## iv:room ## 0.9790318 1 - nl1$coefficients[&quot;iv:central&quot;] ## iv:central ## 0.9758055 Clearly, the correlations within the nests are very high. We can test whether the correlation is significant by means of the t-test discussed before: (nl1$coefficients[&quot;iv:room&quot;] - 1) / sqrt(vcov(nl1)[&quot;iv:room&quot;,&quot;iv:room&quot;]) ## iv:room ## -418.0641 (nl1$coefficients[&quot;iv:central&quot;] - 1) / sqrt(vcov(nl1)[&quot;iv:central&quot;,&quot;iv:central&quot;]) ## iv:central ## -108.1057 The cutoff value of the t-test at the 5% level of confidence is 1.96. The high values of the t-test indicate that the null hypothesis, i.e., \\(\\lambda_s=1\\) can be rejected. In addition, since the multinomial logit model is a special case of the nested logit model (when \\(\\lambda_s=1\\) for all \\(s\\)), it is also possible to conduct a likelihood ratio test, as follows: lrtest(mod3, nl1) ## Likelihood ratio test ## ## Model 1: depvar ~ ic + oc ## Model 2: depvar ~ ic + oc ## #Df LogLik Df Chisq Pr(&gt;Chisq) ## 1 6 -1008.2 ## 2 8 -1003.4 2 9.6697 0.007948 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The likelihood ratio test also indicates that the null hypothesis, i.e., that the nested logit collapses to the multinomial logit, can be rejected at a high level of confidence (\\(p-val &lt; 0.01\\)). It is possible to impose some restrictions to the estimation of the parameters for the nests. The mlogit function allows an additional argument that forces all parameters \\(lambda_s\\) to take identical values. The argument is un.nest.el (for “unique nest elasticity”), and when set to TRUE only one parameter will be estimated (call this Nested Logit Model 2, nl2): nl2 &lt;- mlogit(depvar ~ ic + oc, H, nests = list(room = c( &#39;er&#39;, &#39;gr&#39;), central = c(&#39;ec&#39;, &#39;gc&#39;, &#39;hp&#39;)), un.nest.el = TRUE, steptol = 1e-12) summary(nl2) ## ## Call: ## mlogit(formula = depvar ~ ic + oc, data = H, nests = list(room = c(&quot;er&quot;, ## &quot;gr&quot;), central = c(&quot;ec&quot;, &quot;gc&quot;, &quot;hp&quot;)), un.nest.el = TRUE, ## steptol = 1e-12) ## ## Frequencies of alternatives: ## ec er gc gr hp ## 0.071111 0.093333 0.636667 0.143333 0.055556 ## ## bfgs method ## 19 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 0.0443 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## er:(intercept) -1.1290e+00 7.8672e-02 -14.3508 &lt; 2.2e-16 *** ## gc:(intercept) -1.0474e-02 1.5358e-02 -0.6820 0.4952521 ## gr:(intercept) -1.1817e+00 8.0048e-02 -14.7628 &lt; 2.2e-16 *** ## hp:(intercept) -4.5459e-02 1.5516e-02 -2.9298 0.0033921 ** ## ic -8.3899e-05 2.5826e-05 -3.2486 0.0011598 ** ## oc -2.2841e-04 5.9509e-05 -3.8383 0.0001239 *** ## iv 2.7374e-02 3.2338e-03 8.4649 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -1003.5 ## McFadden R^2: 0.018329 ## Likelihood ratio test : chisq = 37.473 (p.value = 3.6549e-08) Note how the log-likelihood of the Nested Logit Model 2 is almost identical to the log-likelihood of the Nested Logit Model 1. Since nl2 is more parsimonious, it makes sense to prefer this model. 7.10 Substitution patterns with the nested logit model In this section we will revisit the substitution patterns, but now using the nested logit model. To simulate the substitution patterns we will remove each alternative in turn, to examine how the adoption rates change in response. We begin by copying the model matrix: X &lt;- model.matrix(nl2) Using the model matrix, we can calculate the utilities of each alternative. Notice that each utility is divided by the coefficient of the inclusive value: # Electric central exp_V_ec &lt;- exp((X[alt == c(&quot;ec&quot;), &quot;oc&quot;] * coef(nl2)[&quot;oc&quot;] + X[alt == c(&quot;ec&quot;), &quot;ic&quot;] * coef(nl2)[&quot;ic&quot;]) / coef(nl2)[&quot;iv&quot;]) # Gas central exp_V_gc &lt;- exp((coef(nl2)[&quot;gc:(intercept)&quot;] + X[alt == c(&quot;gc&quot;), &quot;oc&quot;] * coef(nl2)[&quot;oc&quot;] + X[alt == c(&quot;gc&quot;), &quot;ic&quot;] * coef(nl2)[&quot;ic&quot;]) / coef(nl2)[&quot;iv&quot;]) # Heat pump exp_V_hp &lt;- exp((coef(nl2)[&quot;hp:(intercept)&quot;] + X[alt == c(&quot;hp&quot;), &quot;oc&quot;] * coef(nl2)[&quot;oc&quot;] + X[alt == c(&quot;hp&quot;), &quot;ic&quot;] * coef(nl2)[&quot;ic&quot;]) / coef(nl2)[&quot;iv&quot;]) # Electric room exp_V_er &lt;- exp((coef(nl2)[&quot;er:(intercept)&quot;] + X[alt == c(&quot;er&quot;), &quot;oc&quot;] * coef(nl2)[&quot;oc&quot;] + X[alt == c(&quot;er&quot;), &quot;ic&quot;] * coef(nl2)[&quot;ic&quot;]) / coef(nl2)[&quot;iv&quot;]) # Gas room exp_V_gr &lt;- exp((coef(nl2)[&quot;gr:(intercept)&quot;] + X[alt == c(&quot;gr&quot;), &quot;oc&quot;] * coef(nl2)[&quot;oc&quot;] + X[alt == c(&quot;gr&quot;), &quot;ic&quot;] * coef(nl2)[&quot;ic&quot;]) / coef(nl2)[&quot;iv&quot;]) The conditional probabilities are the logit models within each nest: #Central, after removing ec cp_mec_c &lt;- data.frame(gc = exp_V_gc / (exp_V_gc + exp_V_hp), hp = exp_V_hp / (exp_V_gc + exp_V_hp)) #Central, after removing gc cp_mgc_c &lt;- data.frame(ec = exp_V_ec / (exp_V_ec + exp_V_hp), hp = exp_V_hp / (exp_V_ec + exp_V_hp)) #Central, after removing hp cp_mhp_c &lt;- data.frame(ec = exp_V_ec / (exp_V_ec + exp_V_gc), gc = exp_V_gc / (exp_V_ec + exp_V_gc)) #Room, after removing central cp_mc_r &lt;- data.frame(er = exp_V_er / (exp_V_er + exp_V_gr), gr = exp_V_gr / (exp_V_er + exp_V_gr)) #Room, after removing er cp_mer_r &lt;- data.frame(gr = exp_V_gr / (exp_V_gr)) #Room, after removing gr cp_mgr_r &lt;- data.frame(er = exp_V_er / (exp_V_er)) #Central, after removing room cp_mr_c &lt;- data.frame(ec = exp_V_ec / (exp_V_ec + exp_V_gc + exp_V_hp), gc = exp_V_gc / (exp_V_ec + exp_V_gc + exp_V_hp), hp = exp_V_hp / (exp_V_ec + exp_V_gc + exp_V_hp)) The marginal probabilities are the logit probabilities of choosing a nest, given the expected maximum utility of each nest: #After removing ec mp_mec &lt;- data.frame(central = exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_gc + exp_V_hp)) / (exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_gc + exp_V_hp)) + exp((coef(nl2)[&quot;iv&quot;] * log(exp_V_er + exp_V_gr)))), room = exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_er + exp_V_gr)) / (exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_gc + exp_V_hp)) + exp((coef(nl2)[&quot;iv&quot;] * log(exp_V_er + exp_V_gr)))) ) #After removing gc mp_mgc &lt;- data.frame(central = exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_ec + exp_V_hp)) / (exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_ec + exp_V_hp)) + exp((coef(nl2)[&quot;iv&quot;] * log(exp_V_er + exp_V_gr)))), room = exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_er + exp_V_gr)) / (exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_ec + exp_V_hp)) + exp((coef(nl2)[&quot;iv&quot;] * log(exp_V_er + exp_V_gr)))) ) #After removing hp mp_mhp &lt;- data.frame(central = exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_ec + exp_V_gc)) / (exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_ec + exp_V_gc)) + exp((coef(nl2)[&quot;iv&quot;] * log(exp_V_er + exp_V_gr)))), room = exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_er + exp_V_gr)) / (exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_ec + exp_V_gc)) + exp((coef(nl2)[&quot;iv&quot;] * log(exp_V_er + exp_V_gr)))) ) #After removing er mp_mer &lt;- data.frame(central = exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_ec + exp_V_gc + exp_V_hp)) / (exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_ec + exp_V_gc + exp_V_hp)) + exp((coef(nl2)[&quot;iv&quot;] * log(exp_V_gr)))), room = exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_gr)) / (exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_ec + exp_V_gc + exp_V_hp)) + exp((coef(nl2)[&quot;iv&quot;] * log(exp_V_gr)))) ) #After removing gr mp_mgr &lt;- data.frame(central = exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_ec + exp_V_gc + exp_V_hp)) / (exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_ec + exp_V_gc + exp_V_hp)) + exp((coef(nl2)[&quot;iv&quot;] * log(exp_V_er)))), room = exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_er)) / (exp(coef(nl2)[&quot;iv&quot;] * log(exp_V_ec + exp_V_gc + exp_V_hp)) + exp((coef(nl2)[&quot;iv&quot;] * log(exp_V_er)))) ) Once that the conditional and marginal choice probabilities for each case have been calculated, the choice probabilities are the product of the conditional and marginal probabilities: #After removing ec nlp_mec &lt;- data.frame(cp_mec_c, cp_mc_r, mp_mec) %&gt;% transmute(p_ec = NA, p_gc = gc * central, p_hp = hp * central, p_er = er * room, p_gr = gr * room) #After removing gc nlp_mgc &lt;- data.frame(cp_mgc_c, cp_mc_r, mp_mgc) %&gt;% transmute(p_ec = ec * central, p_gc = NA, p_hp = hp * central, p_er = er * room, p_gr = gr * room) #After removing hp nlp_mhp &lt;- data.frame(cp_mhp_c, cp_mc_r, mp_mhp) %&gt;% transmute(p_ec = ec * central, p_gc = gc * central, p_hp = NA, p_er = er * room, p_gr = gr * room) #After removing er nlp_mer &lt;- data.frame(cp_mr_c, cp_mer_r, mp_mer) %&gt;% transmute(p_ec = ec * central, p_gc = gc * central, p_hp = hp * central, p_er = NA, p_gr = gr * room) #After removing gr nlp_mgr &lt;- data.frame(cp_mr_c, cp_mgr_r, mp_mgr) %&gt;% transmute(p_ec = ec * central, p_gc = gc * central, p_hp = hp * central, p_er = er * room, p_gr = NA) A quick sanity check can be conducted to ensure that the sum of probabilities for each decision-maker is one: summary(rowSums(nlp_mec, na.rm = TRUE)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 1 1 1 1 1 summary(rowSums(nlp_mgc, na.rm = TRUE)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 1 1 1 1 1 summary(rowSums(nlp_mhp, na.rm = TRUE)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 1 1 1 1 1 summary(rowSums(nlp_mer, na.rm = TRUE)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 1 1 1 1 1 summary(rowSums(nlp_mgr, na.rm = TRUE)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 1 1 1 1 1 Given the above, we can summarize the choice probabilities in the form of adoption rates. The table below shows the original adoption rates (when no alternative was removed) and for the different situations of interest, after removing each alternative: # Original adoption rates p_o &lt;- apply(fitted(nl2, outcome = FALSE), 2, mean) df &lt;- data.frame(Alternative = c(&quot;None&quot;, &quot;ec&quot;, &quot;gc&quot;, &quot;hp&quot;, &quot;er&quot;, &quot;gr&quot; ), rbind(c(p_o[&quot;ec&quot;], p_o[&quot;gc&quot;], p_o[&quot;hp&quot;], p_o[&quot;er&quot;], p_o[&quot;gr&quot;]), apply(nlp_mec, 2, mean), apply(nlp_mgc, 2, mean), apply(nlp_mhp, 2, mean), apply(nlp_mer, 2, mean), apply(nlp_mgr, 2, mean)) ) df %&gt;% kable(col.names = c(&quot;Alternative Removed&quot;, &quot;ec&quot;, &quot;gc&quot;, &quot;hp&quot;, &quot;er&quot;, &quot;gr&quot;), digits = 2) %&gt;% kable_styling() Alternative Removed ec gc hp er gr None 0.07 0.64 0.06 0.09 0.15 ec NA 0.70 0.06 0.09 0.15 gc 0.41 NA 0.34 0.09 0.15 hp 0.08 0.69 NA 0.09 0.15 er 0.07 0.64 0.06 NA 0.23 gr 0.07 0.64 0.06 0.23 NA Notice that the patterns of substitution are no longer proportional between nests. Are these substitution patterns more sensible? 7.11 Elasticities of the nested logit model Recall that the elasticity is the change in probability that results from a one percent change in some attribute. In the case of the multinomial logit model the direct-point elasticity was given by the following expression: \\[ E^{P_{in}}_{x_{ink}} = \\beta_{ik}x_{ink}(1-P_{in}) \\] whereas the cross-point elasticity was: \\[ E^{P_{in}}_{x_{ink}} = -\\beta_{jk}x_{jnk}P_{jn} \\] In the case of the nested logit it is also possible to obtain expressions for the direct- and cross-point elasticity, however, these need to take into account the fact that substitution patterns are not proportional. In particular, the parameter of the inclusive value affects the probability of choosing a node and therefore any alternatives contained there. Accordingly, the direct-point elasticity in the case of a nested logit model is (see Louviere, Hensher, and Swait 2000, 148–49): \\[ E^{P_{in}}_{x_{ink}} = \\Bigg[(1-P_{t}) + \\Big(\\frac{1}{\\lambda_t-1}\\Big)(1-P_{i|t})\\Bigg]\\beta_{ik}x_{ink} \\] where \\(P_t\\) is the marginal probability of choosing nest \\(t\\) and \\(P_{i|t}\\) is the probability of choosing \\(i\\) conditional on choosing nest \\(t\\). Note that when an alternative is not nested, the conditional probability is one, and therefore the elasticity is identical to the elasticity of the multinomial logit model. The cross-elasticity for alternatives in a partition of the nest is: \\[ E^{P_{in}}_{x_{ink}} = -\\Bigg[P_{t} + \\Big(\\frac{1}{\\lambda_t-1}\\Big)P_{i|t}\\Bigg]\\beta_{ik}x_{ink} \\] 7.12 Exercise The blue bus-red bus paradox is a classical illustration of the limitations of the multinomial logit model. This paradox is stated next. The blue bus-red bus paradox There are two initial models, car and blue buses, with systematic utility functions as follows: \\[ V_{blue} = V_{car} \\] According to the multinomial logit model, the probability of choosing either mode is 0.5, since: \\[ P_{car} = \\frac{e^V_{car}}{e^V_{car} + e^V_{blue}} = \\frac{e^V_{car}}{e^V_{car} + e^V_{car}} = \\frac{1}{2} \\] and: \\[ P_{blue} = 1-P_{car}=\\frac{1}{2} \\] A new alternative is introduced. In fact, the new alternative is just some old blue buses painted red. Since consumers do not care about the color of buses, the utility of this new alternative is: \\[ V_{yellow}=V_{bus} = V_{car} \\] The new choice probabilities are now: \\[ \\begin{array}{c} P_{car} = \\frac{e^V{car}}{e^V{car} + e^V{blue} + e^V{yellow}} = \\frac{1}{3}\\\\ P_{blue} = \\frac{e^V{car}}{e^V{car} + e^V{blue} + e^V{yellow}} = \\frac{1}{3}\\\\ P_{yellow} = 1 - P_{car} - P_{blue} = \\frac{1}{3} \\end{array} \\] Proportional substitution patterns imply that the new mode (red bus) draws equally from the alternatives, i.e., car and blue buses. Clearly, this does not make sense. An enterpreneur could paint buses in many different colors and reduce the probability of choosing car to zero as a consequence. Restate the blue bus-red bus situation as a nested model. What are the marginal and conditional probabilities of this model? Use model nl2 in this chapter and calculate the direct-point elasticity at the mean values of the variables, for an increase in the installation costs of Gas Central systems. Use model nl2 in this chapter and calculate the cross-point elasticity at the mean values of the variables, for an increase in the installation costs of Gas Central systems. Reestimate the nested logit model in this chapter, but change the nests to types of energy as follows: Gas: gas central, gas room. Electricity: electric central, electric room, heat pump. Use a single coefficient for the inclusive variables (i.e., set un.nest.el = TRUE). Are the results reasonable? Discuss. References "],
["chapter-7.html", "Chapter 8 Non-Proportional Substitution Patterns I: The Probit Model 8.1 More flexible substitution patterns 8.2 How to use this note 8.3 Learning objectives 8.4 Suggested readings 8.5 Preliminaries 8.6 Probit model 8.7 Exercise", " Chapter 8 Non-Proportional Substitution Patterns I: The Probit Model “Perfectionism means that you try not to leave so much mess to clean up.” — Anne Lamott, Bird by Bird: Some Instructions on Writing and Life “Perfection’s unattainable but it isn’t unapproachable.” — Peter Watts, Blindsight 8.1 More flexible substitution patterns NOTE This chapter is work in progress and is not yet complete. In Chapter 7 the topic of non-proportional substitution was discussed, and a method for deriving logit models using the Generalized Extreme Value method was presented. In this chapter, an alternative model for more flexible substitution patterns is introduced. This model is based on the use of the normal distribution for the random utility, instead of the extreme value distribution. The model is called multinomial probit and can be used to represent arbitrary substitution patterns. On the downside, the model is technically and computationally more demanding. 8.2 How to use this note Remember that the source for the document you are reading is an R Notebook. Throughout the notes, you will find examples of code in segments of text called chunks. This is an example of a chunk: print(&quot;If you are always trying to be normal, you will never know how amazing you can be.&quot;) ## [1] &quot;If you are always trying to be normal, you will never know how amazing you can be.&quot; If you are working with the Notebook version of the document, you can run the code by clicking the ‘play’ icon on the top right corner of the chunk. If you are reading the web-book version of the document, you will often see that the code has already been executed. You can still try it by copying and pasting into your R or RStudio console. 8.3 Learning objectives In this practice, you will learn about: The probit model. Identification of parameters. Estimation of the model. 8.4 Suggested readings Ben-Akiva, M. Lerman, (1985) Discrete Choice Analysis: Theory and Applications to Travel Demand, Chapter 5, pp. 128-129, MIT Press. Ortuzar JD, Willumsen LG (2011) Modelling Transport, Fourth Edition, Chapter 7, pp. 248-250, John Wiley and Sons. Train (2009) Discrete Choice Methods with Simulation, Second Edition, Chapter 5, Cambridge University Press. 8.5 Preliminaries Load the packages used in this section: library(tidyverse) library(mlogit) library(kableExtra) Load the dataset used in this section (from the mlogit package): data(&quot;Heating&quot;) The dataset is in “wide” form, which means that there is one record per decision making unit (i.e. per household), so we need to change the data to “long” format: H &lt;- mlogit.data(Heating, shape = &quot;wide&quot;, choice = &quot;depvar&quot;, varying = c(3:12)) This is the same dataset used in the preceding chapter with five heating systems: Gas Central (gc) Gas Room (gr) Electric Central (ec) Electric Room (er) Heat Pump (hp) This dataset was used to illustrate the nested logit model. The results before suggested that there was a strong correlation among central systems and room-based systems, and therefore a nested logit model was an improvement over the multinomial logit model. 8.6 Probit model Assume that the choice problem of decision-maker \\(n\\) consists of \\(J\\) alternatives, and that the utility of each alternative is decomposed in two parts, a systematic utility \\(V\\) and a random utility \\(\\epsilon\\), as follows: \\[ U_{n1} = V_{n1} + \\epsilon_{n1}\\\\ \\cdots\\\\ U_{nj} = V_{nj} + \\epsilon_{nj}\\\\ \\cdots\\\\ U_{nJ} = V_{nJ} + \\epsilon_{nJ} \\] The random utility terms can be collected as follows: \\[ \\epsilon_n = [\\epsilon_1, \\cdots, \\epsilon_j, \\cdots, \\epsilon_J] \\] Previously, the logit model was derived based on the assumption that the random utility terms \\(\\epsilon_j\\) were Extreme Value Type I. Now, the assumption is that the vector of random utilities \\(\\epsilon_n\\) follows the normal distribution with a mean vector of zero and a covariance matrix \\(\\Sigma_n\\) that can be a function of variables specific to a decision-maker. The probability distribution function is: \\[ \\phi(\\epsilon_n)=\\frac{1}{(2\\pi)^{J/2}|\\Sigma_n|^{1/2}}e^{-\\frac{1}{2}\\epsilon_n^T\\Sigma_n^{-1}\\epsilon_n} \\] As before, the probability of choosing alternative \\(j\\) is given by the following expression: \\[ P_j=P(U_j\\ge U_k;\\forall k\\ne j) \\] Since the probability is a function of the difference in utilities, it is possible to define the following expressions for differences with respect to the utility of the alternative whose probability we wish to calculate (i.e., \\(j\\)): \\[ \\tilde{U}_{nkj}=U_{nk} - U_{nj} \\] which implies: \\[ \\tilde{V}_{nkj}=V_{nk} - V_{nj}\\\\ \\tilde{\\epsilon}_{nkj} = \\epsilon_{nk} - \\epsilon_{nj} \\] In this case, the probability expression becomes: \\[ P_j=P(U_{nkj}\\le 0;\\forall k\\ne j) \\] \\[ \\phi(\\tilde{\\epsilon}_n)=\\frac{1}{(2\\pi)^{J/2}|\\tilde{\\Sigma}_n|^{1/2}}e^{-\\frac{1}{2}\\tilde{\\epsilon}_n^T\\tilde{\\Sigma}_n^{-1}\\tilde{\\epsilon}_n} \\] Now we are ready to estimate a multinomial logit model (we called this Model 3 in Chapter 4): mod3 &lt;- mlogit(depvar ~ ic + oc, H, reflevel = &quot;ec&quot;) summary(mod3) ## ## Call: ## mlogit(formula = depvar ~ ic + oc, data = H, reflevel = &quot;ec&quot;, ## method = &quot;nr&quot;, print.level = 0) ## ## Frequencies of alternatives: ## ec er gc gr hp ## 0.071111 0.093333 0.636667 0.143333 0.055556 ## ## nr method ## 6 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 9.58E-06 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## er:(intercept) 0.19459102 0.20424212 0.9527 0.3407184 ## gc:(intercept) 0.05213336 0.46598878 0.1119 0.9109210 ## gr:(intercept) -1.35058266 0.50715442 -2.6631 0.0077434 ** ## hp:(intercept) -1.65884594 0.44841936 -3.6993 0.0002162 *** ## ic -0.00153315 0.00062086 -2.4694 0.0135333 * ## oc -0.00699637 0.00155408 -4.5019 6.734e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -1008.2 ## McFadden R^2: 0.013691 ## Likelihood ratio test : chisq = 27.99 (p.value = 8.3572e-07) nl2 &lt;- mlogit(depvar ~ ic + oc, H, nests = list(room = c( &#39;er&#39;, &#39;gr&#39;), central = c(&#39;ec&#39;, &#39;gc&#39;, &#39;hp&#39;)), un.nest.el = TRUE, steptol = 1e-12) summary(nl2) ## ## Call: ## mlogit(formula = depvar ~ ic + oc, data = H, nests = list(room = c(&quot;er&quot;, ## &quot;gr&quot;), central = c(&quot;ec&quot;, &quot;gc&quot;, &quot;hp&quot;)), un.nest.el = TRUE, ## steptol = 1e-12) ## ## Frequencies of alternatives: ## ec er gc gr hp ## 0.071111 0.093333 0.636667 0.143333 0.055556 ## ## bfgs method ## 19 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 0.0443 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## er:(intercept) -1.1290e+00 7.8672e-02 -14.3508 &lt; 2.2e-16 *** ## gc:(intercept) -1.0474e-02 1.5358e-02 -0.6820 0.4952521 ## gr:(intercept) -1.1817e+00 8.0048e-02 -14.7628 &lt; 2.2e-16 *** ## hp:(intercept) -4.5459e-02 1.5516e-02 -2.9298 0.0033921 ** ## ic -8.3899e-05 2.5826e-05 -3.2486 0.0011598 ** ## oc -2.2841e-04 5.9509e-05 -3.8383 0.0001239 *** ## iv 2.7374e-02 3.2338e-03 8.4649 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -1003.5 ## McFadden R^2: 0.018329 ## Likelihood ratio test : chisq = 37.473 (p.value = 3.6549e-08) prob1 &lt;- mlogit(depvar ~ ic + oc, H, reflevel = &quot;ec&quot;, probit = TRUE, R=100) summary(prob1) prob2 &lt;- mlogit(depvar ~ ic + oc, H, reflevel = &quot;ec&quot;, probit = TRUE, constPar=c(&#39;hp.hp&#39;, &quot;gr.hp&quot;, &quot;gr.gr&quot;, &quot;gc.gr&quot;, &quot;gc.gc&quot;, &quot;er.hp&quot;)) summary(prob2) prob3 &lt;- mlogit(depvar ~ ic + oc, H, reflevel = &quot;ec&quot;, probit = TRUE, constPar=c(&quot;er.er&quot;, &quot;gr.hp&quot;, &quot;gc.gr&quot;, &quot;er.hp&quot;, &quot;er.gc&quot;), R = 50) summary(prob3) L1 &lt;- matrix(0, 4, 4) L1[!upper.tri(L1)] &lt;- c(1, 0, coef(prob3)[7], 0, coef(prob3)[8], 0, coef(prob3)[9:10], 0, coef(prob3)[11]) L1 L1 %*% t(L1) L1 &lt;- matrix(0, 4, 4) L1[!upper.tri(L1)] &lt;- c(1, coef(prob1)[7:15]) L1 L1 %*% t(L1) To simulate this scenario, we begin by copying the input dataframe: H_rebate &lt;- H In the new dataframe that will simulate the rebate, replacing the cost of installation as follows: H_rebate[H_rebate$alt == &quot;gc&quot;, &quot;ic&quot;] &lt;- 1.15 * H_rebate[H_rebate$alt == &quot;gc&quot;, &quot;ic&quot;] We can calculate the market shares of the “do nothing” and “rebate” policies and compare their shares (which are the mean values of the predictions): data.frame(Policy = c(&quot;Do nothing&quot;, &quot;15% rebate&quot;), rbind(apply(predict(mod3, newdata = H), 2, mean), apply(predict(mod3, newdata = H_rebate), 2, mean))) Nested logit: data.frame(Policy = c(&quot;Do nothing&quot;, &quot;15% rebate&quot;), rbind(apply(predict(nl2, newdata = H), 2, mean), apply(predict(nl2, newdata = H_rebate), 2, mean))) Probit: data.frame(Policy = c(&quot;Do nothing&quot;, &quot;15% rebate&quot;), rbind(apply(predict(prob3, newdata = H), 2, mean), apply(predict(prob3, newdata = H_rebate), 2, mean))) 8.7 Exercise References "],
["chapter-8.html", "Chapter 9 Dealing with Heterogeneity I: The Mixed Logit Model 9.1 Preliminaries 9.2 Taste variations in the population 9.3 How to use this note 9.4 Learning objectives 9.5 Suggested readings 9.6 Mixed logit 9.7 Estimation 9.8 Practical example 9.9 Behavioral insights from the mixed logit model 9.10 Using covariates to capture variations in taste 9.11 Revisiting the example 9.12 Full covariance matrix for random components 9.13 Final remarks 9.14 Exercise", " Chapter 9 Dealing with Heterogeneity I: The Mixed Logit Model “Life produces a different taste each time you take it.” — Frank Herbert “Develop flexibility and you will be firm; cultivate yielding and you will be strong.” — Liezi, The Book of Master Lie 9.1 Preliminaries Load the packages used in this section: library(tidyverse) library(mlogit) library(gmnl) library(kableExtra) library(gridExtra) 9.2 Taste variations in the population The preceding chapters (Chapters 7 and 8) were concerned with the IIA property of the logit model. As discussed there, when the specification of the model is incomplete, the presence of residual correlation can often lead to inappropriate substitution patterns. Accordingly, the GEV family of models, of which the nested logit is a member, and the multinomial probit model aimed at introducing more flexible substitution patterns. The nested logit model achieves this by inducing a hierarchical decision-making structure where alternatives within nests are correlated, whereas the multinomial probit model introduces a flexible, but technically demand, covariance structure for multinomial choices. To motivate the discussion, we will use the following dataset (from the AER package): data(&quot;TravelMode&quot;, package = &quot;AER&quot;) This dataset is included in the AER package (Applied Econometrics with R) and is a companion to Greene’s popular econometrics textbook (Greene 2003). The contents of the dataset are information about mode choices for individual travelers. The alternatives are travel by air, train, bus, and car. The following attributes about the alternatives are available: vcost: the vehicle cost component. travel: travel time in vehicle (in min). wait: waiting time(in min). gcost: a generalized cost measure. The median of these attributes as well as the proportion of choices by mode are summarized in the following table: Proportion &lt;- TravelMode %&gt;% filter(choice == &quot;yes&quot;) %&gt;% select(mode) %&gt;% group_by(mode) %&gt;% summarise(no_rows = length(mode)) df &lt;- data.frame(Mode = c(&quot;air&quot;, &quot;train&quot;, &quot;bus&quot;, &quot;car&quot;), vcost = c(median(filter(TravelMode, mode == &quot;air&quot;)$vcost), median(filter(TravelMode, mode == &quot;train&quot;)$vcost), median(filter(TravelMode, mode == &quot;bus&quot;)$vcost), median(filter(TravelMode, mode == &quot;car&quot;)$vcost)), wait = c(median(filter(TravelMode, mode == &quot;air&quot;)$wait), median(filter(TravelMode, mode == &quot;train&quot;)$wait), median(filter(TravelMode, mode == &quot;bus&quot;)$wait), median(filter(TravelMode, mode == &quot;car&quot;)$wait)), travel = c(median(filter(TravelMode, mode == &quot;air&quot;)$gcost), median(filter(TravelMode, mode == &quot;train&quot;)$gcost), median(filter(TravelMode, mode == &quot;bus&quot;)$gcost), median(filter(TravelMode, mode == &quot;car&quot;)$gcost)), Proportion = Proportion$no_rows/(nrow(TravelMode)/4) ) df %&gt;% kable(digits = 3) %&gt;% kable_styling() Mode vcost wait travel Proportion air 81 64 100.0 0.276 train 42 34 135.0 0.300 bus 32 35 108.0 0.143 car 16 0 94.5 0.281 In addition, there are two individual-level attributes, the income of the traveler (in 10,000s) and the size of the traveling party (number of people in party). The dataset is in “long” form, which means that there is one record per decision-making situation per decision-maker. Here, we format the dataframe in preparation for use with the packages mlogit and gmnl: TM &lt;- mlogit.data(TravelMode, choice = &quot;choice&quot;, shape = &quot;long&quot;, alt.levels = c(&quot;air&quot;, &quot;train&quot;, &quot;bus&quot;, &quot;car&quot;)) We then begin by estimating multinomial logit, nested logit, and multinomial probit models for the sample dataset. The code for estimating each of these models is as follows: library(mlogit) #Multinomial logit mnl0 &lt;- mlogit(choice ~ vcost + travel + wait | 1, data = TM) #summary(mnl0) #Nested logit: nl &lt;- mlogit(choice ~ vcost + travel + wait | 1, data = TM, nests = list(land = c( &quot;car&quot;, &quot;bus&quot;, &quot;train&quot;), air = c(&quot;air&quot;)), un.nest.el = TRUE) #summary(nl) #Multinomial probit: prbt &lt;- mlogit(choice ~ vcost + travel + wait | 1, data = TM, probit = TRUE) #summary(prbt) Notice that the nesting structure places all land-based modes in a nest and air in a separate nest (you can try different nesting structures if you wish!). Figure 9.1: Nests in the nested logit model The results of these models are summarized in Table ((???)) below. # Estimate a constants only model to calculate McFadden&#39;s _adjusted_ rho2 mnl_null &lt;- mlogit(choice ~ 1, data = TM) mnl0.summary &lt;- rownames_to_column(data.frame(summary(mnl0)$CoefTable), &quot;Variable&quot;) %&gt;% transmute(Variable, Estimate, pval = `Pr...z..`) nl.summary &lt;- rownames_to_column(data.frame(summary(nl)$CoefTable), &quot;Variable&quot;) %&gt;% transmute(Variable, Estimate, pval = `Pr...z..`) prbt.summary &lt;- rownames_to_column(data.frame(summary(prbt)$CoefTable), &quot;Variable&quot;) %&gt;% transmute(Variable, Estimate, pval = `Pr...z..`) df_logit &lt;- full_join(mnl0.summary, nl.summary, by = &quot;Variable&quot;) %&gt;% full_join(prbt.summary, by = &quot;Variable&quot;) kable(df_logit, &quot;html&quot;, digits = 4, col.names = c(&quot;Variable&quot;, &quot;Estimate&quot;, &quot;p-value&quot;, &quot;Estimate&quot;, &quot;p-value&quot;, &quot;Estimate&quot;, &quot;p-value&quot;), caption = &quot;Base models: multinomial logit (MNL), nested logit (NL), multinomial probit (MNP)&quot;) %&gt;% kable_styling() %&gt;% add_header_above(c(&quot; &quot; = 1, &quot;MNL&quot; = 2, &quot;NL&quot; = 2, &quot;MNP&quot; = 2)) %&gt;% footnote(general = c(paste0(&quot;Log-Likelihood: MNL = &quot;, round(mnl0$logLik[1], digits = 3), &quot;; NL = &quot;, round(nl$logLik[1], digits = 3), &quot;; MNP = &quot;, round(prbt$logLik[1], digits = 3)), paste0(&quot;McFadden Adjusted R^2: MNL = &quot;, round(1 - (mnl0$logLik[1] - nrow(mnl0.summary)) / mnl_null$logLik[1], digits = 3), &quot;; NL = &quot;, round(1 - (nl$logLik[1] - nrow(nl.summary)) / mnl_null$logLik[1], digits = 3), &quot;; MNP = &quot;, round(1 - (prbt$logLik[1] - nrow(prbt.summary)) / mnl_null$logLik[1], digits = 3)))) Table 9.1: Base models: multinomial logit (MNL), nested logit (NL), multinomial probit (MNP) MNL NL MNP Variable Estimate p-value Estimate p-value Estimate p-value train:(intercept) -0.7867 0.1917 0.5679 0.1106 0.3759 0.1614 bus:(intercept) -1.4336 0.0352 0.1988 0.6538 0.2616 0.2837 car:(intercept) -4.7399 0.0000 -1.8571 0.0084 -0.7027 0.0454 vcost -0.0139 0.0365 -0.0106 0.0227 -0.0070 0.0090 travel -0.0040 0.0000 -0.0036 0.0000 -0.0018 0.0015 wait -0.0969 0.0000 -0.0554 0.0000 -0.0222 0.0000 iv NA NA 0.4655 0.0000 NA NA train.bus NA NA NA NA 0.8199 0.0001 train.car NA NA NA NA 0.8734 0.0000 bus.bus NA NA NA NA 0.3296 0.1037 bus.car NA NA NA NA 0.2652 0.3918 car.car NA NA NA NA 0.3895 0.0168 Note: Log-Likelihood: MNL = -192.889; NL = -187.029; MNP = -192.562 McFadden Adjusted R^2: MNL = 0.299; NL = 0.316; MNP = 0.283 The nested logit model and the multinomial probit model, as discussed in the preceding chapters, accommodate correlations among alternatives. For instance, the inclusive value of the nested logit is in the \\(0&lt;iv&lt;1\\) range, and it is significantly different from 1 (recall that when close to one, the model collapses to the multinomial logit model). The z-score to test whether it is different from one is: (nl$coefficients[&quot;iv&quot;] - 1) / sqrt(vcov(nl)[&quot;iv&quot;,&quot;iv&quot;]) ## iv ## -5.370013 The multinomial probit model also has covariance components that are significant, although, as discussed before, it is not straightforward to interpret them. Based on the results of the models we can obtain some behavioral insights. For instance, we know that the ratio of two coefficients represents the willingness to pay. If we use the coefficients of the multinomial logit model in the example, we can see that the willingness to pay is 0.29 dollars per minute of travel time saved and 6.97 dollars per minute of waiting time saved: \\[ \\begin{array}{c} -\\frac{\\partial \\text{vcost}}{\\partial \\text{travel}} = \\frac{\\beta_{travel}}{\\beta_{vcost}} = \\frac{-0.004}{-0.0139} \\simeq 0.29 \\\\ \\\\ -\\frac{\\partial \\text{vcost}}{\\partial \\text{wait}} = \\frac{\\beta_{wait}}{\\beta_{vcost}} = \\frac{-0.0969}{-0.0139} \\simeq 6.97 \\end{array} \\] From this, we can see that travelers dislike waiting time more than travel time, and would be willing to pay more on a per-minute basis to reduce waiting time than travel time. A question that remains outstanding with these models is whether there are taste variations in the population. For instance, we might wonder whether all decision-makers dislike travel time (or waiting time) identically, or whether there is heterogeneity (i.e., variability) in the way they respond to these attributes of the alternatives. Are younger people more impatient and value more highly their time? Are older people less patient? Or more affluent travelers, they might have a higher value of time. A straightforward way to answer these questions would be to incorporate covariates for the individual attributes that we think are useful/relevant to explain variations in taste. But, as with the unexplained correlations that contradict proportional substitution patterns, in empirical applications it is also possible that unexplained heterogeneity remains even after introducing available covariates. The objective of this chapter is to introduce a method, based on the logit model, to account for unobserved heterogeneity. The model is variously called mixed logit, kernel logit, and logit with error components or random coefficients. These labels result from the way in which a mix of distributions are used in the model to produce either error components or random coefficients. These distributions wrap around the kernel of a multinomial logit model. Technically, the multinomial probit model can deal with the issue of variations in taste, but as we have seen, this is technically quite demanding and the interpretation of the results is complicated too. The mixed logit presents similar computational challenges as the multinomial probit, since the distributions for the mixture require numerical integration. On the other hand, the results are often easier to interpret from a behavioral standpoint. 9.3 How to use this note Remember that the source for the document you are reading is an R Notebook. Throughout the notes, you will find examples of code in segments of text called chunks. This is an example of a chunk: print(&quot;A piece of advice: never punch a shark in the mouth&quot;) ## [1] &quot;A piece of advice: never punch a shark in the mouth&quot; If you are working with the Notebook version of the document, you can run the code by clicking the ‘play’ icon on the top right corner of the chunk. If you are reading the web-book version of the document, you will often see that the code has already been executed. You can still try it by copying and pasting into your R or RStudio console. 9.4 Learning objectives In this practice, you will learn about: Taste heterogeneity. Random coefficients. The mixed logit model. Behavioral insights from random coefficients. 9.5 Suggested readings Ben-Akiva, M. Lerman, (1985) Discrete Choice Analysis: Theory and Applications to Travel Demand, Chapter 5, pp. 124-125, MIT Press. Hensher, D.A., Rose, J.M., Greene, W.H (2005) Applied Choice Analysis: A Primer, Chapters 15 and 16, Cambridge University Press. Louviere, J.J., Hensher, D.A., Swait, J.D. (2000) Stated Choice Methods: Analysis and Application, Chapter 6, pp. 199-205, Cambridge University Press. Ortuzar JD, Willumsen LG (2011) Modelling Transport, Fourth Edition, Chapter 7, pp. 250-256, John Wiley and Sons. Train (2009) Discrete Choice Methods with Simulation, Second Edition, Chapter 6 and Chapter 11, Cambridge University Press. 9.6 Mixed logit To introduce the mixed logit model, we will begin with a simple situation with two alternatives - say, do nothing (\\(DN\\)) and buy a new phone (\\(NP\\)). One way to investigate taste variations in the population is to allow each decision-maker to have their own coefficient for at least some variables. This is illustrated by means of the following utility functions (notice that the utilities have alternative-level attributes only): \\[ \\begin{array}{ll} U_{i,DN} =&amp; \\theta_{DN} &amp;+ \\alpha_{ic}\\text{cost}_{DN} + \\alpha_{is}\\text{speed}_{DN} &amp;+ \\mu_{id} \\text{duration}_{DN} + \\mu_{it} \\text{time}_{DN} &amp;+ \\epsilon_{i,DN}\\\\ U_{i,NP} =&amp; &amp;+\\alpha_{ic}\\text{cost}_{NP} + \\alpha_{is}\\text{speed}_{NP}&amp; + \\mu_{id} \\text{duration}_{NP} + \\mu_{it} \\text{time}_{NP} &amp;+ \\epsilon_{i,NP}\\\\ \\end{array} \\] As seen above, the coefficients are specific to decision-maker \\(i\\). Clearly, estimating a coefficient for each decision-maker is not possible due to the incidental parameter problem: the number of parameters approaches the size of the sample. What we can do, instead, is to define the parameters in some parsimonious way that requires fewer than \\(N\\) coefficients (i.e., the number of decision-makers in the sample). To do this, suppose that we select some coefficients for expansion in the following manner: \\[ \\begin{array}{ll} \\alpha_{ic}=a_c + \\eta_{ic}\\\\ \\alpha_{is}=a_s + \\eta_{is} \\end{array} \\] where the terms \\(a_c\\) and \\(a_s\\) are fixed coefficients, and the terms \\(\\eta_{ic}\\) and \\(\\eta_{is}\\) are random terms drawn from, say, a bivariate random normal distribution: \\[ \\eta \\sim N(0,\\Sigma_{\\eta}) \\] The parameters are now specific to the decision-maker, and depend on only a small number of parameters, i.e., \\(a_c\\), \\(a_s\\), and the parameters used to specify covariance matrix \\(\\Sigma\\), which in the case of two alternatives involves at most three parameters, since: \\[ \\Sigma_{\\eta} = \\begin{bmatrix} \\sigma_{\\eta,cc} &amp; \\sigma_{\\eta,sc} \\\\ \\sigma_{\\eta,cs} &amp; \\sigma_{\\eta,ss} \\end{bmatrix} \\] Next, suppose that the terms \\(\\mu_{id}\\) and \\(\\mu_{it}\\) are random terms drawn from, say again, a bivariate random normal distribution: \\[ \\mu \\sim N(0,\\Sigma_{\\mu}) \\] with: \\[ \\Sigma_{\\mu} = \\begin{bmatrix} \\sigma_{\\mu,dd} &amp; \\sigma_{\\mu,td} \\\\ \\sigma_{\\mu,dt} &amp; \\sigma_{\\mu,tt} \\end{bmatrix} \\] Finally, assume that the terms \\(\\epsilon\\) follow the Extreme Value Type I distribution. There are a few situations that can be derived from this setup as follows. First, imposing the largest number of restrictions to the parameters, we have \\(\\eta_{ic}=\\eta_{is}=\\mu_{id}=\\mu_{it}=0\\). It follows then that the utility functions become: \\[ \\begin{array}{ll} U_{i,DN} =&amp; \\theta_{DN} &amp;+ a_{c}\\text{cost}_{DN} + a_{s}\\text{speed}_{DN} &amp;+ \\epsilon_{i,DN}\\\\ U_{i,NP} =&amp; &amp;+a_{c}\\text{cost}_{NP} + a_{s}\\text{speed}_{NP}&amp; + \\epsilon_{i,NP}\\\\ \\end{array} \\] and clearly the model collapses into the logit model. Next, we could restrict some of the parameters, say \\(\\mu_{id}=\\mu_{it}=0\\), the the utility functions are: \\[ \\begin{array}{ll} U_{i,DN} =&amp; \\theta_{DN} &amp;+ \\alpha_{ic}\\text{cost}_{DN} + \\alpha_{is}\\text{speed}_{DN} &amp;+ \\epsilon_{i,DN}\\\\ U_{i,NP} =&amp; &amp;+\\alpha_{ic}\\text{cost}_{NP} + \\alpha_{is}\\text{speed}_{NP} &amp;+ \\epsilon_{i,NP}\\\\ \\end{array} \\] This is called a model with random coefficients. Alternatively, we could restrict parameters so that \\(\\eta_{ic}=\\eta_{is}=0\\), in which case the utility functions become: \\[ \\begin{array}{ll} U_{i,DN} =&amp; \\theta_{DN} &amp;+ a_{c}\\text{cost}_{DN} + a_{s}\\text{speed}_{DN} &amp;+ \\mu_{id} \\text{duration}_{DN} + \\mu_{it} \\text{time}_{DN} &amp;+ \\epsilon_{i,DN}\\\\ U_{i,NP} =&amp; &amp;+a_{c}\\text{cost}_{NP} + a_{s}\\text{speed}_{NP}&amp; + \\mu_{id} \\text{duration}_{NP} + \\mu_{it} \\text{time}_{NP} &amp;+ \\epsilon_{i,NP}\\\\ \\end{array} \\] The result is the error components model. In fact, the error components and the random coefficients are equivalent depending on whether \\(a_c=a_s=0\\) or conversely, whether constants are added to the error components. More generally, we can express the utility of alternative \\(j\\) for decision-maker \\(i\\) as follows: \\[ U_{ij}=\\theta&#39;w_{ij} + \\alpha_i&#39;x_{ij} + \\mu&#39;_iz_{ij}+\\epsilon_{ij} \\] where \\(\\theta\\) is a vector of fixed parameters associated with attributes \\(w_{ij}\\), \\(\\alpha_i\\) is a vector of random coefficients associated with attributes \\(x_{ij}\\), and \\(\\mu_i\\) is a vector of error components associated with attributes \\(z_{ij}\\). As we saw above, there are several ways of specifying the coefficients to give error components or random coefficients. These terms, in turn, depend on the parsimonious use of random distributions. For simplicity, then, lets say that the utility is written as follows: \\[ U_{ij}=\\beta_i&#39;x_{ij} + \\epsilon_{ij} = \\beta(\\sigma)&#39;x_{ij} + \\epsilon_{ij} = V_{ij} + \\epsilon_{ij} \\] where \\(\\sigma\\) is the collection of parameters that define the distributions used in the specification of the utility function. This means that, following the usual approach, we can obtain the following model for the probability of choosing alternative \\(j\\): \\[ P_j=\\frac{e^{V_{ij}}}{\\sum_k e^{V_{ik}}} = \\frac{e^{\\beta_i&#39;x_{ij}}}{\\sum_k e^{\\beta_i&#39;x_{ik}}} = \\frac{e^{\\beta(\\sigma)&#39;x_{ij}}}{\\sum_k e^{\\beta(\\sigma)&#39;x_{ik}}} \\] Clearly, when there are no random coefficients or error terms, \\(\\beta(\\sigma)=\\beta\\), which is simply a set of fixed coefficients, and the model is simply the multinomial logit, which can be estimated using the usual approach. On the other hand, when we add mixtures of distributions to obtain the mixed logit (with random coefficients and/or error components), we typically do not know a priori the parameters of the distributions, and therefore the probability needs to be calculated as a weighted average of the possible values of \\(\\beta_i\\): \\[ P_j = \\int\\frac{e^{\\beta_i&#39;x_{ij}}}{\\sum_k e^{\\beta_i&#39;x_{ik}}} \\phi(\\beta|\\sigma)d\\beta = \\int \\frac{e^{\\beta(\\sigma)&#39;x_{ij}}}{\\sum_k e^{\\beta(\\sigma)&#39;x_{ik}}}\\phi(\\beta|\\sigma)d\\beta \\] where the integral provides the weighted average given a mixing distribution \\(\\phi\\). A consequence of mixing distributions is that despite the mixed logit, despite having a logit kernel, does not display proportional substitution. To see this, recall that the odds-ratio of the multinomial logit model was: \\[ \\frac{P_j}{P_m}=\\frac{\\frac{e^{V_{ij}}}{\\sum_k e^{V_{ik}}}}{\\frac{e^{V_{im}}}{\\sum_k e^{V_{ik}}}} = \\frac{e^{V_{ij}}}{e^{V_{im}}} \\] The above is the independence of irrelevant alternatives property: the odds-ratio of a given pair of alternatives does not dependend on the utilities/attributes of any other alternatives. However, in the case of the mixed logit, the denominators of the probabilities are inside the integral and do not cancel each other: \\[ \\frac{P_j}{P_m}=\\frac{\\int\\frac{e^{V_{ij}}}{\\sum_k e^{V_{ik}}}\\phi(\\beta|\\sigma)d\\beta}{\\int\\frac{e^{V_{im}}}{\\sum_k e^{V_{ik}}}\\phi(\\beta|\\sigma)d\\beta} \\] As a consequence, the odds-ratio depends on all alternatives, and not only alternatives \\(j\\) and \\(m\\). 9.7 Estimation Very much like the multinomial probit model, estimation of the mixed logit model is based on simulation techniques, more concretely Simulated Maximum Likelihood. 9.8 Practical example For a practical example, we estimate three mixed logit models using the variables vcost (vehicle cost), travel (travel time in min), and wait (waiting time in minutes). The difference between the models is which variables have random coefficients. The models are as follows: Model 1 (MIXL T), a model with a random coefficient for travel time: \\[ \\begin{array}{lll} V_{i,air} &amp;= &amp; + \\beta_{vcost}vcost_{air} + \\beta_{wait}wait_{air} + \\beta_{i,travel}travel_{air}\\\\ V_{i,train} &amp;=\\beta_{train} &amp; + \\beta_{vcost}vcost_{train} + \\beta_{wait}wait_{train} + \\beta_{i,travel}travel_{train}\\\\ V_{i,bus} &amp;=\\beta_{bus} &amp;+ \\beta_{vcost}vcost_{bus}+\\beta_{wait}wait_{bus}+\\beta_{i,travel}travel_{bus}\\\\ V_{i,car} &amp;=\\beta_{car} &amp;+ \\beta_{vcost}vcost_{bus}+\\beta_{wait}wait_{bus}+\\beta_{i,travel}travel_{bus} \\end{array} \\] Model 2 (MIXL W), a model with a random coefficient for wait time: \\[ \\begin{array}{lll} V_{i,air} &amp;= &amp; + \\beta_{vcost}vcost_{air} + \\beta_{i,wait}wait_{air} + \\beta_{travel}travel_{air}\\\\ V_{i,train} &amp;=\\beta_{train} &amp; + \\beta_{vcost}vcost_{train} + \\beta_{i,wait}wait_{train} + \\beta_{travel}travel_{train}\\\\ V_{i,bus} &amp;=\\beta_{bus} &amp;+ \\beta_{vcost}vcost_{bus}+\\beta_{i,wait}wait_{bus}+\\beta_{travel}travel_{bus}\\\\ V_{i,car} &amp;=\\beta_{car} &amp;+ \\beta_{vcost}vcost_{car}+\\beta_{i,wait}wait_{car}+\\beta_{travel}travel_{car} \\end{array} \\] And Model 2 (MIXL W), a model with random coefficients for both travel and wait time: \\[ \\begin{array}{lll} V_{i,air} &amp;= &amp; + \\beta_{vcost}vcost_{air} + \\beta_{i,wait}wait_{air} + \\beta_{i,travel}travel_{air}\\\\ V_{i,train} &amp;=\\beta_{train} &amp; + \\beta_{vcost}vcost_{train} + \\beta_{i,wait}wait_{train} + \\beta_{i,travel}travel_{train}\\\\ V_{i,bus} &amp;=\\beta_{bus} &amp;+ \\beta_{vcost}vcost_{bus}+\\beta_{i,wait}wait_{bus}+\\beta_{i,travel}travel_{bus}\\\\ V_{i,car} &amp;=\\beta_{car} &amp;+ \\beta_{vcost}vcost_{car}+\\beta_{i,wait}wait_{car}+\\beta_{i,travel}travel_{car} \\end{array} \\] The random coefficients are defined in the following manner, with the random components \\(\\eta\\) based on the normal distributions: \\[ \\begin{array}{l} \\beta_{i,travel} = b_{travel} + \\eta_{i,travel}\\\\ \\text{and}\\\\ \\beta_{i,wait} = b_{wait} + \\eta_{i,wait}\\\\ \\end{array} \\] The code to estimate the models is as follows: # MIXL T mixl_t &lt;- gmnl(choice ~ vcost + travel + wait | 1, data = TM, model = &quot;mixl&quot;, ranp = c(travel = &quot;n&quot;), R = 50) ## Estimating MIXL model mixl_t$logLik$message ## [1] &quot;successful convergence &quot; #summary(mixl_t) # MIXL W mixl_w &lt;- gmnl(choice ~ vcost + travel + wait | 1, data = TM, model = &quot;mixl&quot;, ranp = c(wait = &quot;n&quot;), R = 50) ## Estimating MIXL model mixl_w$logLik$message ## [1] &quot;successful convergence &quot; #summary(mixl) # MIXL T&amp;W mixl &lt;- gmnl(choice ~ vcost + travel + wait | 1, data = TM, model = &quot;mixl&quot;, ranp = c(travel = &quot;n&quot;, wait = &quot;n&quot;), R = 60) ## Estimating MIXL model mixl$logLik$message ## [1] &quot;successful convergence &quot; #summary(mixl) The models are summarized in Table ((???)) below: # Estimate a constants only model to calculate McFadden&#39;s _adjusted_ rho2 mixl0 &lt;- gmnl(choice ~ 1, data = TM, model = &quot;mnl&quot;) mixl_t.summary &lt;- rownames_to_column(data.frame(summary(mixl_t)$CoefTable), &quot;Variable&quot;) %&gt;% transmute(Variable, Estimate, pval = `Pr...z..`) mixl_w.summary &lt;- rownames_to_column(data.frame(summary(mixl_w)$CoefTable), &quot;Variable&quot;) %&gt;% transmute(Variable, Estimate, pval = `Pr...z..`) mixl.summary &lt;- rownames_to_column(data.frame(summary(mixl)$CoefTable), &quot;Variable&quot;) %&gt;% transmute(Variable, Estimate, pval = `Pr...z..`) mixl_table_1 &lt;- full_join(mixl_t.summary, mixl_w.summary, by = &quot;Variable&quot;) %&gt;% full_join(mixl.summary, by = &quot;Variable&quot;) kable(mixl_table_1, &quot;html&quot;, digits = 4, col.names = c(&quot;Variable&quot;, &quot;Estimate&quot;, &quot;p-value&quot;, &quot;Estimate&quot;, &quot;p-value&quot;, &quot;Estimate&quot;, &quot;p-value&quot;), caption = &quot;Mixed logit models&quot;) %&gt;% kable_styling() %&gt;% add_header_above(c(&quot; &quot; = 1, &quot;MIXL T&quot; = 2, &quot;MIXL W&quot; = 2, &quot;MIXL T&amp;W&quot; = 2)) %&gt;% footnote(general = c(paste0(&quot;Log-Likelihood: MIXL T = &quot;, round(mixl_t$logLik$maximum, digits = 3), &quot;; MIXL W = &quot;, round(mixl_w$logLik$maximum, digits = 3), &quot;; MIXL T&amp;W = &quot;, round(mixl$logLik$maximum, digits = 3)), paste0(&quot;McFadden Adjusted R^2: MIXL T = &quot;, round(1 - (mixl_t$logLik$maximum - nrow(mixl_t.summary)) / mixl0$logLik$maximum, digits = 3), &quot;; MIXL W = &quot;, round(1 - (mixl_w$logLik$maximum - nrow(mixl_w.summary)) / mixl0$logLik$maximum, digits = 3), &quot;; MIXL T&amp;W = &quot;, round(1 - (mixl$logLik$maximum - nrow(mixl.summary)) / mixl0$logLik$maximum, digits = 3)))) Table 9.2: Mixed logit models MIXL T MIXL W MIXL T&amp;W Variable Estimate p-value Estimate p-value Estimate p-value train:(intercept) 0.3613 0.7206 -0.0865 0.9230 5.2274 0.3990 bus:(intercept) -0.4468 0.6673 -0.9021 0.3810 2.9903 0.5327 car:(intercept) -4.9045 0.0000 -8.2200 0.0001 -13.9055 0.0344 vcost -0.0261 0.0062 -0.0205 0.0453 -0.0776 0.1676 wait -0.1164 0.0000 -0.1759 0.0000 -0.3897 0.0806 travel -0.0081 0.0006 -0.0064 0.0001 -0.0300 0.1820 sd.travel 0.0053 0.0093 NA NA 0.0194 0.2276 sd.wait NA NA 0.1014 0.0049 0.2199 0.0979 Note: Log-Likelihood: MIXL T = -189.105; MIXL W = -178.732; MIXL T&amp;W = -174.898 McFadden Adjusted R^2: MIXL T = 0.309; MIXL W = 0.345; MIXL T&amp;W = 0.355 The standard deviation of the random coefficients in models MIXL T and MIXL W are estimated. In Model MIXL T&amp;W, the levels of significance decline, with only a small increase in goodness-of-fit. Based on these results, the conclusion is that there is likely taste variations in waiting time, but probably not so in the case of travel time. Other coefficients have signs as expected. It can be seen that with respect to air travel, train is preferred, whereas bus and car are less preferred, other things being equal (see the alternative specific constants). So what do we learn from this model that we could not from the standard multinomial logit model or the nested logit model? 9.9 Behavioral insights from the mixed logit model The random coefficient of the MIXL W model was defined as follows: \\[ \\beta_{i,wait} = b_{wait} + \\eta_{i,wait} \\] with \\(\\eta\\) normal iid (independent and identically distributed). Therefore: \\[ \\beta_{i,wait} \\sim N(b_{wait}, \\sigma^2) \\] Based on this, we can explore the variation in taste in the population. 9.9.1 Unconditional distribution of a random parameter Given the parameters estimated above, we can plot the distribution of the random coefficient. This is done as follows: # Define parameters for the distribution mu &lt;- coef(mixl_w)[&#39;wait&#39;] sigma &lt;- coef(mixl_w)[&#39;sd.wait&#39;] # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -0.6, to = 0.2, by = 0.005)) %&gt;% mutate(normal = dnorm(x, mean = mu, sd = sigma)) df_p &lt;- data.frame(x =seq(from = 0, to = 0.2, by = 0.005)) %&gt;% mutate(normal = dnorm(x, mean = mu, sd = sigma)) # Plot ggplot() + geom_area(data = df, aes(x, normal), fill = &quot;orange&quot;, alpha = 0.5) + geom_area(data = df_p, aes(x, normal), fill = &quot;orange&quot;, alpha = 0.5) + #ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) + # Label the y axis ggtitle(&quot;Unconditional distribution for wait parameter&quot;) As seen in the figure, the response to waiting time is heterogeneous in the population. While in general most people dislike waiting, some dislike waiting more than others. The plot also suggests that some people may in fact enjoy their wait time (notice how the distribution includes some positive values for the coefficient). We can calculate what proportion of the population has a positive response to waiting time: 1 - pnorm(-coef(mixl_w)[&#39;wait&#39;] / coef(mixl_w)[&#39;sd.wait&#39;]) ## wait ## 0.04135726 Accordingly, about 4.1% of the population has a positive parameter for waiting time. As discussed above, the willingess to pay for reductions in waiting time was: \\[ -\\frac{\\partial \\text{vcost}}{\\partial \\text{wait}} = \\frac{\\beta_{wait}}{\\beta_{vcost}} = \\frac{-0.0969}{-0.0139} \\simeq 6.97 \\] or approximately 6.97 dollars per minute. But if the response to waiting time is a random variable, what is then the distribution of the willingness to pay? To answer this question, we note that multiplying a normal random variable \\(X\\) by a constant \\(c\\) changes the mean and the standard deviation of the distribution as follows: \\[ \\begin{array} cE[X] = E[cX]\\\\ c^2Var[X] = Var[cX] \\end{array} \\] Accordingly, since willingness to pay is the ratio of a random coefficient to a constant, the distribution of willingness to pay is: \\[ -\\frac{\\partial \\text{vcost}}{\\partial \\text{wait}} \\sim N(\\frac{b_{wait}}{\\beta_{vcost}}, \\frac{\\sigma_{wait}^2}{\\beta_{vcost}^2}) \\] The distribution of willingness to pay for waiting time is shown in the following figure: # Define parameters for the distribution of willingness to pay mu &lt;- coef(mixl_w)[&#39;wait&#39;] * (1 / coef(mixl_w)[&#39;vcost&#39;]) sigma &lt;- coef(mixl_w)[&#39;sd.wait&#39;]* sqrt((1 / coef(mixl_w)[&#39;vcost&#39;])^2) # Create a data frame for plotting df &lt;- data.frame(x =seq(from = -10, to = 30, by = 0.1)) %&gt;% mutate(normal = dnorm(x, mean = mu, sd = sigma)) #df_p &lt;- data.frame(x =seq(from = 0, to = 0.2, by = 0.005)) %&gt;% # mutate(normal = dnorm(x, mean = mu, sd = sigma)) # Plot ggplot() + geom_area(data = df, aes(x, normal), fill = &quot;orange&quot;, alpha = 0.5) + # geom_area(data = df_p, aes(x, normal), fill = &quot;orange&quot;, alpha = 0.5) + #ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) + # Label the y axis ggtitle(&quot;Unconditional distribution for willingness to pay for wait&quot;) It can be seen from the figure that whereas the willingness to pay for saving waiting time was estimated at approximately seven dollars per minute by the multinomial logit model, there appears to be a segment of the population willing to pay more than that value. 9.9.2 Conditional distribution of the random coefficients plot(mixl_w, &quot;wait&quot;, main = &quot;Conditional distribution for wait (MIXL W)&quot;) 9.10 Using covariates to capture variations in taste The models above strongly suggest that there are important variations in taste in the way decision-makers respond to waiting time. However, no variables were used to characterize the decision-makers. As previously suggested, the variation might happen on a socio-economic dimension, for instance, income. In this section, we will explore whether including individual-level attributes can capture heterogeneity in decision-making, and if so, to what extent. We will test two different models, with the specifications described below. Multinomial logit with individual-level covariates (MNL-Covariates): \\[ \\begin{array}{lll} V_{i,air} &amp;= &amp; + \\beta_{vcost}vcost_{air} + \\beta_{i,wait}wait_{air} + \\beta_{travel}travel_{air}\\\\ V_{i,train} &amp;=\\beta_{train} &amp; + \\beta_{vcost}vcost_{train} + \\beta_{wait}wait_{train} + \\beta_{travel}travel_{train} &amp; + \\beta_{train:income}income_i + \\beta_{train:size}size_i\\\\ V_{i,bus} &amp;=\\beta_{bus} &amp;+ \\beta_{vcost}vcost_{bus}+\\beta_{wait}wait_{bus}+\\beta_{travel}travel_{bus}&amp; + \\beta_{bus:income}income_i + \\beta_{bus:size}size_i\\\\ V_{i,car} &amp;=\\beta_{car} &amp;+ \\beta_{vcost}vcost_{car}+\\beta_{wait}wait_{car}+\\beta_{travel}travel_{car}&amp; + \\beta_{car:income}income_i + \\beta_{car:size}size_i \\end{array} \\] This model is similar to the base multinomial logit model, with the addition of individual-level attributes. Multinomial logit with deterministic expansions of coefficients (MNL-Expansion): \\[ \\begin{array}{lll} V_{i,air} &amp;= &amp; + \\beta_{vcost}vcost_{air} + \\beta_{i,wait}wait_{air} + \\beta_{i,travel}travel_{air}\\\\ V_{i,train} &amp;=\\beta_{train} &amp; + \\beta_{vcost}vcost_{train} + \\beta_{i,wait}wait_{train} + \\beta_{i,travel}travel_{train}\\\\ V_{i,bus} &amp;=\\beta_{bus} &amp;+ \\beta_{vcost}vcost_{bus}+\\beta_{i,wait}wait_{bus}+\\beta_{i,travel}travel_{bus}\\\\ V_{i,car} &amp;=\\beta_{car} &amp;+ \\beta_{vcost}vcost_{car}+\\beta_{i,wait}wait_{car}+\\beta_{i,travel}travel_{car} \\end{array} \\] with expanded coefficients as follows: \\[ \\begin{array}{l} \\beta_{i,travel} = b_{travel} + b_{travel:income}income_i + b_{travel:size}size_i\\\\ \\text{and}\\\\ \\beta_{i,wait} = b_{wait} + b_{wait:income}income_i \\end{array} \\] To implement the expansion of the coefficients we need to add variable interactions to the table, and reformat for use in mlogit and gmnl: TM &lt;- mutate(TravelMode, `wait:income` = wait * income, `travel:income` = travel * income, `wait:size` = wait * size, `travel:size` = travel * size) TM &lt;- mlogit.data(TM, choice = &quot;choice&quot;, shape = &quot;long&quot;, alt.levels = c(&quot;air&quot;, &quot;train&quot;, &quot;bus&quot;, &quot;car&quot;)) The models are estimated using the following code: mnl_cov &lt;- mlogit(choice ~ vcost + travel + wait | income + size, data = TM) #summary(mnl1) mnl_exp &lt;- mlogit(choice ~ vcost + travel + travel:income + travel:size + wait + wait:income | 1, data = TM) #summary(mnl2) Summarize results mnl with covariates: mnl_null &lt;- mlogit(choice ~ 1, data = TM) mnl1.summary &lt;- rownames_to_column(data.frame(summary(mnl_cov)$CoefTable), &quot;Variable&quot;) %&gt;% transmute(Variable, Estimate, pval = `Pr...z..`) mnl2.summary &lt;- rownames_to_column(data.frame(summary(mnl_exp)$CoefTable), &quot;Variable&quot;) %&gt;% transmute(Variable, Estimate, pval = `Pr...z..`) df_logit_2 &lt;- full_join(mnl1.summary, mnl2.summary, by = &quot;Variable&quot;) kable(df_logit_2, &quot;html&quot;, digits = 4, col.names = c(&quot;Variable&quot;, &quot;Estimate&quot;, &quot;p-value&quot;, &quot;Estimate&quot;, &quot;p-value&quot;)) %&gt;% kable_styling() %&gt;% add_header_above(c(&quot; &quot; = 1, &quot;MNL - Covariates&quot; = 2, &quot;MNL - Expansion&quot; = 2)) %&gt;% footnote(general = c(paste0(&quot;Log-Likelihood: MNL - Covariates = &quot;, round(mnl_cov$logLik[1], digits = 3), &quot;; MNL - Expansion = &quot;, round(mnl_exp$logLik[1], digits = 3)), paste0(&quot;McFadden Adjusted R^2: MNL - Covariates = &quot;, round(1 - (mnl_cov$logLik[1] - nrow(mnl1.summary)) / mnl_null$logLik[1], digits = 3), &quot;; NL = &quot;, round(1 - (mnl_exp$logLik[1] - nrow(mnl2.summary)) / mnl_null$logLik[1], digits = 3)))) MNL - Covariates MNL - Expansion Variable Estimate p-value Estimate p-value train:(intercept) -0.4616 0.6288 -0.7241 0.2412 bus:(intercept) -1.5305 0.1577 -1.3373 0.0541 car:(intercept) -6.0352 0.0000 -5.1305 0.0000 vcost -0.0087 0.2710 -0.0158 0.0333 travel -0.0041 0.0000 -0.0049 0.0004 wait -0.1012 0.0000 -0.0817 0.0000 train:income -0.0667 0.0000 NA NA bus:income -0.0284 0.0963 NA NA car:income -0.0075 0.5710 NA NA train:size 1.1387 0.0002 NA NA bus:size 0.7745 0.0447 NA NA car:size 0.9224 0.0004 NA NA travel:income NA NA -0.0001 0.0004 travel:size NA NA 0.0020 0.0000 wait:income NA NA -0.0006 0.0089 Note: Log-Likelihood: MNL - Covariates = -172.468; MNL - Expansion = -174.887 McFadden Adjusted R^2: MNL - Covariates = 0.35; NL = 0.352 Both models above suggest that there at least some heterogeneity in behavior can be attributed to the two characteristics of the decision-makers investigated (income and party size). In particular, we can see from the expanded coefficients that the disutility of travel time and waiting time is greater at higher levels of income: # Create a data frame for plotting df &lt;- data.frame(income =seq(from = min(TM$income), to = max(TM$income), by = 1)) %&gt;% mutate(time = coef(mnl_exp)[&#39;travel&#39;] + coef(mnl_exp)[&#39;travel:income&#39;] * income, wait = coef(mnl_exp)[&#39;wait&#39;] + coef(mnl_exp)[&#39;wait:income&#39;] * income) %&gt;% gather(variable, coefficient, 2:3) # Plot ggplot(df) + geom_line(aes(x = income, y = coefficient, color = variable)) However, is the inclusion of these variables sufficient to capture variations in taste? What if we are missing some other relevant variable, such as age? In the following section we will revisit the specification of the models to test this idea. 9.11 Revisiting the example We will reestimate the mixed logit models, but now after introducing the individual-level covariates as stand-alone variables and also as part of expansions. The expansion of the coefficient for waiting time includes a random component to give a random coefficient as follows: \\[ \\begin{array}{l} \\beta_{i,travel} = b_{travel} + b_{travel:income}income_i + b_{travel:size}size_i\\\\ \\text{and}\\\\ \\beta_{i,wait} = b_{wait} + b_{wait:income}income_i + \\eta_{i,wait} \\end{array} \\] The models are estimated next: mixl_w1 &lt;- gmnl(choice ~ vcost + travel + wait | income + size, data = TM, model = &quot;mixl&quot;, ranp = c(wait = &quot;n&quot;), R = 50) ## Estimating MIXL model #mixl_cov$logLik$message #summary(mixl_w1) mixl_w2 &lt;- gmnl(choice ~ vcost + travel + travel:income + travel:size + wait + wait:income | 1, data = TM, model = &quot;mixl&quot;, ranp = c(wait = &quot;n&quot;), R = 50) ## Estimating MIXL model #mixl_cov$logLik$message #summary(mixl_w2) and the results summarized in the table below: mixl_w1.summary &lt;- rownames_to_column(data.frame(summary(mixl_w1)$CoefTable), &quot;Variable&quot;) %&gt;% transmute(Variable, Estimate, pval = `Pr...z..`) mixl_w2.summary &lt;- rownames_to_column(data.frame(summary(mixl_w2)$CoefTable), &quot;Variable&quot;) %&gt;% transmute(Variable, Estimate, pval = `Pr...z..`) mixl_table_2 &lt;- full_join(mixl_w1.summary, mixl_w2.summary, by = &quot;Variable&quot;) kable(mixl_table_2, &quot;html&quot;, digits = 4, col.names = c(&quot;Variable&quot;, &quot;Estimate&quot;, &quot;p-value&quot;, &quot;Estimate&quot;, &quot;p-value&quot;)) %&gt;% kable_styling() %&gt;% add_header_above(c(&quot; &quot; = 1, &quot;MIXL W-1&quot; = 2, &quot;MIXL W-2&quot; = 2)) %&gt;% footnote(general = c(paste0(&quot;Log-Likelihood: MIXL W-1 = &quot;, round(mixl_w1$logLik$maximum, digits = 3), &quot;; MIXL W-2 = &quot;, round(mixl_w1$logLik$maximum, digits = 3)), paste0(&quot;McFadden Adjusted R^2: MIXL W-1 = &quot;, round(1 - (mixl_w1$logLik$maximum - nrow(mixl_w1.summary)) / mixl0$logLik$maximum, digits = 3), &quot;; MIXL W-2 = &quot;, round(1 - (mixl_w2$logLik$maximum - nrow(mixl_w2.summary)) / mixl0$logLik$maximum, digits = 3)))) MIXL W-1 MIXL W-2 Variable Estimate p-value Estimate p-value train:(intercept) 0.1422 0.9187 -0.3663 0.6594 bus:(intercept) -1.1624 0.4455 -1.1091 0.2417 car:(intercept) -8.5571 0.0000 -7.2119 0.0000 vcost -0.0104 0.3230 -0.0197 0.0495 travel -0.0058 0.0001 -0.0065 0.0009 train:income -0.0741 0.0004 NA NA bus:income -0.0287 0.1784 NA NA car:income -0.0038 0.8757 NA NA train:size 1.2021 0.0021 NA NA bus:size 0.8061 0.0914 NA NA car:size 1.1972 0.0098 NA NA wait -0.1505 0.0000 -0.1179 0.0000 sd.wait 0.0638 0.0029 0.0599 0.0008 travel:income NA NA -0.0001 0.0010 travel:size NA NA 0.0024 0.0002 wait:income NA NA -0.0011 0.0213 Note: Log-Likelihood: MIXL W-1 = -163.542; MIXL W-2 = -163.542 McFadden Adjusted R^2: MIXL W-1 = 0.378; MIXL W-2 = 0.381 Given the expansion, the distribution of the random coefficient now is: \\[ \\beta_{i,wait} \\sim N\\Big(b_{wait} + b_{wait:income}income, \\sigma_{wait}^2\\Big) \\] which means that we can calculate the distribution at different levels of income, as shown in the following figure for the first quartile (yellow) and the third quartile (red) of income. The dashed line is the distribution of willingness to pay according to model MIXL W. # Define parameters for the distribution of willingness to pay # Obtain quartiles q &lt;- quantile(TM$income, c(0, 0.25, 0.5, 0.75, 1)) # Define parameters for the distribution mu_w &lt;- coef(mixl_w)[&#39;wait&#39;] sigma_w &lt;- coef(mixl_w)[&#39;sd.wait&#39;] # First quartile mu_w2.1 &lt;- coef(mixl_w2)[&#39;wait&#39;] + coef(mixl_w2)[&quot;wait:income&quot;] * q[2] sigma_w2.1 &lt;- coef(mixl_w2)[&#39;sd.wait&#39;] # Third quartile mu_w2.3 &lt;- coef(mixl_w2)[&#39;wait&#39;] + coef(mixl_w2)[&quot;wait:income&quot;] * q[4] sigma_w2.3 &lt;- coef(mixl_w2)[&#39;sd.wait&#39;] # Create a data frame for plotting df_w &lt;- data.frame(x =seq(from = -0.6, to = 0.2, by = 0.005)) %&gt;% mutate(normal = dnorm(x, mean = mu_w, sd = sigma_w)) df_w2.1 &lt;- data.frame(x =seq(from = -0.6, to = 0.2, by = 0.005)) %&gt;% mutate(normal = dnorm(x, mean = mu_w2.1, sd = sigma_w2.1)) df_w2.3 &lt;- data.frame(x =seq(from = -0.6, to = 0.2, by = 0.005)) %&gt;% mutate(normal = dnorm(x, mean = mu_w2.3, sd = sigma_w2.3)) # Plot ggplot() + geom_area(data = df_w2.1, aes(x, normal), fill = &quot;yellow&quot;, alpha = 0.3) + geom_line(data = df_w2.1, aes(x, normal), alpha = 0.3) + geom_area(data = df_w2.3, aes(x, normal), fill = &quot;red&quot;, alpha = 0.3) + geom_line(data = df_w2.3, aes(x, normal), alpha = 0.3) + geom_line(data = df_w, aes(x, normal), linetype = 3) + #ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) + # Label the y axis ggtitle(&quot;Unconditional distribution for wait parameter (dashed line is MIXL W)&quot;) As before, we can also obtain the conditional distribution of the random component, which here is compared to the earlier model MIXL-W (without the income effect): plot(mixl_w, &quot;wait&quot;, main = &quot;Conditional distribution for wait (MIXL W)&quot;) plot(mixl_w2, &quot;wait&quot;, main = &quot;Conditional distribution for wait (MIXL W-2)&quot;) In a similar way, the distribution of the willingness to pay for waiting time according to model MIXL W-2 is: \\[ -\\frac{\\partial \\text{vcost}}{\\partial \\text{wait}} \\sim N\\Big(\\frac{b_{wait} + b_{wait: income}income}{\\beta_{vcost}}, \\frac{\\sigma_{wait}^2}{\\beta_{vcost}^2}\\Big) \\] which is shown in the following figure for the first quartile (yellow) and the third quartile (red) of income. The dashed line is the distribution of willingness to pay according to model MIXL W. # Define parameters for the distribution of willingness to pay # Obtain quartiles q &lt;- quantile(TM$income, c(0, 0.25, 0.5, 0.75, 1)) # MIX W2 First quartile mu_w2.1 &lt;- (coef(mixl_w2)[&#39;wait&#39;] + coef(mixl_w2)[&#39;wait:income&#39;] * q[2]) * (1 / coef(mixl_w2)[&#39;vcost&#39;]) sigma_w2.1 &lt;- coef(mixl_w2)[&#39;sd.wait&#39;] * sqrt((1 / coef(mixl_w2)[&#39;vcost&#39;])^2) # MIX W2 Third quartile mu_w2.3 &lt;- (coef(mixl_w2)[&#39;wait&#39;] + coef(mixl_w2)[&#39;wait:income&#39;] * q[4]) * (1 / coef(mixl_w2)[&#39;vcost&#39;]) sigma_w2.3 &lt;- coef(mixl_w2)[&#39;sd.wait&#39;] * sqrt((1 / coef(mixl_w2)[&#39;vcost&#39;])^2) # MIX W mu_w &lt;- coef(mixl_w)[&#39;wait&#39;] * (1 / coef(mixl_w)[&#39;vcost&#39;]) sigma_w &lt;- coef(mixl_w)[&#39;sd.wait&#39;] * sqrt((1 / coef(mixl_w)[&#39;vcost&#39;])^2) # Create data frames for plotting df_w2.1 &lt;- data.frame(x =seq(from = -10, to = 30, by = 0.1)) %&gt;% mutate(normal = dnorm(x, mean = mu_w2.1, sd = sigma_w2.1)) df_w2.3 &lt;- data.frame(x =seq(from = -10, to = 30, by = 0.1)) %&gt;% mutate(normal = dnorm(x, mean = mu_w2.3, sd = sigma_w2.3)) df_w &lt;- data.frame(x =seq(from = -10, to = 30, by = 0.1)) %&gt;% mutate(normal = dnorm(x, mean = mu_w, sd = sigma_w)) # Plot ggplot() + geom_area(data = df_w2.1, aes(x, normal), fill = &quot;yellow&quot;, alpha = 0.3) + geom_line(data = df_w2.1, aes(x, normal), alpha = 0.3) + geom_area(data = df_w2.3, aes(x, normal), fill = &quot;red&quot;, alpha = 0.3) + geom_line(data = df_w2.3, aes(x, normal), alpha = 0.3) + geom_line(data = df_w, aes(x, normal), linetype = 3) + #ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis geom_hline(yintercept = 0) + # Add y axis geom_vline(xintercept = 0) + # Add x axis ylab(&quot;f(x)&quot;) + # Label the y axis ggtitle(&quot;Unconditional distribution for willingness to pay for wait (dashed line is MIXL W)&quot;) The results indicate that the introduction of individual-level covariates has absorbed some of the heterogeneity previously observed in the random coefficients, but it was not sufficient to completely account for variations in taste. 9.12 Full covariance matrix for random components TO DO: Discuss parameter covariance with correlation 9.13 Final remarks The mixed logit modeling framework addresses two important issues that can arise in the case of the standard multinomial logit model: taste variations and non-proportional substitution patterns. As discussed above, the multinomial probit model is technically capable of doing this as well. So, what are the relative merits of these two modeling approaches? Both are computationally demanding and need numerical integration methods. In my view, the advantage of the mixed logit model over the multinomial probit model is that the results are more intuitive and easy to interpret in the case of the mixed logit model. Unlike the probit model, where the covariance matrix is of the differences of utilities, in the case of the mixed logit the covariance matrix relates directly to the error components and/or random coefficients. As well, it is possible to show (see Train Chapter 6) that the mixed logit model can approximate to any arbitrary degree any desired discrete choice model, including the multinomial probit model. 9.14 Exercise What is the difference between an error component and a random coefficient in a mixed logit model? Do mixed logit models display the IIA property? Explain. Load the following dataset from the mlogit package: data(&quot;MobilePhones&quot;) Using the dataset above, estimate a mixed logit model. Justify your choice of variables. Which variables do you choose to have random coefficients and why? Graphically show the random coefficients of your model, both the unconditional and conditional distributions. Discuss the results. References "],
["references.html", "References", " References "]
]
